<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>二进制方式部署kubernetes高可用集群 | PaperMod</title>
<meta name="keywords" content="">
<meta name="description" content="环境说明 宿主机系统：Windows 10 虚拟机版本：VMware® Workstation 16 Pro IOS镜像版本：CentOS Linux release 7.9.2009 Kubernetes版本：1.26.4 Runtime：Containerd v1.6.20 Etcd版本：3.5.6 集群操作用户：root 更新时间：2023-04-21 CentOS7安装请参考博客文章：LINUX之VMWARE WORKSTATION安装CENTOS-7
资源分配 网段划分
Kubernetes集群需要规划三个网段：
宿主机网段：Kubernetes集群节点的网段 Pod网段：集群内Pod的网段，相当于容器的IP Service网段：集群内服务发现使用的网段，service用于集群容器通信 生产环境根据申请到的IP资源进行分配即可，原则是三个网段不允许有重合IP。IP网段计算可以参考：在线IP地址计算。本文虚拟机练习环境IP地址网段分配如下：
宿主机网段：192.168.43.1/24 Pod网段：172.16.0.0/12 Service：10.96.0.0/12 节点分配
采用3管理节点2工作节点的高可用Kubernetes集群模式：
k8s-master01/k8s-master02/k8s-master03 集群的Master节点 三个master节点同时做etcd集群 k8s-node01/k8s-node02 集群的Node节点 k8s-master-vip做高可用k8s-master01~03的VIP，不占用物理资源 主机节点名称 IP CPU核心数 内存大小 磁盘大小 k8s-master-vip 192.168.43.200 / / / k8s-master01 192.168.43.201 2 2G 40G k8s-master02 192.168.43.202 2 2G 40G k8s-master03 192.168.43.203 2 2G 40G k8s-node01 192.168.43.204 2 2G 40G k8s-node02 192.168.43.205 2 2G 40G 操作步骤 标题后小括号注释表明操作范围：">
<meta name="author" content="Theme PaperMod">
<link rel="canonical" href="https://deemoprobe.github.io/%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2kubernetes%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.214d617c1a333f5560008d2a177e6922ea88d9490ca3d7c15011ea76686aa49b.css" integrity="sha256-IU1hfBozP1VgAI0qF35pIuqI2UkMo9fBUBHqdmhqpJs=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://deemoprobe.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://deemoprobe.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://deemoprobe.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://deemoprobe.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://deemoprobe.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://deemoprobe.github.io/%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2kubernetes%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="二进制方式部署kubernetes高可用集群" />
<meta property="og:description" content="环境说明 宿主机系统：Windows 10 虚拟机版本：VMware® Workstation 16 Pro IOS镜像版本：CentOS Linux release 7.9.2009 Kubernetes版本：1.26.4 Runtime：Containerd v1.6.20 Etcd版本：3.5.6 集群操作用户：root 更新时间：2023-04-21 CentOS7安装请参考博客文章：LINUX之VMWARE WORKSTATION安装CENTOS-7
资源分配 网段划分
Kubernetes集群需要规划三个网段：
宿主机网段：Kubernetes集群节点的网段 Pod网段：集群内Pod的网段，相当于容器的IP Service网段：集群内服务发现使用的网段，service用于集群容器通信 生产环境根据申请到的IP资源进行分配即可，原则是三个网段不允许有重合IP。IP网段计算可以参考：在线IP地址计算。本文虚拟机练习环境IP地址网段分配如下：
宿主机网段：192.168.43.1/24 Pod网段：172.16.0.0/12 Service：10.96.0.0/12 节点分配
采用3管理节点2工作节点的高可用Kubernetes集群模式：
k8s-master01/k8s-master02/k8s-master03 集群的Master节点 三个master节点同时做etcd集群 k8s-node01/k8s-node02 集群的Node节点 k8s-master-vip做高可用k8s-master01~03的VIP，不占用物理资源 主机节点名称 IP CPU核心数 内存大小 磁盘大小 k8s-master-vip 192.168.43.200 / / / k8s-master01 192.168.43.201 2 2G 40G k8s-master02 192.168.43.202 2 2G 40G k8s-master03 192.168.43.203 2 2G 40G k8s-node01 192.168.43.204 2 2G 40G k8s-node02 192.168.43.205 2 2G 40G 操作步骤 标题后小括号注释表明操作范围：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://deemoprobe.github.io/%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2kubernetes%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/" /><meta property="og:image" content="https://deemoprobe.github.io/papermod-cover.png"/><meta property="article:section" content="" />
<meta property="article:published_time" content="2023-04-25T02:04:03+08:00" />
<meta property="article:modified_time" content="2023-04-25T02:04:03+08:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://deemoprobe.github.io/papermod-cover.png"/>

<meta name="twitter:title" content="二进制方式部署kubernetes高可用集群"/>
<meta name="twitter:description" content="环境说明 宿主机系统：Windows 10 虚拟机版本：VMware® Workstation 16 Pro IOS镜像版本：CentOS Linux release 7.9.2009 Kubernetes版本：1.26.4 Runtime：Containerd v1.6.20 Etcd版本：3.5.6 集群操作用户：root 更新时间：2023-04-21 CentOS7安装请参考博客文章：LINUX之VMWARE WORKSTATION安装CENTOS-7
资源分配 网段划分
Kubernetes集群需要规划三个网段：
宿主机网段：Kubernetes集群节点的网段 Pod网段：集群内Pod的网段，相当于容器的IP Service网段：集群内服务发现使用的网段，service用于集群容器通信 生产环境根据申请到的IP资源进行分配即可，原则是三个网段不允许有重合IP。IP网段计算可以参考：在线IP地址计算。本文虚拟机练习环境IP地址网段分配如下：
宿主机网段：192.168.43.1/24 Pod网段：172.16.0.0/12 Service：10.96.0.0/12 节点分配
采用3管理节点2工作节点的高可用Kubernetes集群模式：
k8s-master01/k8s-master02/k8s-master03 集群的Master节点 三个master节点同时做etcd集群 k8s-node01/k8s-node02 集群的Node节点 k8s-master-vip做高可用k8s-master01~03的VIP，不占用物理资源 主机节点名称 IP CPU核心数 内存大小 磁盘大小 k8s-master-vip 192.168.43.200 / / / k8s-master01 192.168.43.201 2 2G 40G k8s-master02 192.168.43.202 2 2G 40G k8s-master03 192.168.43.203 2 2G 40G k8s-node01 192.168.43.204 2 2G 40G k8s-node02 192.168.43.205 2 2G 40G 操作步骤 标题后小括号注释表明操作范围："/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "二进制方式部署kubernetes高可用集群",
      "item": "https://deemoprobe.github.io/%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2kubernetes%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "二进制方式部署kubernetes高可用集群",
  "name": "二进制方式部署kubernetes高可用集群",
  "description": "环境说明 宿主机系统：Windows 10 虚拟机版本：VMware® Workstation 16 Pro IOS镜像版本：CentOS Linux release 7.9.2009 Kubernetes版本：1.26.4 Runtime：Containerd v1.6.20 Etcd版本：3.5.6 集群操作用户：root 更新时间：2023-04-21 CentOS7安装请参考博客文章：LINUX之VMWARE WORKSTATION安装CENTOS-7\n资源分配 网段划分\nKubernetes集群需要规划三个网段：\n宿主机网段：Kubernetes集群节点的网段 Pod网段：集群内Pod的网段，相当于容器的IP Service网段：集群内服务发现使用的网段，service用于集群容器通信 生产环境根据申请到的IP资源进行分配即可，原则是三个网段不允许有重合IP。IP网段计算可以参考：在线IP地址计算。本文虚拟机练习环境IP地址网段分配如下：\n宿主机网段：192.168.43.1/24 Pod网段：172.16.0.0/12 Service：10.96.0.0/12 节点分配\n采用3管理节点2工作节点的高可用Kubernetes集群模式：\nk8s-master01/k8s-master02/k8s-master03 集群的Master节点 三个master节点同时做etcd集群 k8s-node01/k8s-node02 集群的Node节点 k8s-master-vip做高可用k8s-master01~03的VIP，不占用物理资源 主机节点名称 IP CPU核心数 内存大小 磁盘大小 k8s-master-vip 192.168.43.200 / / / k8s-master01 192.168.43.201 2 2G 40G k8s-master02 192.168.43.202 2 2G 40G k8s-master03 192.168.43.203 2 2G 40G k8s-node01 192.168.43.204 2 2G 40G k8s-node02 192.168.43.205 2 2G 40G 操作步骤 标题后小括号注释表明操作范围：",
  "keywords": [
    
  ],
  "articleBody": "环境说明 宿主机系统：Windows 10 虚拟机版本：VMware® Workstation 16 Pro IOS镜像版本：CentOS Linux release 7.9.2009 Kubernetes版本：1.26.4 Runtime：Containerd v1.6.20 Etcd版本：3.5.6 集群操作用户：root 更新时间：2023-04-21 CentOS7安装请参考博客文章：LINUX之VMWARE WORKSTATION安装CENTOS-7\n资源分配 网段划分\nKubernetes集群需要规划三个网段：\n宿主机网段：Kubernetes集群节点的网段 Pod网段：集群内Pod的网段，相当于容器的IP Service网段：集群内服务发现使用的网段，service用于集群容器通信 生产环境根据申请到的IP资源进行分配即可，原则是三个网段不允许有重合IP。IP网段计算可以参考：在线IP地址计算。本文虚拟机练习环境IP地址网段分配如下：\n宿主机网段：192.168.43.1/24 Pod网段：172.16.0.0/12 Service：10.96.0.0/12 节点分配\n采用3管理节点2工作节点的高可用Kubernetes集群模式：\nk8s-master01/k8s-master02/k8s-master03 集群的Master节点 三个master节点同时做etcd集群 k8s-node01/k8s-node02 集群的Node节点 k8s-master-vip做高可用k8s-master01~03的VIP，不占用物理资源 主机节点名称 IP CPU核心数 内存大小 磁盘大小 k8s-master-vip 192.168.43.200 / / / k8s-master01 192.168.43.201 2 2G 40G k8s-master02 192.168.43.202 2 2G 40G k8s-master03 192.168.43.203 2 2G 40G k8s-node01 192.168.43.204 2 2G 40G k8s-node02 192.168.43.205 2 2G 40G 操作步骤 标题后小括号注释表明操作范围：\nALL 所有节点（k8s-master01/k8s-master02/k8s-master03/k8s-node01/k9s-node02）执行 Master 只需要在master节点（k8s-master01/k8s-master02/k8s-master03）执行 Node 只需要在node节点（k8s-node01/k8s-node02）执行 已标注的个别命令只需要在某一台机器执行，会在操作前说明 未标注的会在操作时说明 使用cat \u003c\u003c \"EOF\" \u003e\u003e file或cat \u003e\u003e file \u003c\u003c \"EOF\"添加文件内容注意cat后面的EOF一定要加上双引号（标准输入的），否则不会保留输入时的缩进格式而且会直接解析输入时的变量，进而造成文件可读性差甚至不可用；同时注意文件的\u003e重写与\u003e\u003e追加。虽然单独转义输入时的变量也能避免变量被解析，但是不推荐，漏转义会造成不必要的麻烦。\n准备工作(ALL) 添加主机信息、关闭防火墙、关闭swap、关闭SELinux、dnsmasq、NetworkManager\n# 添加主机信息 cat \u003c\u003c \"EOF\" \u003e\u003e /etc/hosts 192.168.43.200 k8s-master-vip 192.168.43.201 k8s-master01 192.168.43.202 k8s-master02 192.168.43.203 k8s-master03 192.168.43.204 k8s-node01 192.168.43.205 k8s-node02 EOF # 关闭防火墙、dnsmasq、NetworkManager，--now参数表示关闭服务并移除开机自启 # 这些服务是否可以关闭视情况而定，本文是虚拟机实践，没有用到这些服务 systemctl disable --now firewalld systemctl disable --now dnsmasq systemctl disable --now NetworkManager # 关闭swap，并注释fstab文件swap所在行 swapoff -a sed -i '/swap/s/^\\(.*\\)$/#\\1/g' /etc/fstab # 关闭SELinux，并更改selinux配置文件 setenforce 0 sed -i \"s/=enforcing/=disabled/g\" /etc/selinux/config 值得注意的是/etc/sysconfig/selinux文件是/etc/selinux/config文件的软连接，用sed -i命令修改软连接文件会破坏软连接属性，将/etc/sysconfig/selinux变为一个独立的文件，即使该文件被修改了，但源文件/etc/selinux/config配置是没变的。此外，使用vim等编辑器编辑源文件或链接文件（编辑模式不会修改文件属性）修改也可以。软链接原理可参考博客：LINUX之INODE详解\n# 默认的yum源太慢，更新为阿里源，同时用sed命令删除文件中不需要的两个URL的行 curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo sed -i -e '/mirrors.cloud.aliyuncs.com/d' -e '/mirrors.aliyuncs.com/d' /etc/yum.repos.d/CentOS-Base.repo # 安装常用工具包 yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git -y # 配置ntpdate，同步服务器时间 rpm -ivh http://mirrors.wlnmp.com/centos/wlnmp-release-centos.noarch.rpm yum install ntpdate -y # 同步时区和时间 ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime echo 'Asia/Shanghai' \u003e/etc/timezone ntpdate time2.aliyun.com # 可以加入计划任务，保证集群时钟是一致的 # /var/spool/cron/root文件也是crontab -e写入的文件 # crontab执行日志查看可用：tail -f /var/log/cron cat \u003c\u003c \"EOF\" \u003e\u003e /var/spool/cron/root */5 * * * * /usr/sbin/ntpdate time2.aliyun.com EOF # 须知：如果设置了定时任务，会经常收到提示“You have new mail in /var/spool/mail/root” # （可选）禁用提示：echo \"unset MAILCHECK\" \u003e\u003e ~/.bashrc;source ~/.bashrc # 禁用提示后/var/spool/mail/root文件依旧会记录root操作日志，可随时查看 # 保证文件句柄不会限制集群的可持续发展，配置limits ulimit -SHn 65500 cat \u003c\u003c \"EOF\" \u003e\u003e /etc/security/limits.conf * soft nofile 65500 * hard nofile 65500 * soft nproc 65500 * hard nproc 65500 * soft memlock unlimited * hard memlock unlimited EOF # 配置免密登录，k8s-master01到其他节点 # 生成密钥对（在k8s-master01节点配置即可） ssh-keygen -t rsa # 拷贝公钥到其他节点，首次需要认证一下各个节点的root密码，以后就可以免密ssh到其他节点 for i in k8s-master02 k8s-master03 k8s-node01 k8s-node02;do ssh-copy-id -i .ssh/id_rsa.pub $i;done # 克隆二进制仓库1.26分支的文件（k8s-master01上操作即可） cd /root;git clone https://gitee.com/deemoprobe/k8s-ha-install.git -b manual-installation-v1.26.x # 所有节点系统升级 yum update --exclude=kernel* -y 升级内核，4.17以下的内核cgroup存在内存泄漏的BUG，具体分析过程浏览器搜Kubernetes集群为什么要升级内核会有很多文章讲解\n内核备用下载（下载到本地后上传到服务器，尽量不要用wget）：\nkernel-ml-devel-4.19.12-1.el7.elrepo.x86_64.rpm kernel-ml-4.19.12-1.el7.elrepo.x86_64.rpm # 下载4.19版本内核，如果无法下载，可以用上面提供的备用下载 cd /root wget http://193.49.22.109/elrepo/kernel/el7/x86_64/RPMS/kernel-ml-devel-4.19.12-1.el7.elrepo.x86_64.rpm wget http://193.49.22.109/elrepo/kernel/el7/x86_64/RPMS/kernel-ml-4.19.12-1.el7.elrepo.x86_64.rpm # 可以在k8s-master01节点下载后，免密传到其他节点 for i in k8s-master02 k8s-master03 k8s-node01 k8s-node02;do scp kernel-ml-* $i:/root;done # 所有节点安装内核 cd /root \u0026\u0026 yum localinstall -y kernel-ml* # 所有节点更改内核启动顺序 grub2-set-default 0 \u0026\u0026 grub2-mkconfig -o /etc/grub2.cfg grubby --args=\"user_namespace.enable=1\" --update-kernel=\"$(grubby --default-kernel)\" # 查看默认内核，并重启节点 grubby --default-kernel reboot # 确认内核版本 uname -a # （可选）删除老版本的内核，避免以后被升级取代默认的开机4.19内核 rpm -qa | grep kernel yum remove -y kernel-3* # 升级系统软件包（如果跳过内核升级加参数 --exclude=kernel*） yum update -y # 安装IPVS相关工具，由于IPVS在资源消耗和性能上均已明显优于iptables，所以推荐开启 # 具体原因可参考官网介绍 https://kubernetes.io/zh/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/ yum install ipvsadm ipset sysstat conntrack libseccomp -y # 加载模块，最后一条4.18及以下内核使用nf_conntrack_ipv4，4.19已改为nf_conntrack modprobe -- ip_vs modprobe -- ip_vs_rr modprobe -- ip_vs_wrr modprobe -- ip_vs_sh modprobe -- nf_conntrack # 编写参数文件 cat \u003c\u003c \"EOF\" \u003e /etc/modules-load.d/ipvs.conf ip_vs ip_vs_lc ip_vs_wlc ip_vs_rr ip_vs_wrr ip_vs_lblc ip_vs_lblcr ip_vs_dh ip_vs_sh ip_vs_fo ip_vs_nq ip_vs_sed ip_vs_ftp ip_vs_sh nf_conntrack ip_tables ip_set xt_set ipt_set ipt_rpfilter ipt_REJECT ipip EOF # systemd-modules-load加入开机自启 systemctl enable --now systemd-modules-load # 自定义内核参数优化配置文件 cat \u003c\u003c \"EOF\" \u003e /etc/sysctl.d/kubernetes.conf net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 fs.may_detach_mounts = 1 net.ipv4.conf.all.route_localnet = 1 vm.overcommit_memory=1 vm.panic_on_oom=0 fs.inotify.max_user_watches=89100 fs.file-max=52706963 fs.nr_open=52706963 net.netfilter.nf_conntrack_max=2310720 net.ipv4.tcp_keepalive_time = 600 net.ipv4.tcp_keepalive_probes = 3 net.ipv4.tcp_keepalive_intvl =15 net.ipv4.tcp_max_tw_buckets = 36000 net.ipv4.tcp_tw_reuse = 1 net.ipv4.tcp_max_orphans = 327680 net.ipv4.tcp_orphan_retries = 3 net.ipv4.tcp_syncookies = 1 net.ipv4.tcp_max_syn_backlog = 16384 net.ipv4.ip_conntrack_max = 65536 net.ipv4.tcp_max_syn_backlog = 16384 net.ipv4.tcp_timestamps = 0 net.core.somaxconn = 16384 EOF # 加载 sysctl --system # 重启查看IPVS模块是否依旧加载 reboot lsmod | grep -e ip_vs -e nf_conntrack 保证每台服务器中IPVS加载成功，以k8s-master01为例，如图： 部署Containerd(ALL) Kubernetes1.24版本以后将不再支持Docker作为Runtime，本文安装使用Containerd作为Runtime。\n# 配置阿里docker源 yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo # 安装最新版本docker和containerd.io，安装docker是为了使用docker CLI yum install docker-ce docker-ce-cli containerd.io -y # （可选）也可以按需安装指定版本 yum install docker-ce-20.10.* docker-ce-cli-20.10.* containerd -y # 配置Containerd模块 cat \u003c\u003c \"EOF\" \u003e /etc/modules-load.d/containerd.conf overlay br_netfilter EOF # 加载模块 modprobe -- overlay modprobe -- br_netfilter # 配置内核参数 cat \u003c\u003c \"EOF\" \u003e /etc/sysctl.d/containerd.conf net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 EOF # 加载内核参数 sysctl --system # 生成默认配置文件 mkdir -p /etc/containerd containerd config default | tee /etc/containerd/config.toml # 更改Cgroup为Systemd，在containerd.runtimes.runc.options行后的SystemdCgroup = false修改为true，如果配置项不存在就自行添加，缩进俩空格添加SystemdCgroup = true一行 vim /etc/containerd/config.toml ... [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options] SystemdCgroup = true ... # 将sandbox_image的Pause镜像地址改成国内：registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.7 vim /etc/containerd/config.toml sandbox_image = \"registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.7\" # （可选）也可以在k8s-master01编辑/etc/containerd/config.toml文件后，将编辑后同名文件同步到其他服务器，自动覆盖 for i in k8s-master02 k8s-master03 k8s-node01 k8s-node02;do scp /etc/containerd/config.toml $i:/etc/containerd/config.toml;done # 查看确认是否配置成功 cat /etc/containerd/config.toml | grep -e Systemd -e sandbox_image # 启动并加入开机自启 systemctl daemon-reload systemctl enable --now containerd # containerd运行时的CLI是ctr [root@k8s-master01 ~]# ctr images ls REF TYPE DIGEST SIZE PLATFORMS LABELS [root@k8s-master01 ~]# ctr version Client: Version: 1.6.20 Revision: 2806fc1057397dbaeefbea0e4e17bddfbd388f38 Go version: go1.19.7 Server: Version: 1.6.20 Revision: 2806fc1057397dbaeefbea0e4e17bddfbd388f38 UUID: 9958928e-300c-4778-b83c-6c0073414f3e # 配置crictl连接的运行时socket接口（指向containerd's GRPC server：/run/containerd/containerd.sock） # crictl 默认连接到unix:///var/run/dockershim.sock cat \u003c\u003c \"EOF\" \u003e /etc/crictl.yaml runtime-endpoint: unix:///run/containerd/containerd.sock image-endpoint: unix:///run/containerd/containerd.sock timeout: 10 debug: false EOF # crictl按需选择，https://github.com/kubernetes-sigs/cri-tools/releases # containerd容器管理CLI是crictl，该命令使用和docker命令类似，下载包上传到k8s-master01 [root@k8s-master01 ~]# tar -zxvf crictl-v1.27.0-linux-amd64.tar.gz [root@k8s-master01 ~]# mv crictl /usr/local/bin/ [root@k8s-master01 ~]# crictl version Version: 0.1.0 RuntimeName: containerd RuntimeVersion: 1.6.20 RuntimeApiVersion: v1 # 二进制可执行文件发送到其他节点 [root@k8s-master01 ~]# for i in k8s-master02 k8s-master03 k8s-node01 k8s-node02;do scp /usr/local/bin/crictl $i:/usr/local/bin/;done crictl 是CRI（Container Runtime Interface）兼容的容器运行时的CLI（Command-Line Interface）。可以这个命令来检查和调试 Kubernetes 节点上的容器运行时和应用程序。介绍和安装方式可见：critools或Kubernetes官方介绍\n二进制包和证书 k8s-master01节点上下载并安装Kubernetes二进制安装包（server-binaries，选择对应的架构即可）和ETCD二进制安装包，可在GitHub上查看Kubernetes1.23.x版本的信息，官方GitHub-Kubernetes1.23版本链接，ETCD官方链接\n# 以下在k8s-master01执行 # 下载太慢的话可以在相应链接网页上下载好传到服务器 wget https://dl.k8s.io/v1.26.4/kubernetes-server-linux-amd64.tar.gz wget https://github.com/etcd-io/etcd/releases/download/v3.5.6/etcd-v3.5.6-linux-amd64.tar.gz # 解压安装，--strip-components=N表示解压时忽略解压后的N层目录，直接获取N层目录后的目标文件。kubernetes/server/bin/是三层，etcd-v3.5.6-linux-amd64/是一层 tar -zxvf kubernetes-server-linux-amd64.tar.gz --strip-components=3 -C /usr/local/bin kubernetes/server/bin/kube{let,ctl,-apiserver,-controller-manager,-scheduler,-proxy} tar -zxvf etcd-v3.5.6-linux-amd64.tar.gz --strip-components=1 -C /usr/local/bin etcd-v3.5.6-linux-amd64/etcd{,ctl} # 确认版本 kubectl version kubelet --version etcdctl version # 拷贝组件到其他节点，Node节点只需要kubelet和kube-proxy for i in k8s-master02 k8s-master03; do scp /usr/local/bin/kube{let,ctl,-apiserver,-controller-manager,-scheduler,-proxy} $i:/usr/local/bin/; scp /usr/local/bin/etcd* $i:/usr/local/bin/; done for i in k8s-node01 k8s-node02; do scp /usr/local/bin/kube{let,-proxy} $i:/usr/local/bin/; done 配置证书 # master节点创建etcd证书目录 mkdir -p /etc/etcd/ssl # 所有节点创建pki证书目录和CNI目录（后面calico使用） mkdir -p /etc/kubernetes/pki mkdir -p /opt/cni/bin # 以下在k8s-master01操作 # 安装证书生成工具，如果速度慢可以浏览器下载后上传至服务器改名即可 wget \"https://pkg.cfssl.org/R1.2/cfssl_linux-amd64\" -O /usr/local/bin/cfssl wget \"https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64\" -O /usr/local/bin/cfssljson chmod +x /usr/local/bin/cfssl /usr/local/bin/cfssljson # 在master01节点生成etcd证书 cd /root/k8s-ha-install/pki cfssl gencert -initca etcd-ca-csr.json | cfssljson -bare /etc/etcd/ssl/etcd-ca cfssl gencert \\ -ca=/etc/etcd/ssl/etcd-ca.pem \\ -ca-key=/etc/etcd/ssl/etcd-ca-key.pem \\ -config=ca-config.json \\ -hostname=127.0.0.1,k8s-master01,k8s-master02,k8s-master03,192.168.43.201,192.168.43.202,192.168.43.203 \\ -profile=kubernetes \\ etcd-csr.json | cfssljson -bare /etc/etcd/ssl/etcd # 复制etcd证书到其他master节点 for i in k8s-master02 k8s-master03; do for FILE in etcd-ca-key.pem etcd-ca.pem etcd-key.pem etcd.pem; do scp /etc/etcd/ssl/${FILE} $i:/etc/etcd/ssl/${FILE} done done # 生成Kubernetes集群ca证书 cd /root/k8s-ha-install/pki cfssl gencert -initca ca-csr.json | cfssljson -bare /etc/kubernetes/pki/ca # k8s service网段10.96.0.0/12，填入网段第一个IP；集群VIP地址192.168.43.200 cfssl gencert -ca=/etc/kubernetes/pki/ca.pem -ca-key=/etc/kubernetes/pki/ca-key.pem -config=ca-config.json -hostname=10.96.0.1,192.168.43.200,127.0.0.1,kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.default.svc.cluster.local,192.168.43.201,192.168.43.202,192.168.43.203 -profile=kubernetes apiserver-csr.json | cfssljson -bare /etc/kubernetes/pki/apiserver # 生成apiserver的第三方组件使用的聚合证书，告警可以忽略 cfssl gencert -initca front-proxy-ca-csr.json | cfssljson -bare /etc/kubernetes/pki/front-proxy-ca cfssl gencert -ca=/etc/kubernetes/pki/front-proxy-ca.pem -ca-key=/etc/kubernetes/pki/front-proxy-ca-key.pem -config=ca-config.json -profile=kubernetes front-proxy-client-csr.json | cfssljson -bare /etc/kubernetes/pki/front-proxy-client # 生成controller-manager证书 cfssl gencert \\ -ca=/etc/kubernetes/pki/ca.pem \\ -ca-key=/etc/kubernetes/pki/ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ manager-csr.json | cfssljson -bare /etc/kubernetes/pki/controller-manager # 设置集群信息，集群名：kubernetes server：https://192.168.43.200:8443 # 如果不是高可用集群，192.168.43.200:8443改为master01的地址，8443改为apiserver的端口，默认是6443 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/pki/ca.pem \\ --embed-certs=true \\ --server=https://192.168.43.200:8443 \\ --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig # 设置上下文信息，context为system:kube-controller-manager@kubernetes kubectl config set-context system:kube-controller-manager@kubernetes \\ --cluster=kubernetes \\ --user=system:kube-controller-manager \\ --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig # 设置用户认证信息，用户为system:kube-controller-manager kubectl config set-credentials system:kube-controller-manager \\ --client-certificate=/etc/kubernetes/pki/controller-manager.pem \\ --client-key=/etc/kubernetes/pki/controller-manager-key.pem \\ --embed-certs=true \\ --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig # 设置默认集群环境 kubectl config use-context system:kube-controller-manager@kubernetes \\ --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig # 生成scheduler证书 cfssl gencert \\ -ca=/etc/kubernetes/pki/ca.pem \\ -ca-key=/etc/kubernetes/pki/ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ scheduler-csr.json | cfssljson -bare /etc/kubernetes/pki/scheduler # 同样的，scheduler需要和controller-manager设置相同的集群上下文等信息 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/pki/ca.pem \\ --embed-certs=true \\ --server=https://192.168.43.200:8443 \\ --kubeconfig=/etc/kubernetes/scheduler.kubeconfig kubectl config set-credentials system:kube-scheduler \\ --client-certificate=/etc/kubernetes/pki/scheduler.pem \\ --client-key=/etc/kubernetes/pki/scheduler-key.pem \\ --embed-certs=true \\ --kubeconfig=/etc/kubernetes/scheduler.kubeconfig kubectl config set-context system:kube-scheduler@kubernetes \\ --cluster=kubernetes \\ --user=system:kube-scheduler \\ --kubeconfig=/etc/kubernetes/scheduler.kubeconfig kubectl config use-context system:kube-scheduler@kubernetes \\ --kubeconfig=/etc/kubernetes/scheduler.kubeconfig # 配置admin证书，以及集群上下文等信息 cfssl gencert \\ -ca=/etc/kubernetes/pki/ca.pem \\ -ca-key=/etc/kubernetes/pki/ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ admin-csr.json | cfssljson -bare /etc/kubernetes/pki/admin kubectl config set-cluster kubernetes --certificate-authority=/etc/kubernetes/pki/ca.pem --embed-certs=true --server=https://192.168.43.200:8443 --kubeconfig=/etc/kubernetes/admin.kubeconfig kubectl config set-credentials kubernetes-admin --client-certificate=/etc/kubernetes/pki/admin.pem --client-key=/etc/kubernetes/pki/admin-key.pem --embed-certs=true --kubeconfig=/etc/kubernetes/admin.kubeconfig kubectl config set-context kubernetes-admin@kubernetes --cluster=kubernetes --user=kubernetes-admin --kubeconfig=/etc/kubernetes/admin.kubeconfig kubectl config use-context kubernetes-admin@kubernetes --kubeconfig=/etc/kubernetes/admin.kubeconfig # 创建ServiceAccount密钥对 openssl genrsa -out /etc/kubernetes/pki/sa.key 2048 openssl rsa -in /etc/kubernetes/pki/sa.key -pubout -out /etc/kubernetes/pki/sa.pub # 拷贝证书到其他master节点 for i in k8s-master02 k8s-master03; do for FILE in $(ls /etc/kubernetes/pki | grep -v etcd); do scp /etc/kubernetes/pki/${FILE} $i:/etc/kubernetes/pki/${FILE}; done; for FILE in admin.kubeconfig controller-manager.kubeconfig scheduler.kubeconfig; do scp /etc/kubernetes/${FILE} $i:/etc/kubernetes/${FILE}; done; done # 查看证书数量是否为23 ls /etc/kubernetes/pki/ | wc -l 23 ETCD集群(Master) 如果Etcd集群服务器和Kubernetes集群服务器不重合（即独立的Etcd集群），需要根据实际情况配置集群IP。\n# master01 cat \u003c\u003c \"EOF\" \u003e /etc/etcd/etcd.config.yml name: 'k8s-master01' data-dir: /var/lib/etcd wal-dir: /var/lib/etcd/wal snapshot-count: 5000 heartbeat-interval: 100 election-timeout: 1000 quota-backend-bytes: 0 listen-peer-urls: 'https://192.168.43.201:2380' listen-client-urls: 'https://192.168.43.201:2379,http://127.0.0.1:2379' max-snapshots: 3 max-wals: 5 cors: initial-advertise-peer-urls: 'https://192.168.43.201:2380' advertise-client-urls: 'https://192.168.43.201:2379' discovery: discovery-fallback: 'proxy' discovery-proxy: discovery-srv: initial-cluster: 'k8s-master01=https://192.168.43.201:2380,k8s-master02=https://192.168.43.202:2380,k8s-master03=https://192.168.43.203:2380' initial-cluster-token: 'etcd-k8s-cluster' initial-cluster-state: 'new' strict-reconfig-check: false enable-v2: true enable-pprof: true proxy: 'off' proxy-failure-wait: 5000 proxy-refresh-interval: 30000 proxy-dial-timeout: 1000 proxy-write-timeout: 5000 proxy-read-timeout: 0 client-transport-security: cert-file: '/etc/kubernetes/pki/etcd/etcd.pem' key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem' client-cert-auth: true trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem' auto-tls: true peer-transport-security: cert-file: '/etc/kubernetes/pki/etcd/etcd.pem' key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem' peer-client-cert-auth: true trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem' auto-tls: true debug: false log-package-levels: log-outputs: [default] force-new-cluster: false EOF # master02 cat \u003c\u003c \"EOF\" \u003e /etc/etcd/etcd.config.yml name: 'k8s-master02' data-dir: /var/lib/etcd wal-dir: /var/lib/etcd/wal snapshot-count: 5000 heartbeat-interval: 100 election-timeout: 1000 quota-backend-bytes: 0 listen-peer-urls: 'https://192.168.43.202:2380' listen-client-urls: 'https://192.168.43.202:2379,http://127.0.0.1:2379' max-snapshots: 3 max-wals: 5 cors: initial-advertise-peer-urls: 'https://192.168.43.202:2380' advertise-client-urls: 'https://192.168.43.202:2379' discovery: discovery-fallback: 'proxy' discovery-proxy: discovery-srv: initial-cluster: 'k8s-master01=https://192.168.43.201:2380,k8s-master02=https://192.168.43.202:2380,k8s-master03=https://192.168.43.203:2380' initial-cluster-token: 'etcd-k8s-cluster' initial-cluster-state: 'new' strict-reconfig-check: false enable-v2: true enable-pprof: true proxy: 'off' proxy-failure-wait: 5000 proxy-refresh-interval: 30000 proxy-dial-timeout: 1000 proxy-write-timeout: 5000 proxy-read-timeout: 0 client-transport-security: cert-file: '/etc/kubernetes/pki/etcd/etcd.pem' key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem' client-cert-auth: true trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem' auto-tls: true peer-transport-security: cert-file: '/etc/kubernetes/pki/etcd/etcd.pem' key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem' peer-client-cert-auth: true trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem' auto-tls: true debug: false log-package-levels: log-outputs: [default] force-new-cluster: false EOF # master03 cat \u003c\u003c \"EOF\" \u003e /etc/etcd/etcd.config.yml name: 'k8s-master03' data-dir: /var/lib/etcd wal-dir: /var/lib/etcd/wal snapshot-count: 5000 heartbeat-interval: 100 election-timeout: 1000 quota-backend-bytes: 0 listen-peer-urls: 'https://192.168.43.203:2380' listen-client-urls: 'https://192.168.43.203:2379,http://127.0.0.1:2379' max-snapshots: 3 max-wals: 5 cors: initial-advertise-peer-urls: 'https://192.168.43.203:2380' advertise-client-urls: 'https://192.168.43.203:2379' discovery: discovery-fallback: 'proxy' discovery-proxy: discovery-srv: initial-cluster: 'k8s-master01=https://192.168.43.201:2380,k8s-master02=https://192.168.43.202:2380,k8s-master03=https://192.168.43.203:2380' initial-cluster-token: 'etcd-k8s-cluster' initial-cluster-state: 'new' strict-reconfig-check: false enable-v2: true enable-pprof: true proxy: 'off' proxy-failure-wait: 5000 proxy-refresh-interval: 30000 proxy-dial-timeout: 1000 proxy-write-timeout: 5000 proxy-read-timeout: 0 client-transport-security: cert-file: '/etc/kubernetes/pki/etcd/etcd.pem' key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem' client-cert-auth: true trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem' auto-tls: true peer-transport-security: cert-file: '/etc/kubernetes/pki/etcd/etcd.pem' key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem' peer-client-cert-auth: true trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem' auto-tls: true debug: false log-package-levels: log-outputs: [default] force-new-cluster: false EOF # 在所有master节点创建etcd服务并启动 cat \u003c\u003c \"EOF\" \u003e /usr/lib/systemd/system/etcd.service [Unit] Description=Etcd Service Documentation=https://coreos.com/etcd/docs/latest/ After=network.target [Service] Type=notify ExecStart=/usr/local/bin/etcd --config-file=/etc/etcd/etcd.config.yml Restart=on-failure RestartSec=10 LimitNOFILE=65536 [Install] WantedBy=multi-user.target Alias=etcd3.service EOF # 所有Master节点创建etcd证书目录并链接证书，否则启动会失败 mkdir /etc/kubernetes/pki/etcd ln -s /etc/etcd/ssl/* /etc/kubernetes/pki/etcd/ # 启动 systemctl daemon-reload \u0026\u0026 systemctl enable --now etcd # 查看etcd集群状态如下即可 ETCDCTL_API=3 etcdctl --endpoints=\"192.168.43.203:2379,192.168.43.202:2379,192.168.43.201:2379\" --cacert=/etc/kubernetes/pki/etcd/etcd-ca.pem --cert=/etc/kubernetes/pki/etcd/etcd.pem --key=/etc/kubernetes/pki/etcd/etcd-key.pem endpoint status --write-out=table +---------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS | +---------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ | 192.168.43.203:2379 | fd1372a073304e | 3.5.1 | 20 kB | false | false | 2 | 9 | 9 | | | 192.168.43.202:2379 | 837ce9c47e0719eb | 3.5.1 | 20 kB | false | false | 2 | 9 | 9 | | | 192.168.43.201:2379 | e9bf8d99824c9061 | 3.5.1 | 20 kB | true | false | 2 | 9 | 9 | | +---------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ 高可用组件(Master) # 所有master节点安装Keepalived和haproxy，并创建配置文件目录 yum install keepalived haproxy -y mkdir /etc/haproxy mkdir /etc/keepalived # 为所有master节点添加haproxy配置，配置都一样，检查最后三行主机名和IP地址对应上就行 cat \u003c\u003c \"EOF\" \u003e /etc/haproxy/haproxy.cfg global maxconn 2000 ulimit-n 16384 log 127.0.0.1 local0 err stats timeout 30s defaults log global mode http option httplog timeout connect 5000 timeout client 50000 timeout server 50000 timeout http-request 15s timeout http-keep-alive 15s frontend monitor-in bind *:33305 mode http option httplog monitor-uri /monitor frontend k8s-master bind 0.0.0.0:8443 bind 127.0.0.1:8443 mode tcp option tcplog tcp-request inspect-delay 5s default_backend k8s-master backend k8s-master mode tcp option tcplog option tcp-check balance roundrobin default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100 server k8s-master01 192.168.43.201:6443 check server k8s-master02 192.168.43.202:6443 check server k8s-master03 192.168.43.203:6443 check EOF # keepalived配置不一样，注意区分网卡名、IP地址和虚拟IP地址 # 检查服务器网卡名 ip a 或 ifconfig # k8s-master01 Keepalived配置 cat \u003c\u003c \"EOF\" \u003e /etc/keepalived/keepalived.conf ! Configuration File for keepalived global_defs { router_id LVS_DEVEL script_user root enable_script_security } vrrp_script chk_apiserver { script \"/etc/keepalived/check_apiserver.sh\" interval 5 weight -5 fall 2 rise 1 } vrrp_instance VI_1 { state MASTER interface ens33 mcast_src_ip 192.168.43.201 virtual_router_id 51 priority 101 advert_int 2 authentication { auth_type PASS auth_pass K8SHA_KA_AUTH } virtual_ipaddress { 192.168.43.200 } track_script { chk_apiserver } } EOF # k8s-master02 Keepalived配置 cat \u003c\u003c \"EOF\" \u003e /etc/keepalived/keepalived.conf ! Configuration File for keepalived global_defs { router_id LVS_DEVEL script_user root enable_script_security } vrrp_script chk_apiserver { script \"/etc/keepalived/check_apiserver.sh\" interval 5 weight -5 fall 2 rise 1 } vrrp_instance VI_1 { state BACKUP interface ens33 mcast_src_ip 192.168.43.202 virtual_router_id 51 priority 100 advert_int 2 authentication { auth_type PASS auth_pass K8SHA_KA_AUTH } virtual_ipaddress { 192.168.43.200 } track_script { chk_apiserver } } EOF # k8s-master03 Keepalived配置 cat \u003c\u003c \"EOF\" \u003e /etc/keepalived/keepalived.conf ! Configuration File for keepalived global_defs { router_id LVS_DEVEL script_user root enable_script_security } vrrp_script chk_apiserver { script \"/etc/keepalived/check_apiserver.sh\" interval 5 weight -5 fall 2 rise 1 } vrrp_instance VI_1 { state BACKUP interface ens33 mcast_src_ip 192.168.43.203 virtual_router_id 51 priority 100 advert_int 2 authentication { auth_type PASS auth_pass K8SHA_KA_AUTH } virtual_ipaddress { 192.168.43.200 } track_script { chk_apiserver } } EOF # 所有master节点配置Keepalived健康检查脚本 cat \u003c\u003c \"EOF\" \u003e /etc/keepalived/check_apiserver.sh #!/bin/bash err=0 for k in $(seq 1 3) do check_code=$(pgrep haproxy) if [[ $check_code == \"\" ]]; then err=$(expr $err + 1) sleep 1 continue else err=0 break fi done if [[ $err != \"0\" ]]; then echo \"systemctl stop keepalived\" /usr/bin/systemctl stop keepalived exit 1 else exit 0 fi EOF # 赋予可执行权限 chmod +x /etc/keepalived/check_apiserver.sh # 启动haproxy和Keepalived并加入开机启动 systemctl daemon-reload \u0026\u0026 systemctl enable --now haproxy \u0026\u0026 systemctl enable --now keepalived # 检查服务是否正常 # 这种告警可以忽略：Mar 06 14:03:35 k8s-master01 haproxy-systemd-wrapper[1981]: [WARNING] 064/140335 (1982) : config : frontend 'GLOBAL' has no 'bind' systemctl status haproxy systemctl status keepalived # 测试一波 telnet k8s-master-vip 8443 ping k8s-master-vip 配置集群组件 # 所有节点创建以下集群资源目录 mkdir -p /etc/kubernetes/manifests/ /etc/systemd/system/kubelet.service.d /var/lib/kubelet /var/log/kubernetes Apiserver(Master)\n所有master节点配置kube-apiserver服务 # service网段为10.96.0.0/12，可自行设置，其他参数按需配置即可 # master01配置 cat \u003c\u003c \"EOF\" \u003e /usr/lib/systemd/system/kube-apiserver.service [Unit] Description=Kubernetes API Server Documentation=https://github.com/kubernetes/kubernetes After=network.target [Service] ExecStart=/usr/local/bin/kube-apiserver \\ --v=2 \\ --allow-privileged=true \\ --bind-address=0.0.0.0 \\ --secure-port=6443 \\ --advertise-address=192.168.43.201 \\ --service-cluster-ip-range=10.96.0.0/12 \\ --service-node-port-range=30000-32767 \\ --etcd-servers=https://192.168.43.201:2379,https://192.168.43.202:2379,https://192.168.43.203:2379 \\ --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem \\ --etcd-certfile=/etc/etcd/ssl/etcd.pem \\ --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \\ --client-ca-file=/etc/kubernetes/pki/ca.pem \\ --tls-cert-file=/etc/kubernetes/pki/apiserver.pem \\ --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem \\ --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem \\ --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem \\ --service-account-key-file=/etc/kubernetes/pki/sa.pub \\ --service-account-signing-key-file=/etc/kubernetes/pki/sa.key \\ --service-account-issuer=https://kubernetes.default.svc.cluster.local \\ --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname \\ --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota \\ --authorization-mode=Node,RBAC \\ --enable-bootstrap-token-auth=true \\ --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem \\ --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem \\ --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem \\ --requestheader-allowed-names=aggregator \\ --requestheader-group-headers=X-Remote-Group \\ --requestheader-extra-headers-prefix=X-Remote-Extra- \\ --requestheader-username-headers=X-Remote-User # --token-auth-file=/etc/kubernetes/token.csv Restart=on-failure RestartSec=10s LimitNOFILE=65535 [Install] WantedBy=multi-user.target EOF # master02配置 cat \u003c\u003c \"EOF\" \u003e /usr/lib/systemd/system/kube-apiserver.service [Unit] Description=Kubernetes API Server Documentation=https://github.com/kubernetes/kubernetes After=network.target [Service] ExecStart=/usr/local/bin/kube-apiserver \\ --v=2 \\ --allow-privileged=true \\ --bind-address=0.0.0.0 \\ --secure-port=6443 \\ --advertise-address=192.168.43.202 \\ --service-cluster-ip-range=10.96.0.0/12 \\ --service-node-port-range=30000-32767 \\ --etcd-servers=https://192.168.43.201:2379,https://192.168.43.202:2379,https://192.168.43.203:2379 \\ --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem \\ --etcd-certfile=/etc/etcd/ssl/etcd.pem \\ --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \\ --client-ca-file=/etc/kubernetes/pki/ca.pem \\ --tls-cert-file=/etc/kubernetes/pki/apiserver.pem \\ --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem \\ --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem \\ --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem \\ --service-account-key-file=/etc/kubernetes/pki/sa.pub \\ --service-account-signing-key-file=/etc/kubernetes/pki/sa.key \\ --service-account-issuer=https://kubernetes.default.svc.cluster.local \\ --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname \\ --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota \\ --authorization-mode=Node,RBAC \\ --enable-bootstrap-token-auth=true \\ --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem \\ --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem \\ --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem \\ --requestheader-allowed-names=aggregator \\ --requestheader-group-headers=X-Remote-Group \\ --requestheader-extra-headers-prefix=X-Remote-Extra- \\ --requestheader-username-headers=X-Remote-User # --token-auth-file=/etc/kubernetes/token.csv Restart=on-failure RestartSec=10s LimitNOFILE=65535 [Install] WantedBy=multi-user.target EOF # master03配置 cat \u003c\u003c \"EOF\" \u003e /usr/lib/systemd/system/kube-apiserver.service [Unit] Description=Kubernetes API Server Documentation=https://github.com/kubernetes/kubernetes After=network.target [Service] ExecStart=/usr/local/bin/kube-apiserver \\ --v=2 \\ --allow-privileged=true \\ --bind-address=0.0.0.0 \\ --secure-port=6443 \\ --advertise-address=192.168.43.203 \\ --service-cluster-ip-range=10.96.0.0/12 \\ --service-node-port-range=30000-32767 \\ --etcd-servers=https://192.168.43.201:2379,https://192.168.43.202:2379,https://192.168.43.203:2379 \\ --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem \\ --etcd-certfile=/etc/etcd/ssl/etcd.pem \\ --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \\ --client-ca-file=/etc/kubernetes/pki/ca.pem \\ --tls-cert-file=/etc/kubernetes/pki/apiserver.pem \\ --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem \\ --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem \\ --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem \\ --service-account-key-file=/etc/kubernetes/pki/sa.pub \\ --service-account-signing-key-file=/etc/kubernetes/pki/sa.key \\ --service-account-issuer=https://kubernetes.default.svc.cluster.local \\ --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname \\ --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota \\ --authorization-mode=Node,RBAC \\ --enable-bootstrap-token-auth=true \\ --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem \\ --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem \\ --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem \\ --requestheader-allowed-names=aggregator \\ --requestheader-group-headers=X-Remote-Group \\ --requestheader-extra-headers-prefix=X-Remote-Extra- \\ --requestheader-username-headers=X-Remote-User # --token-auth-file=/etc/kubernetes/token.csv Restart=on-failure RestartSec=10s LimitNOFILE=65535 [Install] WantedBy=multi-user.target EOF # 启动kube-apiserver systemctl daemon-reload \u0026\u0026 systemctl enable --now kube-apiserver ControllerManager(Master)\n所有Master节点配置kube-controller-manager服务，配置都一样\n# Pod网段是172.16.0.0/12，可按需更改，不要和其他在用网段冲突即可 # 给所有master节点配置服务 cat \u003c\u003c \"EOF\" \u003e /usr/lib/systemd/system/kube-controller-manager.service [Unit] Description=Kubernetes Controller Manager Documentation=https://github.com/kubernetes/kubernetes After=network.target [Service] ExecStart=/usr/local/bin/kube-controller-manager \\ --v=2 \\ --root-ca-file=/etc/kubernetes/pki/ca.pem \\ --cluster-signing-cert-file=/etc/kubernetes/pki/ca.pem \\ --cluster-signing-key-file=/etc/kubernetes/pki/ca-key.pem \\ --service-account-private-key-file=/etc/kubernetes/pki/sa.key \\ --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig \\ --feature-gates=LegacyServiceAccountTokenNoAutoGeneration=false \\ --leader-elect=true \\ --use-service-account-credentials=true \\ --node-monitor-grace-period=40s \\ --node-monitor-period=5s \\ --pod-eviction-timeout=2m0s \\ --controllers=*,bootstrapsigner,tokencleaner \\ --allocate-node-cidrs=true \\ --cluster-cidr=172.16.0.0/12 \\ --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem \\ --node-cidr-mask-size=24 Restart=always RestartSec=10s [Install] WantedBy=multi-user.target EOF # 启动 systemctl daemon-reload \u0026\u0026 systemctl enable --now kube-controller-manager Scheduler(Master)\n所有Master节点配置kube-scheduler服务，配置都一样\ncat \u003c\u003c \"EOF\" \u003e /usr/lib/systemd/system/kube-scheduler.service [Unit] Description=Kubernetes Scheduler Documentation=https://github.com/kubernetes/kubernetes After=network.target [Service] ExecStart=/usr/local/bin/kube-scheduler \\ --v=2 \\ --leader-elect=true \\ --authentication-kubeconfig=/etc/kubernetes/scheduler.kubeconfig \\ --authorization-kubeconfig=/etc/kubernetes/scheduler.kubeconfig \\ --kubeconfig=/etc/kubernetes/scheduler.kubeconfig Restart=always RestartSec=10s [Install] WantedBy=multi-user.target EOF # 启动 systemctl daemon-reload \u0026\u0026 systemctl enable --now kube-scheduler 确认集群组件状态\n# 所有Master节点均检查，-l参数输出完整信息 # 如果有E开头的报错，需要排查解决一下，常见问题是IP冲突、证书错误 # W告警可暂时忽略 systemctl status kube-apiserver -l systemctl status kube-controller-manager -l systemctl status kube-scheduler -l 配置TLS Bootstrapping 只需在master01节点配置，TLS Bootstrapping的官方说明请见：TLS Bootstrapping\ncd /root/k8s-ha-install/bootstrap # 查看一下secret，name后要和token-id一致，token-id.token-secret和集群设置--token=对应上 [root@k8s-master01 bootstrap]# cat bootstrap.secret.yaml apiVersion: v1 kind: Secret metadata: name: bootstrap-token-c8ad9c namespace: kube-system type: bootstrap.kubernetes.io/token stringData: description: \"The default bootstrap token generated by 'kubelet '.\" token-id: c8ad9c token-secret: 2e4d610cf3e7426e ... kubectl config set-cluster kubernetes --certificate-authority=/etc/kubernetes/pki/ca.pem --embed-certs=true --server=https://192.168.43.200:8443 --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig kubectl config set-credentials tls-bootstrap-token-user --token=c8ad9c.2e4d610cf3e7426e --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig kubectl config set-context tls-bootstrap-token-user@kubernetes --cluster=kubernetes --user=tls-bootstrap-token-user --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig kubectl config use-context tls-bootstrap-token-user@kubernetes --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig # 创建配置目录，拷贝admin.kubeconfig文件为config，授权kubectl，使得当前用户可以使用kubectl创建资源 # 其他master节点如果需要使用kubectl命令创建资源，也可以拷贝文件过去 # 下面是没有授权的情况 # [root@k8s-master01 bootstrap]# kubectl get cs # The connection to the server localhost:8080 was refused - did you specify the right host or port? mkdir -p /root/.kube;cp /etc/kubernetes/admin.kubeconfig /root/.kube/config # 授权后查看集群状态 [root@k8s-master01 bootstrap]# kubectl get cs Warning: v1 ComponentStatus is deprecated in v1.19+ NAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy {\"health\":\"true\",\"reason\":\"\"} etcd-1 Healthy {\"health\":\"true\",\"reason\":\"\"} etcd-2 Healthy {\"health\":\"true\",\"reason\":\"\"} # 创建bootstrap-secret [root@k8s-master01 bootstrap]# kubectl apply -f bootstrap.secret.yaml secret/bootstrap-token-c8ad9c created clusterrolebinding.rbac.authorization.k8s.io/kubelet-bootstrap created clusterrolebinding.rbac.authorization.k8s.io/node-autoapprove-bootstrap created clusterrolebinding.rbac.authorization.k8s.io/node-autoapprove-certificate-rotation created clusterrole.rbac.authorization.k8s.io/system:kube-apiserver-to-kubelet created clusterrolebinding.rbac.authorization.k8s.io/system:kube-apiserver created 配置Kubelet # 从master01拷贝证书文件到其他节点 cd /etc/kubernetes/ for i in k8s-master02 k8s-master03 k8s-node01 k8s-node02; do ssh $i mkdir -p /etc/kubernetes/pki for FILE in pki/ca.pem pki/ca-key.pem pki/front-proxy-ca.pem bootstrap-kubelet.kubeconfig; do scp /etc/kubernetes/$FILE $i:/etc/kubernetes/${FILE} done done # 所有节点配置kubelet服务 cat \u003c\u003c \"EOF\" \u003e /usr/lib/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kubelet Restart=always StartLimitInterval=0 RestartSec=10 [Install] WantedBy=multi-user.target EOF # Runtime为Containerd，kubelet服务的配置文件 cat \u003c\u003c \"EOF\" \u003e /etc/systemd/system/kubelet.service.d/10-kubelet.conf [Service] Environment=\"KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig --kubeconfig=/etc/kubernetes/kubelet.kubeconfig\" Environment=\"KUBELET_SYSTEM_ARGS=--container-runtime=remote --runtime-request-timeout=15m --container-runtime-endpoint=unix:///run/containerd/containerd.sock\" Environment=\"KUBELET_CONFIG_ARGS=--config=/etc/kubernetes/kubelet-conf.yml\" Environment=\"KUBELET_EXTRA_ARGS=--node-labels=node.kubernetes.io/node='' \" ExecStart= ExecStart=/usr/local/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_SYSTEM_ARGS $KUBELET_EXTRA_ARGS EOF # 创建kubelet配置文件，对应上面Environment配置KUBELET_CONFIG_ARGS # clusterDNS配置为service网段的第十个地址 cat \u003c\u003c \"EOF\" \u003e /etc/kubernetes/kubelet-conf.yml apiVersion: kubelet.config.k8s.io/v1beta1 kind: KubeletConfiguration address: 0.0.0.0 port: 10250 readOnlyPort: 10255 authentication: anonymous: enabled: false webhook: cacheTTL: 2m0s enabled: true x509: clientCAFile: /etc/kubernetes/pki/ca.pem authorization: mode: Webhook webhook: cacheAuthorizedTTL: 5m0s cacheUnauthorizedTTL: 30s cgroupDriver: systemd cgroupsPerQOS: true clusterDNS: - 10.96.0.10 clusterDomain: cluster.local containerLogMaxFiles: 5 containerLogMaxSize: 10Mi contentType: application/vnd.kubernetes.protobuf cpuCFSQuota: true cpuManagerPolicy: none cpuManagerReconcilePeriod: 10s enableControllerAttachDetach: true enableDebuggingHandlers: true enforceNodeAllocatable: - pods eventBurst: 10 eventRecordQPS: 5 evictionHard: imagefs.available: 15% memory.available: 100Mi nodefs.available: 10% nodefs.inodesFree: 5% evictionPressureTransitionPeriod: 5m0s failSwapOn: true fileCheckFrequency: 20s hairpinMode: promiscuous-bridge healthzBindAddress: 127.0.0.1 healthzPort: 10248 httpCheckFrequency: 20s imageGCHighThresholdPercent: 85 imageGCLowThresholdPercent: 80 imageMinimumGCAge: 2m0s iptablesDropBit: 15 iptablesMasqueradeBit: 14 kubeAPIBurst: 10 kubeAPIQPS: 5 makeIPTablesUtilChains: true maxOpenFiles: 1000000 maxPods: 110 nodeStatusUpdateFrequency: 10s oomScoreAdj: -999 podPidsLimit: -1 registryBurst: 10 registryPullQPS: 5 resolvConf: /etc/resolv.conf rotateCertificates: true runtimeRequestTimeout: 2m0s serializeImagePulls: true staticPodPath: /etc/kubernetes/manifests streamingConnectionIdleTimeout: 4h0m0s syncFrequency: 1m0s volumeStatsAggPeriod: 1m0s EOF # 启动服务 systemctl daemon-reload \u0026\u0026 systemctl enable --now kubelet # 以k8s-master01为例查看运行日志和状态，只有CNI一处报错表示配置正确，后面CNI配置好后就会正常 tail -f /var/log/messages ... Apr 21 16:15:56 k8s-master01 kubelet: E0421 16:15:56.608747 7169 kubelet.go:2475] \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\" ... [root@k8s-master01 ~]# systemctl status kubelet ● kubelet.service - Kubernetes Kubelet Loaded: loaded (/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled) Drop-In: /etc/systemd/system/kubelet.service.d └─10-kubelet.conf Active: active (running) since 五 2023-04-21 16:09:00 CST; 6min ago Docs: https://github.com/kubernetes/kubernetes Main PID: 7169 (kubelet) Tasks: 11 Memory: 40.6M CGroup: /system.slice/kubelet.service └─7169 /usr/local/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig --kubeconfig=/etc/kuberne... 4月 21 16:14:26 k8s-master01 kubelet[7169]: E0421 16:14:26.578931 7169 kubelet.go:2475] \"Container runtime network not read...alized\" 4月 21 16:14:31 k8s-master01 kubelet[7169]: E0421 16:14:31.580077 7169 kubelet.go:2475] \"Container runtime network not read...alized\" .... # 并且此时节点状态应该是可查询且处于NotReady，CNI配置后就会ready [root@k8s-master01 ~]# kubectl get node NAME STATUS ROLES AGE VERSION k8s-master01 NotReady 7m12s v1.26.4 k8s-master02 NotReady 7m11s v1.26.4 k8s-master03 NotReady 7m10s v1.26.4 k8s-node01 NotReady 7m24s v1.26.4 k8s-node02 NotReady 7m12s v1.26.4 配置Kube-proxy # 只需在k8s-master01上执行 cd /root/k8s-ha-install kubectl -n kube-system create serviceaccount kube-proxy kubectl create clusterrolebinding system:kube-proxy --clusterrole system:node-proxier --serviceaccount kube-system:kube-proxy SECRET=$(kubectl -n kube-system get sa/kube-proxy \\ --output=jsonpath='{.secrets[0].name}') JWT_TOKEN=$(kubectl -n kube-system get secret/$SECRET \\ --output=jsonpath='{.data.token}' | base64 -d) PKI_DIR=/etc/kubernetes/pki K8S_DIR=/etc/kubernetes kubectl config set-cluster kubernetes --certificate-authority=/etc/kubernetes/pki/ca.pem --embed-certs=true --server=https://192.168.43.200:8443 --kubeconfig=${K8S_DIR}/kube-proxy.kubeconfig kubectl config set-credentials kubernetes --token=${JWT_TOKEN} --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig kubectl config set-context kubernetes --cluster=kubernetes --user=kubernetes --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig kubectl config use-context kubernetes --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig # 从k8s-master01将kubeconfig拷贝到其他节点 for i in k8s-master02 k8s-master03 k8s-node01 k8s-node02; do scp /etc/kubernetes/kube-proxy.kubeconfig $i:/etc/kubernetes/kube-proxy.kubeconfig done # 所有节点配置kube-proxy服务 cat \u003c\u003c \"EOF\" \u003e /usr/lib/systemd/system/kube-proxy.service [Unit] Description=Kubernetes Kube Proxy Documentation=https://github.com/kubernetes/kubernetes After=network.target [Service] ExecStart=/usr/local/bin/kube-proxy \\ --config=/etc/kubernetes/kube-proxy.yaml \\ --v=2 Restart=always RestartSec=10s [Install] WantedBy=multi-user.target EOF # 所有节点配置kube-proxy.yaml,clusterCIDR为pod网段 cat \u003c\u003c \"EOF\" \u003e /etc/kubernetes/kube-proxy.yaml apiVersion: kubeproxy.config.k8s.io/v1alpha1 bindAddress: 0.0.0.0 clientConnection: acceptContentTypes: \"\" burst: 10 contentType: application/vnd.kubernetes.protobuf kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig qps: 5 clusterCIDR: 172.16.0.0/12 configSyncPeriod: 15m0s conntrack: max: null maxPerCore: 32768 min: 131072 tcpCloseWaitTimeout: 1h0m0s tcpEstablishedTimeout: 24h0m0s enableProfiling: false healthzBindAddress: 0.0.0.0:10256 hostnameOverride: \"\" iptables: masqueradeAll: false masqueradeBit: 14 minSyncPeriod: 0s syncPeriod: 30s ipvs: masqueradeAll: true minSyncPeriod: 5s scheduler: \"rr\" syncPeriod: 30s kind: KubeProxyConfiguration metricsBindAddress: 127.0.0.1:10249 mode: \"ipvs\" nodePortAddresses: null oomScoreAdj: -999 portRange: \"\" udpIdleTimeout: 250ms EOF # 启动 systemctl daemon-reload \u0026\u0026 systemctl enable --now kube-proxy 配置Calico # k8s-master01上执行，更改calico中Pod网段为自己的 cd /root/k8s-ha-install/calico/ sed -i \"s#POD_CIDR#172.16.0.0/12#g\" calico.yaml # 检查是否更改成功 [root@k8s-master01 calico]# grep \"IPV4POOL_CIDR\" calico.yaml -A 1 - name: CALICO_IPV4POOL_CIDR value: \"172.16.0.0/12\" # 应用calico [root@k8s-master01 calico]# kubectl apply -f calico.yaml poddisruptionbudget.policy/calico-kube-controllers created poddisruptionbudget.policy/calico-typha created serviceaccount/calico-kube-controllers created serviceaccount/calico-node created configmap/calico-config created customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created clusterrole.rbac.authorization.k8s.io/calico-node created clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created clusterrolebinding.rbac.authorization.k8s.io/calico-node created service/calico-typha created daemonset.apps/calico-node created deployment.apps/calico-kube-controllers created deployment.apps/calico-typha created # （中间状态）在calico生效过程中查看各节点污点状态，发现均有不可调度污点，不必管他，等待calico生效即可，这种污点是kubernetes集群保护机制，在节点处于not ready状态时，节点不可调度 [root@k8s-master01 calico]# kubectl describe node| grep Taints: Taints: node.kubernetes.io/not-ready:NoSchedule Taints: node.kubernetes.io/not-ready:NoSchedule Taints: node.kubernetes.io/not-ready:NoSchedule Taints: node.kubernetes.io/not-ready:NoSchedule Taints: node.kubernetes.io/not-ready:NoSchedule # calico Pod处于Running后集群网络将建立，node将处于Ready状态，calico建立网络取决于电脑性能（硬件和网络环境），一般几分钟即可完成，性能差的可能会花费更长时间 [root@k8s-master01 calico]# kubectl get po -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-6bd6b69df9-5nwkl 1/1 Running 0 10m kube-system calico-node-8cvcd 1/1 Running 0 10m kube-system calico-node-96mx8 1/1 Running 1 (36s ago) 10m kube-system calico-node-f49rb 1/1 Running 0 10m kube-system calico-node-rgs7f 1/1 Running 0 10m kube-system calico-node-vzrls 1/1 Running 0 10m kube-system calico-typha-77fc8866f5-h9bsj 1/1 Running 0 10m # 查看集群状态和资源 [root@k8s-master01 calico]# kubectl get node NAME STATUS ROLES AGE VERSION k8s-master01 Ready 23m v1.26.4 k8s-master02 Ready 23m v1.26.4 k8s-master03 Ready 23m v1.26.4 k8s-node01 Ready 23m v1.26.4 k8s-node02 Ready 23m v1.26.4 配置CoreDNS 在k8s-master01操作 # 如果更改了k8s service的网段需要将coredns的serviceIP改成k8s service网段的第十个IP cd /root/k8s-ha-install/ COREDNS_SERVICE_IP=`kubectl get svc | grep kubernetes | awk '{print $3}'`0 sed -i \"s#KUBEDNS_SERVICE_IP#${COREDNS_SERVICE_IP}#g\" CoreDNS/coredns.yaml kubectl apply -f CoreDNS/coredns.yaml [root@k8s-master01 k8s-ha-install]# kubectl get pod -A | grep coredns kube-system coredns-5db5696c7-tsqts 1/1 Running 0 80s 配置Metrics 在k8s-master01操作 在新版的Kubernetes中系统资源的采集均使用Metrics-server，可以通过Metrics采集节点和Pod的内存、磁盘、CPU和网络的使用率。\ncd /root/k8s-ha-install/metrics-server kubectl create -f . # 等待metrics-server部署好后，便可使用 [root@k8s-master01 metrics-server]# kubectl top nodes NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% k8s-master01 186m 9% 1094Mi 58% k8s-master02 192m 9% 1176Mi 62% k8s-master03 176m 8% 1123Mi 60% k8s-node01 72m 3% 463Mi 24% k8s-node02 66m 3% 472Mi 25% 配置Dashboard Dashboard是一个展示Kubernetes集群资源和Pod日志，甚至可以执行容器命令的web控制台。\n# 直接部署即可 cd /root/k8s-ha-install/dashboard/ kubectl apply -f . # 查看dashboard端口，默认是NodePort模式，访问集群内任意节点的32486端口即可 [root@k8s-master01 ~]# kubectl get svc -A | grep dash kubernetes-dashboard dashboard-metrics-scraper ClusterIP 10.111.39.8 8000/TCP 12m kubernetes-dashboard kubernetes-dashboard NodePort 10.98.143.126 443:32486/TCP 12m 访问dashboard：https://集群内任意节点IP:32486\n发现提示隐私设置错误的问题，解决方法是在Chrome浏览器启动参数加入--test-type --ignore-certificate-errors，再访问就没有这个提示\n# 获取登陆令牌（token） [root@k8s-master01 dashboard]# kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}') Name: admin-user-token-cj2kt Namespace: kube-system Labels: Annotations: kubernetes.io/service-account.name: admin-user kubernetes.io/service-account.uid: c86fbde2-36ea-4dd3-94fd-8ce8012fdf22 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1411 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6ImFPeklobHBkNVRzZzZYVF9nbG5BMTgwOHdvMUNkV2FGbW1wdmUzZzdJRXcifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLWNqMmt0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJjODZmYmRlMi0zNmVhLTRkZDMtOTRmZC04Y2U4MDEyZmRmMjIiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.YgxIsaxR-hfyT9YLGdszggQ0Rvoc4SvyqswgvHz2ySc27q8lAQ7EJxhze3bhrdTL79z3J30T6vmuA5Be3kq_c2r42r2Iy-pC92t8xTISlPWEl7JfSg8GSbX2-UxUM_wqCmbMO3RWGYW5FpzrJ2pSVaeIGlu2JmYTugtS50LCFi87DmP2tDAKLQfh1NRylpEPI1AJPbl41E2wyDBUlS86YF_glUnQxyDCyrf2wJ2Akjqe7If2b9tAXHbSZBcQJFGHENymYhdBW6QObmTRxUsaOX9wdTToFcoHr-FaE4LcP9KuXhxP-gNNyVN7HN0k0WbhAp6CBIoypFCVLIN96EvNIg 选择ALL namespace，可以查看如下图 集群优化(可选) Docker可在/etc/docker/daemon.json自定义优化配置，所有配置可见：官方docker configuration，docker常用优化配置见下方注释说明。\n# （！！！如果使用docker作为Runtime的话）优化docker配置 # /etc/docker/daemon.json文件，按需配置，不需要全部都照抄，使用时删除注释，因为JSON文件不支持注释 { \"exec-opts\": [\"native.cgroupdriver=systemd\"], # cgroups驱动 \"registry-mirrors\": [\"https://ynirk4k5.mirror.aliyuncs.com\"], # 镜像加速器地址 \"allow-nondistributable-artifacts\": [], \"api-cors-header\": \"\", \"authorization-plugins\": [], \"bip\": \"\", \"bridge\": \"\", \"cgroup-parent\": \"\", \"cluster-advertise\": \"\", \"cluster-store\": \"\", \"cluster-store-opts\": {}, \"containerd\": \"/run/containerd/containerd.sock\", \"containerd-namespace\": \"docker\", \"data-root\": \"\", # 数据根目录，大量docker镜像可能会占用较大存储，可以设置系统盘外的挂载盘 \"debug\": true, \"default-address-pools\": [ { \"base\": \"172.30.0.0/16\", \"size\": 24 }, { \"base\": \"172.31.0.0/16\", \"size\": 24 } ], \"default-cgroupns-mode\": \"private\", \"default-gateway\": \"\", \"default-gateway-v6\": \"\", \"default-runtime\": \"runc\", \"default-shm-size\": \"64M\", \"default-ulimits\": { \"nofile\": { \"Hard\": 64000, \"Name\": \"nofile\", \"Soft\": 64000 } }, \"dns\": [], \"dns-opts\": [], \"dns-search\": [], \"exec-root\": \"\", \"experimental\": false, \"features\": {}, \"fixed-cidr\": \"\", \"fixed-cidr-v6\": \"\", \"group\": \"\", \"hosts\": [], \"icc\": false, \"init\": false, \"init-path\": \"/usr/libexec/docker-init\", \"insecure-registries\": [], \"ip\": \"0.0.0.0\", \"ip-forward\": false, \"ip-masq\": false, \"iptables\": false, \"ip6tables\": false, \"ipv6\": false, \"labels\": [], \"live-restore\": true, # docker进程宕机时容器依然保持存活 \"log-driver\": \"json-file\", # 日志格式 \"log-level\": \"\", # 日志级别 \"log-opts\": { # 日志优化 \"cache-disabled\": \"false\", \"cache-max-file\": \"5\", \"cache-max-size\": \"20m\", \"cache-compress\": \"true\", \"env\": \"os,customer\", \"labels\": \"somelabel\", \"max-file\": \"5\", # 最大日志数量 \"max-size\": \"10m\" # 保存的最大日志大小 }, \"max-concurrent-downloads\": 3, # pull下载并发数 \"max-concurrent-uploads\": 5, # push上传并发数 \"max-download-attempts\": 5, \"mtu\": 0, \"no-new-privileges\": false, \"node-generic-resources\": [ \"NVIDIA-GPU=UUID1\", \"NVIDIA-GPU=UUID2\" ], \"oom-score-adjust\": -500, \"pidfile\": \"\", \"raw-logs\": false, \"runtimes\": { \"cc-runtime\": { \"path\": \"/usr/bin/cc-runtime\" }, \"custom\": { \"path\": \"/usr/local/bin/my-runc-replacement\", \"runtimeArgs\": [ \"--debug\" ] } }, \"seccomp-profile\": \"\", \"selinux-enabled\": false, \"shutdown-timeout\": 15, \"storage-driver\": \"\", \"storage-opts\": [], \"swarm-default-advertise-addr\": \"\", \"tls\": true, \"tlscacert\": \"\", \"tlscert\": \"\", \"tlskey\": \"\", \"tlsverify\": true, \"userland-proxy\": false, \"userland-proxy-path\": \"/usr/libexec/docker-proxy\", \"userns-remap\": \"\" } # 无注释版 { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"registry-mirrors\": [\"https://ynirk4k5.mirror.aliyuncs.com\"], \"containerd-namespace\": \"docker\", \"data-root\": \"\", \"debug\": true, \"default-cgroupns-mode\": \"private\", \"default-gateway\": \"\", \"default-gateway-v6\": \"\", \"default-runtime\": \"runc\", \"default-shm-size\": \"64M\", \"default-ulimits\": { \"nofile\": { \"Hard\": 64000, \"Name\": \"nofile\", \"Soft\": 64000 } }, \"init-path\": \"/usr/libexec/docker-init\", \"live-restore\": true, \"log-driver\": \"json-file\", \"log-level\": \"\", \"log-opts\": { \"cache-disabled\": \"false\", \"cache-max-file\": \"5\", \"cache-max-size\": \"20m\", \"cache-compress\": \"true\", \"env\": \"os,customer\", \"labels\": \"somelabel\", \"max-file\": \"5\", \"max-size\": \"10m\" }, \"max-concurrent-downloads\": 3, \"max-concurrent-uploads\": 5, \"max-download-attempts\": 5, \"mtu\": 0, \"no-new-privileges\": false, \"oom-score-adjust\": -500, \"pidfile\": \"\", \"raw-logs\": false, \"runtimes\": { \"cc-runtime\": { \"path\": \"/usr/bin/cc-runtime\" }, \"custom\": { \"path\": \"/usr/local/bin/my-runc-replacement\", \"runtimeArgs\": [ \"--debug\" ] } }, \"seccomp-profile\": \"\", \"selinux-enabled\": false, \"shutdown-timeout\": 15, \"storage-driver\": \"\", \"storage-opts\": [], \"swarm-default-advertise-addr\": \"\", \"userland-proxy-path\": \"/usr/libexec/docker-proxy\", \"userns-remap\": \"\" } # 设置证书有效期 [root@k8s-master01 ~]# vim /usr/lib/systemd/system/kube-controller-manager.service ... # 加入下面配置 --experimental-cluster-signing-duration=876000h0m0s ... [root@k8s-master01 ~]# systemctl daemon-reload [root@k8s-master01 ~]# systemctl restart kube-controller-manager # kubelet优化加密算法，默认的算法容易被漏洞扫描；增长镜像下载周期，避免有些大镜像未下载完成就被动死亡退出 # --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 # --image-pull-progress-deadline=30m [root@k8s-master01 ~]# vim /etc/systemd/system/kubelet.service.d/10-kubelet.conf ... # 下面这行中KUBELET_EXTRA_ARGS=后加入配置 Environment=\"KUBELET_EXTRA_ARGS=--tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 --image-pull-progress-deadline=30m\" ... # 集群配置优化，详见https://kubernetes.io/zh/docs/tasks/administer-cluster/reserve-compute-resources/ [root@k8s-master01 ~]# vim /etc/kubernetes/kubelet-conf.yml # 文件中添加如下配置 rotateServerCertificates: true allowedUnsafeSysctls: # 允许在修改内核参数，此操作按情况选择，用不到就不用设置 - \"net.core*\" - \"net.ipv4.*\" kubeReserved: # 为Kubernetes集群守护进程组件预留资源，例如：kubelet、Runtime等 cpu: \"100m\" memory: 100Mi ephemeral-storage: 1Gi systemReserved: # 为系统守护进程预留资源，例如：sshd、cron等 cpu: \"100m\" memory: 100Mi ephemeral-storage: 1Gi # 为集群节点打标签，删除标签把 = 换成 - 即可 kubectl label nodes k8s-node01 node-role.kubernetes.io/node= kubectl label nodes k8s-node02 node-role.kubernetes.io/node= kubectl label nodes k8s-master01 node-role.kubernetes.io/master= kubectl label nodes k8s-master02 node-role.kubernetes.io/master= kubectl label nodes k8s-master03 node-role.kubernetes.io/master= # 添加标签后查看集群状态 [root@k8s-master01 ~]# kubectl get node NAME STATUS ROLES AGE VERSION k8s-master01 Ready master 35m v1.26.4 k8s-master02 Ready master 35m v1.26.4 k8s-master03 Ready master 35m v1.26.4 k8s-node01 Ready node 35m v1.26.4 k8s-node02 Ready node 35m v1.26.4 生产环境建议ETCD集群和Kubernetes集群分离，而且使用高性能数据盘存储数据，根据情况决定是否将Master节点也作为Pod调度节点。\n测试集群 # 测试namespace kubectl get namespace kubectl create namespace test kubectl get namespace kubectl delete namespace test # 创建nginx实例并开放端口 kubectl create deployment nginx --image=nginx kubectl expose deployment nginx --port=80 --type=NodePort # 查看调度状态和端口号 [root@k8s-master01 ~]# kubectl get po,svc -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/nginx-748c667d99-dmtn6 1/1 Running 0 9m55s 172.25.244.194 k8s-master01 NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/kubernetes ClusterIP 10.96.0.1 443/TCP 69m service/nginx NodePort 10.110.18.105 80:31687/TCP 9s app=nginx 在浏览器输入http://任意节点IP:31687/ 访问nginx，访问结果如图\n至此，基于二进制方式的Kubernetes高可用集群部署并验证成功。\n",
  "wordCount" : "4072",
  "inLanguage": "en",
  "datePublished": "2023-04-25T02:04:03+08:00",
  "dateModified": "2023-04-25T02:04:03+08:00",
  "author":{
    "@type": "Person",
    "name": "Theme PaperMod"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://deemoprobe.github.io/%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2kubernetes%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "PaperMod",
    "logo": {
      "@type": "ImageObject",
      "url": "https://deemoprobe.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://deemoprobe.github.io/" accesskey="h" title="PaperMod (Alt + H)">PaperMod</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                    <li>
                        <a href="https://deemoprobe.github.io/fr/" title="French"
                            aria-label=":fr:">🇫🇷</a>
                    </li>
                    <li>
                        <a href="https://deemoprobe.github.io/fa/" title="Fa"
                            aria-label="Fa">Fa</a>
                    </li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://deemoprobe.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://deemoprobe.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://deemoprobe.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://discord.gg/ahpmTvhVmp" title="Discord">
                    <span>Discord</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
            <li>
                <a href="https://github.com/adityatelange/hugo-PaperMod/wiki/" title="WiKi">
                    <span>WiKi</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://deemoprobe.github.io/">Home</a></div>
    <h1 class="post-title">
      二进制方式部署kubernetes高可用集群
    </h1>
    <div class="post-meta"><span title='2023-04-25 02:04:03 +0800 CST'>April 25, 2023</span>&nbsp;·&nbsp;20 min&nbsp;·&nbsp;Theme PaperMod&nbsp;|&nbsp;<a href="https://github.com/adityatelange/hugo-PaperMod/tree/exampleSite/content/%e4%ba%8c%e8%bf%9b%e5%88%b6%e6%96%b9%e5%bc%8f%e9%83%a8%e7%bd%b2kubernetes%e9%ab%98%e5%8f%af%e7%94%a8%e9%9b%86%e7%be%a4.md" rel="noopener noreferrer" target="_blank">Suggest Changes</a>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#%e7%8e%af%e5%a2%83%e8%af%b4%e6%98%8e" aria-label="环境说明">环境说明</a></li>
                <li>
                    <a href="#%e8%b5%84%e6%ba%90%e5%88%86%e9%85%8d" aria-label="资源分配">资源分配</a></li>
                <li>
                    <a href="#%e6%93%8d%e4%bd%9c%e6%ad%a5%e9%aa%a4" aria-label="操作步骤">操作步骤</a><ul>
                        
                <li>
                    <a href="#%e5%87%86%e5%a4%87%e5%b7%a5%e4%bd%9call" aria-label="准备工作(ALL)">准备工作(ALL)</a></li>
                <li>
                    <a href="#%e9%83%a8%e7%bd%b2containerdall" aria-label="部署Containerd(ALL)">部署Containerd(ALL)</a></li>
                <li>
                    <a href="#%e4%ba%8c%e8%bf%9b%e5%88%b6%e5%8c%85%e5%92%8c%e8%af%81%e4%b9%a6" aria-label="二进制包和证书">二进制包和证书</a></li>
                <li>
                    <a href="#etcd%e9%9b%86%e7%be%a4master" aria-label="ETCD集群(Master)">ETCD集群(Master)</a></li>
                <li>
                    <a href="#%e9%ab%98%e5%8f%af%e7%94%a8%e7%bb%84%e4%bb%b6master" aria-label="高可用组件(Master)">高可用组件(Master)</a></li>
                <li>
                    <a href="#%e9%85%8d%e7%bd%ae%e9%9b%86%e7%be%a4%e7%bb%84%e4%bb%b6" aria-label="配置集群组件">配置集群组件</a></li>
                <li>
                    <a href="#%e9%85%8d%e7%bd%aetls-bootstrapping" aria-label="配置TLS Bootstrapping">配置TLS Bootstrapping</a></li>
                <li>
                    <a href="#%e9%85%8d%e7%bd%aekubelet" aria-label="配置Kubelet">配置Kubelet</a></li>
                <li>
                    <a href="#%e9%85%8d%e7%bd%aekube-proxy" aria-label="配置Kube-proxy">配置Kube-proxy</a></li>
                <li>
                    <a href="#%e9%85%8d%e7%bd%aecalico" aria-label="配置Calico">配置Calico</a></li>
                <li>
                    <a href="#%e9%85%8d%e7%bd%aecoredns" aria-label="配置CoreDNS">配置CoreDNS</a></li></ul>
                </li>
                <li>
                    <a href="#%e9%85%8d%e7%bd%aemetrics" aria-label="配置Metrics">配置Metrics</a></li>
                <li>
                    <a href="#%e9%85%8d%e7%bd%aedashboard" aria-label="配置Dashboard">配置Dashboard</a></li>
                <li>
                    <a href="#%e9%9b%86%e7%be%a4%e4%bc%98%e5%8c%96%e5%8f%af%e9%80%89" aria-label="集群优化(可选)">集群优化(可选)</a></li>
                <li>
                    <a href="#%e6%b5%8b%e8%af%95%e9%9b%86%e7%be%a4" aria-label="测试集群">测试集群</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="环境说明">环境说明<a hidden class="anchor" aria-hidden="true" href="#环境说明">#</a></h2>
<ul>
<li>宿主机系统：Windows 10</li>
<li>虚拟机版本：VMware® Workstation 16 Pro</li>
<li>IOS镜像版本：CentOS Linux release 7.9.2009</li>
<li>Kubernetes版本：1.26.4</li>
<li>Runtime：Containerd v1.6.20</li>
<li>Etcd版本：3.5.6</li>
<li>集群操作用户：root</li>
<li>更新时间：2023-04-21</li>
</ul>
<p>CentOS7安装请参考博客文章：<a href="https://www.deemo.dev/linux/centos7install/">LINUX之VMWARE WORKSTATION安装CENTOS-7</a></p>
<h2 id="资源分配">资源分配<a hidden class="anchor" aria-hidden="true" href="#资源分配">#</a></h2>
<p><strong>网段划分</strong></p>
<p>Kubernetes集群需要规划三个网段：</p>
<ul>
<li>宿主机网段：Kubernetes集群节点的网段</li>
<li>Pod网段：集群内Pod的网段，相当于容器的IP</li>
<li>Service网段：集群内服务发现使用的网段，service用于集群容器通信</li>
</ul>
<p>生产环境根据申请到的IP资源进行分配即可，原则是三个网段不允许有重合IP。IP网段计算可以参考：<a href="http://tools.jb51.net/aideddesign/ip_net_calc/">在线IP地址计算</a>。本文虚拟机练习环境IP地址网段分配如下：</p>
<ul>
<li>宿主机网段：<code>192.168.43.1/24</code></li>
<li>Pod网段：<code>172.16.0.0/12</code></li>
<li>Service：<code>10.96.0.0/12</code></li>
</ul>
<p><strong>节点分配</strong></p>
<p>采用<code>3管理节点2工作节点</code>的高可用Kubernetes集群模式：</p>
<ul>
<li>k8s-master01/k8s-master02/k8s-master03 集群的Master节点</li>
<li>三个master节点同时做etcd集群</li>
<li>k8s-node01/k8s-node02 集群的Node节点</li>
<li>k8s-master-vip做高可用k8s-master01~03的VIP，不占用物理资源</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align:left">主机节点名称</th>
<th style="text-align:center">IP</th>
<th style="text-align:center">CPU核心数</th>
<th style="text-align:center">内存大小</th>
<th style="text-align:center">磁盘大小</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">k8s-master-vip</td>
<td style="text-align:center">192.168.43.200</td>
<td style="text-align:center">/</td>
<td style="text-align:center">/</td>
<td style="text-align:center">/</td>
</tr>
<tr>
<td style="text-align:left">k8s-master01</td>
<td style="text-align:center">192.168.43.201</td>
<td style="text-align:center">2</td>
<td style="text-align:center">2G</td>
<td style="text-align:center">40G</td>
</tr>
<tr>
<td style="text-align:left">k8s-master02</td>
<td style="text-align:center">192.168.43.202</td>
<td style="text-align:center">2</td>
<td style="text-align:center">2G</td>
<td style="text-align:center">40G</td>
</tr>
<tr>
<td style="text-align:left">k8s-master03</td>
<td style="text-align:center">192.168.43.203</td>
<td style="text-align:center">2</td>
<td style="text-align:center">2G</td>
<td style="text-align:center">40G</td>
</tr>
<tr>
<td style="text-align:left">k8s-node01</td>
<td style="text-align:center">192.168.43.204</td>
<td style="text-align:center">2</td>
<td style="text-align:center">2G</td>
<td style="text-align:center">40G</td>
</tr>
<tr>
<td style="text-align:left">k8s-node02</td>
<td style="text-align:center">192.168.43.205</td>
<td style="text-align:center">2</td>
<td style="text-align:center">2G</td>
<td style="text-align:center">40G</td>
</tr>
</tbody>
</table>
<h2 id="操作步骤">操作步骤<a hidden class="anchor" aria-hidden="true" href="#操作步骤">#</a></h2>
<p>标题后小括号注释表明操作范围：</p>
<ul>
<li>ALL 所有节点（k8s-master01/k8s-master02/k8s-master03/k8s-node01/k9s-node02）执行</li>
<li>Master 只需要在master节点（k8s-master01/k8s-master02/k8s-master03）执行</li>
<li>Node 只需要在node节点（k8s-node01/k8s-node02）执行</li>
<li>已标注的个别命令只需要在某一台机器执行，会在操作前说明</li>
<li>未标注的会在操作时说明</li>
</ul>
<p>使用<code>cat &lt;&lt; &quot;EOF&quot; &gt;&gt; file</code>或<code>cat &gt;&gt; file &lt;&lt; &quot;EOF&quot;</code>添加文件内容注意cat后面的EOF一定要加上双引号（标准输入的），否则不会保留输入时的缩进格式而且会直接解析输入时的变量，进而造成文件可读性差甚至不可用；同时注意文件的<code>&gt;</code>重写与<code>&gt;&gt;</code>追加。虽然单独转义输入时的变量也能避免变量被解析，但是不推荐，漏转义会造成不必要的麻烦。</p>
<h3 id="准备工作all">准备工作(ALL)<a hidden class="anchor" aria-hidden="true" href="#准备工作all">#</a></h3>
<p>添加主机信息、关闭防火墙、关闭swap、关闭SELinux、dnsmasq、NetworkManager</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># 添加主机信息</span>
</span></span><span class="line"><span class="cl">cat &lt;&lt; <span class="s2">&#34;EOF&#34;</span> &gt;&gt; /etc/hosts
</span></span><span class="line"><span class="cl">192.168.43.200    k8s-master-vip
</span></span><span class="line"><span class="cl">192.168.43.201    k8s-master01
</span></span><span class="line"><span class="cl">192.168.43.202    k8s-master02
</span></span><span class="line"><span class="cl">192.168.43.203    k8s-master03
</span></span><span class="line"><span class="cl">192.168.43.204    k8s-node01
</span></span><span class="line"><span class="cl">192.168.43.205    k8s-node02
</span></span><span class="line"><span class="cl">EOF
</span></span><span class="line"><span class="cl"><span class="c1"># 关闭防火墙、dnsmasq、NetworkManager，--now参数表示关闭服务并移除开机自启</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 这些服务是否可以关闭视情况而定，本文是虚拟机实践，没有用到这些服务</span>
</span></span><span class="line"><span class="cl">systemctl disable --now firewalld
</span></span><span class="line"><span class="cl">systemctl disable --now dnsmasq
</span></span><span class="line"><span class="cl">systemctl disable --now NetworkManager
</span></span><span class="line"><span class="cl"><span class="c1"># 关闭swap，并注释fstab文件swap所在行</span>
</span></span><span class="line"><span class="cl">swapoff -a
</span></span><span class="line"><span class="cl">sed -i <span class="s1">&#39;/swap/s/^\(.*\)$/#\1/g&#39;</span> /etc/fstab
</span></span><span class="line"><span class="cl"><span class="c1"># 关闭SELinux，并更改selinux配置文件</span>
</span></span><span class="line"><span class="cl">setenforce <span class="m">0</span>
</span></span><span class="line"><span class="cl">sed -i <span class="s2">&#34;s/=enforcing/=disabled/g&#34;</span> /etc/selinux/config
</span></span></code></pre></div><p>值得注意的是<code>/etc/sysconfig/selinux</code>文件是<code>/etc/selinux/config</code>文件的软连接，用<code>sed -i</code>命令修改软连接文件会破坏软连接属性，将<code>/etc/sysconfig/selinux</code>变为一个独立的文件，即使该文件被修改了，但源文件<code>/etc/selinux/config</code>配置是没变的。此外，使用vim等编辑器编辑源文件或链接文件（编辑模式不会修改文件属性）修改也可以。软链接原理可参考博客：<a href="http://www.deemoprobe.com/yunv/inode/">LINUX之INODE详解</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># 默认的yum源太慢，更新为阿里源，同时用sed命令删除文件中不需要的两个URL的行</span>
</span></span><span class="line"><span class="cl">curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo
</span></span><span class="line"><span class="cl">sed -i -e <span class="s1">&#39;/mirrors.cloud.aliyuncs.com/d&#39;</span> -e <span class="s1">&#39;/mirrors.aliyuncs.com/d&#39;</span> /etc/yum.repos.d/CentOS-Base.repo
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 安装常用工具包</span>
</span></span><span class="line"><span class="cl">yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git -y
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 配置ntpdate，同步服务器时间</span>
</span></span><span class="line"><span class="cl">rpm -ivh http://mirrors.wlnmp.com/centos/wlnmp-release-centos.noarch.rpm
</span></span><span class="line"><span class="cl">yum install ntpdate -y
</span></span><span class="line"><span class="cl"><span class="c1"># 同步时区和时间</span>
</span></span><span class="line"><span class="cl">ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> <span class="s1">&#39;Asia/Shanghai&#39;</span> &gt;/etc/timezone
</span></span><span class="line"><span class="cl">ntpdate time2.aliyun.com
</span></span><span class="line"><span class="cl"><span class="c1"># 可以加入计划任务，保证集群时钟是一致的</span>
</span></span><span class="line"><span class="cl"><span class="c1"># /var/spool/cron/root文件也是crontab -e写入的文件</span>
</span></span><span class="line"><span class="cl"><span class="c1"># crontab执行日志查看可用：tail -f /var/log/cron</span>
</span></span><span class="line"><span class="cl">cat &lt;&lt; <span class="s2">&#34;EOF&#34;</span> &gt;&gt; /var/spool/cron/root
</span></span><span class="line"><span class="cl">*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com
</span></span><span class="line"><span class="cl">EOF
</span></span><span class="line"><span class="cl"><span class="c1"># 须知：如果设置了定时任务，会经常收到提示“You have new mail in /var/spool/mail/root”</span>
</span></span><span class="line"><span class="cl"><span class="c1"># （可选）禁用提示：echo &#34;unset MAILCHECK&#34; &gt;&gt; ~/.bashrc;source ~/.bashrc</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 禁用提示后/var/spool/mail/root文件依旧会记录root操作日志，可随时查看</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 保证文件句柄不会限制集群的可持续发展，配置limits</span>
</span></span><span class="line"><span class="cl"><span class="nb">ulimit</span> -SHn <span class="m">65500</span>
</span></span><span class="line"><span class="cl">cat &lt;&lt; <span class="s2">&#34;EOF&#34;</span> &gt;&gt; /etc/security/limits.conf
</span></span><span class="line"><span class="cl">* soft nofile <span class="m">65500</span>
</span></span><span class="line"><span class="cl">* hard nofile <span class="m">65500</span>
</span></span><span class="line"><span class="cl">* soft nproc <span class="m">65500</span>
</span></span><span class="line"><span class="cl">* hard nproc <span class="m">65500</span>
</span></span><span class="line"><span class="cl">* soft memlock unlimited
</span></span><span class="line"><span class="cl">* hard memlock unlimited
</span></span><span class="line"><span class="cl">EOF
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 配置免密登录，k8s-master01到其他节点</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 生成密钥对（在k8s-master01节点配置即可）</span>
</span></span><span class="line"><span class="cl">ssh-keygen -t rsa
</span></span><span class="line"><span class="cl"><span class="c1"># 拷贝公钥到其他节点，首次需要认证一下各个节点的root密码，以后就可以免密ssh到其他节点</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> i in k8s-master02 k8s-master03 k8s-node01 k8s-node02<span class="p">;</span><span class="k">do</span> ssh-copy-id -i .ssh/id_rsa.pub <span class="nv">$i</span><span class="p">;</span><span class="k">done</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 克隆二进制仓库1.26分支的文件（k8s-master01上操作即可）</span>
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> /root<span class="p">;</span>git clone https://gitee.com/deemoprobe/k8s-ha-install.git -b manual-installation-v1.26.x
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 所有节点系统升级</span>
</span></span><span class="line"><span class="cl">yum update --exclude<span class="o">=</span>kernel* -y
</span></span></code></pre></div><p>升级内核，4.17以下的内核cgroup存在内存泄漏的BUG，具体分析过程浏览器搜<code>Kubernetes集群为什么要升级内核</code>会有很多文章讲解</p>
<p>内核备用下载（下载到本地后上传到服务器，尽量不要用<code>wget</code>）：</p>
<ul>
<li><a href="https://deemoprobe.oss-cn-shanghai.aliyuncs.com/repo/kernel-ml-devel-4.19.12-1.el7.elrepo.x86_64.rpm?versionId=CAEQNBiBgMDJiNbj.hciIDUyMDZlYjU5YzIwMzQ0MmNhNzBmNjBiMDY3Yjc0Y2Jl">kernel-ml-devel-4.19.12-1.el7.elrepo.x86_64.rpm</a></li>
<li><a href="https://deemoprobe.oss-cn-shanghai.aliyuncs.com/repo/kernel-ml-4.19.12-1.el7.elrepo.x86_64.rpm?versionId=CAEQNBiBgMDNiNbj.hciIGQ0M2RmZDAwNDhlNjQyNjE5MTE4MDk1OGU4OThiNWY4">kernel-ml-4.19.12-1.el7.elrepo.x86_64.rpm</a></li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># 下载4.19版本内核，如果无法下载，可以用上面提供的备用下载</span>
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> /root
</span></span><span class="line"><span class="cl">wget http://193.49.22.109/elrepo/kernel/el7/x86_64/RPMS/kernel-ml-devel-4.19.12-1.el7.elrepo.x86_64.rpm
</span></span><span class="line"><span class="cl">wget http://193.49.22.109/elrepo/kernel/el7/x86_64/RPMS/kernel-ml-4.19.12-1.el7.elrepo.x86_64.rpm
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 可以在k8s-master01节点下载后，免密传到其他节点</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> i in k8s-master02 k8s-master03 k8s-node01 k8s-node02<span class="p">;</span><span class="k">do</span> scp kernel-ml-* <span class="nv">$i</span>:/root<span class="p">;</span><span class="k">done</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 所有节点安装内核</span>
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> /root <span class="o">&amp;&amp;</span> yum localinstall -y kernel-ml*
</span></span><span class="line"><span class="cl"><span class="c1"># 所有节点更改内核启动顺序</span>
</span></span><span class="line"><span class="cl">grub2-set-default  <span class="m">0</span> <span class="o">&amp;&amp;</span> grub2-mkconfig -o /etc/grub2.cfg
</span></span><span class="line"><span class="cl">grubby --args<span class="o">=</span><span class="s2">&#34;user_namespace.enable=1&#34;</span> --update-kernel<span class="o">=</span><span class="s2">&#34;</span><span class="k">$(</span>grubby --default-kernel<span class="k">)</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 查看默认内核，并重启节点</span>
</span></span><span class="line"><span class="cl">grubby --default-kernel
</span></span><span class="line"><span class="cl">reboot
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 确认内核版本</span>
</span></span><span class="line"><span class="cl">uname -a
</span></span><span class="line"><span class="cl"><span class="c1"># （可选）删除老版本的内核，避免以后被升级取代默认的开机4.19内核</span>
</span></span><span class="line"><span class="cl">rpm -qa <span class="p">|</span> grep kernel
</span></span><span class="line"><span class="cl">yum remove -y kernel-3*
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 升级系统软件包（如果跳过内核升级加参数 --exclude=kernel*）</span>
</span></span><span class="line"><span class="cl">yum update -y
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 安装IPVS相关工具，由于IPVS在资源消耗和性能上均已明显优于iptables，所以推荐开启</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 具体原因可参考官网介绍 https://kubernetes.io/zh/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/</span>
</span></span><span class="line"><span class="cl">yum install ipvsadm ipset sysstat conntrack libseccomp -y
</span></span><span class="line"><span class="cl"><span class="c1"># 加载模块，最后一条4.18及以下内核使用nf_conntrack_ipv4，4.19已改为nf_conntrack</span>
</span></span><span class="line"><span class="cl">modprobe -- ip_vs
</span></span><span class="line"><span class="cl">modprobe -- ip_vs_rr
</span></span><span class="line"><span class="cl">modprobe -- ip_vs_wrr
</span></span><span class="line"><span class="cl">modprobe -- ip_vs_sh
</span></span><span class="line"><span class="cl">modprobe -- nf_conntrack
</span></span><span class="line"><span class="cl"><span class="c1"># 编写参数文件</span>
</span></span><span class="line"><span class="cl">cat &lt;&lt; <span class="s2">&#34;EOF&#34;</span> &gt; /etc/modules-load.d/ipvs.conf 
</span></span><span class="line"><span class="cl">ip_vs
</span></span><span class="line"><span class="cl">ip_vs_lc
</span></span><span class="line"><span class="cl">ip_vs_wlc
</span></span><span class="line"><span class="cl">ip_vs_rr
</span></span><span class="line"><span class="cl">ip_vs_wrr
</span></span><span class="line"><span class="cl">ip_vs_lblc
</span></span><span class="line"><span class="cl">ip_vs_lblcr
</span></span><span class="line"><span class="cl">ip_vs_dh
</span></span><span class="line"><span class="cl">ip_vs_sh
</span></span><span class="line"><span class="cl">ip_vs_fo
</span></span><span class="line"><span class="cl">ip_vs_nq
</span></span><span class="line"><span class="cl">ip_vs_sed
</span></span><span class="line"><span class="cl">ip_vs_ftp
</span></span><span class="line"><span class="cl">ip_vs_sh
</span></span><span class="line"><span class="cl">nf_conntrack
</span></span><span class="line"><span class="cl">ip_tables
</span></span><span class="line"><span class="cl">ip_set
</span></span><span class="line"><span class="cl">xt_set
</span></span><span class="line"><span class="cl">ipt_set
</span></span><span class="line"><span class="cl">ipt_rpfilter
</span></span><span class="line"><span class="cl">ipt_REJECT
</span></span><span class="line"><span class="cl">ipip
</span></span><span class="line"><span class="cl">EOF
</span></span><span class="line"><span class="cl"><span class="c1"># systemd-modules-load加入开机自启</span>
</span></span><span class="line"><span class="cl">systemctl <span class="nb">enable</span> --now systemd-modules-load
</span></span><span class="line"><span class="cl"><span class="c1"># 自定义内核参数优化配置文件</span>
</span></span><span class="line"><span class="cl">cat &lt;&lt; <span class="s2">&#34;EOF&#34;</span> &gt; /etc/sysctl.d/kubernetes.conf
</span></span><span class="line"><span class="cl">net.ipv4.ip_forward <span class="o">=</span> <span class="m">1</span>
</span></span><span class="line"><span class="cl">net.bridge.bridge-nf-call-iptables <span class="o">=</span> <span class="m">1</span>
</span></span><span class="line"><span class="cl">net.bridge.bridge-nf-call-ip6tables <span class="o">=</span> <span class="m">1</span>
</span></span><span class="line"><span class="cl">fs.may_detach_mounts <span class="o">=</span> <span class="m">1</span>
</span></span><span class="line"><span class="cl">net.ipv4.conf.all.route_localnet <span class="o">=</span> <span class="m">1</span>
</span></span><span class="line"><span class="cl">vm.overcommit_memory<span class="o">=</span><span class="m">1</span>
</span></span><span class="line"><span class="cl">vm.panic_on_oom<span class="o">=</span><span class="m">0</span>
</span></span><span class="line"><span class="cl">fs.inotify.max_user_watches<span class="o">=</span><span class="m">89100</span>
</span></span><span class="line"><span class="cl">fs.file-max<span class="o">=</span><span class="m">52706963</span>
</span></span><span class="line"><span class="cl">fs.nr_open<span class="o">=</span><span class="m">52706963</span>
</span></span><span class="line"><span class="cl">net.netfilter.nf_conntrack_max<span class="o">=</span><span class="m">2310720</span>
</span></span><span class="line"><span class="cl">net.ipv4.tcp_keepalive_time <span class="o">=</span> <span class="m">600</span>
</span></span><span class="line"><span class="cl">net.ipv4.tcp_keepalive_probes <span class="o">=</span> <span class="m">3</span>
</span></span><span class="line"><span class="cl">net.ipv4.tcp_keepalive_intvl <span class="o">=</span><span class="m">15</span>
</span></span><span class="line"><span class="cl">net.ipv4.tcp_max_tw_buckets <span class="o">=</span> <span class="m">36000</span>
</span></span><span class="line"><span class="cl">net.ipv4.tcp_tw_reuse <span class="o">=</span> <span class="m">1</span>
</span></span><span class="line"><span class="cl">net.ipv4.tcp_max_orphans <span class="o">=</span> <span class="m">327680</span>
</span></span><span class="line"><span class="cl">net.ipv4.tcp_orphan_retries <span class="o">=</span> <span class="m">3</span>
</span></span><span class="line"><span class="cl">net.ipv4.tcp_syncookies <span class="o">=</span> <span class="m">1</span>
</span></span><span class="line"><span class="cl">net.ipv4.tcp_max_syn_backlog <span class="o">=</span> <span class="m">16384</span>
</span></span><span class="line"><span class="cl">net.ipv4.ip_conntrack_max <span class="o">=</span> <span class="m">65536</span>
</span></span><span class="line"><span class="cl">net.ipv4.tcp_max_syn_backlog <span class="o">=</span> <span class="m">16384</span>
</span></span><span class="line"><span class="cl">net.ipv4.tcp_timestamps <span class="o">=</span> <span class="m">0</span>
</span></span><span class="line"><span class="cl">net.core.somaxconn <span class="o">=</span> <span class="m">16384</span>
</span></span><span class="line"><span class="cl">EOF
</span></span><span class="line"><span class="cl"><span class="c1"># 加载</span>
</span></span><span class="line"><span class="cl">sysctl --system
</span></span><span class="line"><span class="cl"><span class="c1"># 重启查看IPVS模块是否依旧加载</span>
</span></span><span class="line"><span class="cl">reboot
</span></span><span class="line"><span class="cl">lsmod <span class="p">|</span> grep -e ip_vs -e nf_conntrack
</span></span></code></pre></div><ul>
<li>保证每台服务器中IPVS加载成功，以k8s-master01为例，如图：</li>
</ul>
<p><img loading="lazy" src="https://deemoprobe.oss-cn-shanghai.aliyuncs.com/images/20220305153926.png" alt="20220305153926"  />
</p>
<h3 id="部署containerdall">部署Containerd(ALL)<a hidden class="anchor" aria-hidden="true" href="#部署containerdall">#</a></h3>
<p>Kubernetes1.24版本以后将不再支持Docker作为Runtime，本文安装使用Containerd作为Runtime。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># 配置阿里docker源</span>
</span></span><span class="line"><span class="cl">yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 安装最新版本docker和containerd.io，安装docker是为了使用docker CLI</span>
</span></span><span class="line"><span class="cl">yum install docker-ce docker-ce-cli containerd.io -y
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># （可选）也可以按需安装指定版本</span>
</span></span><span class="line"><span class="cl">yum install docker-ce-20.10.* docker-ce-cli-20.10.* containerd -y
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 配置Containerd模块</span>
</span></span><span class="line"><span class="cl">cat &lt;&lt; <span class="s2">&#34;EOF&#34;</span> &gt; /etc/modules-load.d/containerd.conf
</span></span><span class="line"><span class="cl">overlay
</span></span><span class="line"><span class="cl">br_netfilter
</span></span><span class="line"><span class="cl">EOF
</span></span><span class="line"><span class="cl"><span class="c1"># 加载模块</span>
</span></span><span class="line"><span class="cl">modprobe -- overlay
</span></span><span class="line"><span class="cl">modprobe -- br_netfilter
</span></span><span class="line"><span class="cl"><span class="c1"># 配置内核参数</span>
</span></span><span class="line"><span class="cl">cat &lt;&lt; <span class="s2">&#34;EOF&#34;</span> &gt; /etc/sysctl.d/containerd.conf
</span></span><span class="line"><span class="cl">net.bridge.bridge-nf-call-iptables  <span class="o">=</span> <span class="m">1</span>
</span></span><span class="line"><span class="cl">net.ipv4.ip_forward                 <span class="o">=</span> <span class="m">1</span>
</span></span><span class="line"><span class="cl">net.bridge.bridge-nf-call-ip6tables <span class="o">=</span> <span class="m">1</span>
</span></span><span class="line"><span class="cl">EOF
</span></span><span class="line"><span class="cl"><span class="c1"># 加载内核参数</span>
</span></span><span class="line"><span class="cl">sysctl --system
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 生成默认配置文件</span>
</span></span><span class="line"><span class="cl">mkdir -p /etc/containerd
</span></span><span class="line"><span class="cl">containerd config default <span class="p">|</span> tee /etc/containerd/config.toml
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 更改Cgroup为Systemd，在containerd.runtimes.runc.options行后的SystemdCgroup = false修改为true，如果配置项不存在就自行添加，缩进俩空格添加SystemdCgroup = true一行</span>
</span></span><span class="line"><span class="cl">vim /etc/containerd/config.toml
</span></span><span class="line"><span class="cl">          ...
</span></span><span class="line"><span class="cl">          <span class="o">[</span>plugins.<span class="s2">&#34;io.containerd.grpc.v1.cri&#34;</span>.containerd.runtimes.runc.options<span class="o">]</span>
</span></span><span class="line"><span class="cl">            <span class="nv">SystemdCgroup</span> <span class="o">=</span> <span class="nb">true</span>
</span></span><span class="line"><span class="cl">          ...
</span></span><span class="line"><span class="cl"><span class="c1"># 将sandbox_image的Pause镜像地址改成国内：registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.7</span>
</span></span><span class="line"><span class="cl">vim /etc/containerd/config.toml
</span></span><span class="line"><span class="cl">    <span class="nv">sandbox_image</span> <span class="o">=</span> <span class="s2">&#34;registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.7&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># （可选）也可以在k8s-master01编辑/etc/containerd/config.toml文件后，将编辑后同名文件同步到其他服务器，自动覆盖</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> i in k8s-master02 k8s-master03 k8s-node01 k8s-node02<span class="p">;</span><span class="k">do</span> scp /etc/containerd/config.toml <span class="nv">$i</span>:/etc/containerd/config.toml<span class="p">;</span><span class="k">done</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 查看确认是否配置成功</span>
</span></span><span class="line"><span class="cl">cat /etc/containerd/config.toml <span class="p">|</span> grep -e Systemd -e sandbox_image
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 启动并加入开机自启</span>
</span></span><span class="line"><span class="cl">systemctl daemon-reload
</span></span><span class="line"><span class="cl">systemctl <span class="nb">enable</span> --now containerd
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># containerd运行时的CLI是ctr</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@k8s-master01 ~<span class="o">]</span><span class="c1"># ctr images ls</span>
</span></span><span class="line"><span class="cl">REF TYPE DIGEST SIZE PLATFORMS LABELS 
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@k8s-master01 ~<span class="o">]</span><span class="c1"># ctr version</span>
</span></span><span class="line"><span class="cl">Client:
</span></span><span class="line"><span class="cl">  Version:  1.6.20
</span></span><span class="line"><span class="cl">  Revision: 2806fc1057397dbaeefbea0e4e17bddfbd388f38
</span></span><span class="line"><span class="cl">  Go version: go1.19.7
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Server:
</span></span><span class="line"><span class="cl">  Version:  1.6.20
</span></span><span class="line"><span class="cl">  Revision: 2806fc1057397dbaeefbea0e4e17bddfbd388f38
</span></span><span class="line"><span class="cl">  UUID: 9958928e-300c-4778-b83c-6c0073414f3e
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 配置crictl连接的运行时socket接口（指向containerd&#39;s GRPC server：/run/containerd/containerd.sock）</span>
</span></span><span class="line"><span class="cl"><span class="c1"># crictl 默认连接到unix:///var/run/dockershim.sock</span>
</span></span><span class="line"><span class="cl">cat &lt;&lt; <span class="s2">&#34;EOF&#34;</span> &gt; /etc/crictl.yaml 
</span></span><span class="line"><span class="cl">runtime-endpoint: unix:///run/containerd/containerd.sock
</span></span><span class="line"><span class="cl">image-endpoint: unix:///run/containerd/containerd.sock
</span></span><span class="line"><span class="cl">timeout: <span class="m">10</span>
</span></span><span class="line"><span class="cl">debug: <span class="nb">false</span>
</span></span><span class="line"><span class="cl">EOF
</span></span><span class="line"><span class="cl"><span class="c1"># crictl按需选择，https://github.com/kubernetes-sigs/cri-tools/releases</span>
</span></span><span class="line"><span class="cl"><span class="c1"># containerd容器管理CLI是crictl，该命令使用和docker命令类似，下载包上传到k8s-master01</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@k8s-master01 ~<span class="o">]</span><span class="c1"># tar -zxvf crictl-v1.27.0-linux-amd64.tar.gz</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@k8s-master01 ~<span class="o">]</span><span class="c1"># mv crictl /usr/local/bin/</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@k8s-master01 ~<span class="o">]</span><span class="c1"># crictl version</span>
</span></span><span class="line"><span class="cl">Version:  0.1.0
</span></span><span class="line"><span class="cl">RuntimeName:  containerd
</span></span><span class="line"><span class="cl">RuntimeVersion:  1.6.20
</span></span><span class="line"><span class="cl">RuntimeApiVersion:  v1
</span></span><span class="line"><span class="cl"><span class="c1"># 二进制可执行文件发送到其他节点</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@k8s-master01 ~<span class="o">]</span><span class="c1"># for i in k8s-master02 k8s-master03 k8s-node01 k8s-node02;do scp /usr/local/bin/crictl $i:/usr/local/bin/;done</span>
</span></span></code></pre></div><blockquote>
<p>crictl 是CRI（Container Runtime Interface）兼容的容器运行时的CLI（Command-Line Interface）。可以这个命令来检查和调试 Kubernetes 节点上的容器运行时和应用程序。介绍和安装方式可见：<a href="https://github.com/kubernetes-sigs/cri-tools">critools</a>或<a href="https://kubernetes.io/zh/docs/tasks/debug-application-cluster/crictl/">Kubernetes官方介绍</a></p>
</blockquote>
<h3 id="二进制包和证书">二进制包和证书<a hidden class="anchor" aria-hidden="true" href="#二进制包和证书">#</a></h3>
<p>k8s-master01节点上下载并安装Kubernetes二进制安装包（server-binaries，选择对应的架构即可）和ETCD二进制安装包，可在GitHub上查看Kubernetes1.23.x版本的信息，<a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.23.md">官方GitHub-Kubernetes1.23版本链接</a>，<a href="https://github.com/etcd-io/etcd/releases">ETCD官方链接</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># 以下在k8s-master01执行</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 下载太慢的话可以在相应链接网页上下载好传到服务器</span>
</span></span><span class="line"><span class="cl">wget https://dl.k8s.io/v1.26.4/kubernetes-server-linux-amd64.tar.gz
</span></span><span class="line"><span class="cl">wget https://github.com/etcd-io/etcd/releases/download/v3.5.6/etcd-v3.5.6-linux-amd64.tar.gz
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 解压安装，--strip-components=N表示解压时忽略解压后的N层目录，直接获取N层目录后的目标文件。kubernetes/server/bin/是三层，etcd-v3.5.6-linux-amd64/是一层</span>
</span></span><span class="line"><span class="cl">tar -zxvf kubernetes-server-linux-amd64.tar.gz --strip-components<span class="o">=</span><span class="m">3</span> -C /usr/local/bin kubernetes/server/bin/kube<span class="o">{</span>let,ctl,-apiserver,-controller-manager,-scheduler,-proxy<span class="o">}</span>
</span></span><span class="line"><span class="cl">tar -zxvf etcd-v3.5.6-linux-amd64.tar.gz --strip-components<span class="o">=</span><span class="m">1</span> -C /usr/local/bin etcd-v3.5.6-linux-amd64/etcd<span class="o">{</span>,ctl<span class="o">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 确认版本</span>
</span></span><span class="line"><span class="cl">kubectl version
</span></span><span class="line"><span class="cl">kubelet --version
</span></span><span class="line"><span class="cl">etcdctl version
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 拷贝组件到其他节点，Node节点只需要kubelet和kube-proxy</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> i in k8s-master02 k8s-master03<span class="p">;</span> <span class="k">do</span> scp /usr/local/bin/kube<span class="o">{</span>let,ctl,-apiserver,-controller-manager,-scheduler,-proxy<span class="o">}</span> <span class="nv">$i</span>:/usr/local/bin/<span class="p">;</span> scp /usr/local/bin/etcd* <span class="nv">$i</span>:/usr/local/bin/<span class="p">;</span> <span class="k">done</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> i in k8s-node01 k8s-node02<span class="p">;</span> <span class="k">do</span> scp /usr/local/bin/kube<span class="o">{</span>let,-proxy<span class="o">}</span> <span class="nv">$i</span>:/usr/local/bin/<span class="p">;</span> <span class="k">done</span>
</span></span></code></pre></div><ul>
<li>配置证书</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># master节点创建etcd证书目录</span>
</span></span><span class="line"><span class="cl">mkdir -p /etc/etcd/ssl
</span></span><span class="line"><span class="cl"><span class="c1"># 所有节点创建pki证书目录和CNI目录（后面calico使用）</span>
</span></span><span class="line"><span class="cl">mkdir -p /etc/kubernetes/pki
</span></span><span class="line"><span class="cl">mkdir -p /opt/cni/bin
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 以下在k8s-master01操作</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 安装证书生成工具，如果速度慢可以浏览器下载后上传至服务器改名即可</span>
</span></span><span class="line"><span class="cl">wget <span class="s2">&#34;https://pkg.cfssl.org/R1.2/cfssl_linux-amd64&#34;</span> -O /usr/local/bin/cfssl
</span></span><span class="line"><span class="cl">wget <span class="s2">&#34;https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64&#34;</span> -O /usr/local/bin/cfssljson
</span></span><span class="line"><span class="cl">chmod +x /usr/local/bin/cfssl /usr/local/bin/cfssljson
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 在master01节点生成etcd证书</span>
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> /root/k8s-ha-install/pki
</span></span><span class="line"><span class="cl">cfssl gencert -initca etcd-ca-csr.json <span class="p">|</span> cfssljson -bare /etc/etcd/ssl/etcd-ca
</span></span><span class="line"><span class="cl">cfssl gencert <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>   -ca<span class="o">=</span>/etc/etcd/ssl/etcd-ca.pem <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>   -ca-key<span class="o">=</span>/etc/etcd/ssl/etcd-ca-key.pem <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>   -config<span class="o">=</span>ca-config.json <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>   -hostname<span class="o">=</span>127.0.0.1,k8s-master01,k8s-master02,k8s-master03,192.168.43.201,192.168.43.202,192.168.43.203 <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>   -profile<span class="o">=</span>kubernetes <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>   etcd-csr.json <span class="p">|</span> cfssljson -bare /etc/etcd/ssl/etcd
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 复制etcd证书到其他master节点</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> i in k8s-master02 k8s-master03<span class="p">;</span> <span class="k">do</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> FILE in etcd-ca-key.pem  etcd-ca.pem  etcd-key.pem  etcd.pem<span class="p">;</span> <span class="k">do</span>
</span></span><span class="line"><span class="cl">      scp /etc/etcd/ssl/<span class="si">${</span><span class="nv">FILE</span><span class="si">}</span> <span class="nv">$i</span>:/etc/etcd/ssl/<span class="si">${</span><span class="nv">FILE</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">    <span class="k">done</span>
</span></span><span class="line"><span class="cl"><span class="k">done</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 生成Kubernetes集群ca证书</span>
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> /root/k8s-ha-install/pki
</span></span><span class="line"><span class="cl">cfssl gencert -initca ca-csr.json <span class="p">|</span> cfssljson -bare /etc/kubernetes/pki/ca
</span></span><span class="line"><span class="cl"><span class="c1"># k8s service网段10.96.0.0/12，填入网段第一个IP；集群VIP地址192.168.43.200</span>
</span></span><span class="line"><span class="cl">cfssl gencert -ca<span class="o">=</span>/etc/kubernetes/pki/ca.pem -ca-key<span class="o">=</span>/etc/kubernetes/pki/ca-key.pem -config<span class="o">=</span>ca-config.json -hostname<span class="o">=</span>10.96.0.1,192.168.43.200,127.0.0.1,kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.default.svc.cluster.local,192.168.43.201,192.168.43.202,192.168.43.203 -profile<span class="o">=</span>kubernetes apiserver-csr.json <span class="p">|</span> cfssljson -bare /etc/kubernetes/pki/apiserver
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 生成apiserver的第三方组件使用的聚合证书，告警可以忽略</span>
</span></span><span class="line"><span class="cl">cfssl gencert -initca front-proxy-ca-csr.json <span class="p">|</span> cfssljson -bare /etc/kubernetes/pki/front-proxy-ca 
</span></span><span class="line"><span class="cl">cfssl gencert -ca<span class="o">=</span>/etc/kubernetes/pki/front-proxy-ca.pem -ca-key<span class="o">=</span>/etc/kubernetes/pki/front-proxy-ca-key.pem -config<span class="o">=</span>ca-config.json -profile<span class="o">=</span>kubernetes front-proxy-client-csr.json <span class="p">|</span> cfssljson -bare /etc/kubernetes/pki/front-proxy-client
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 生成controller-manager证书</span>
</span></span><span class="line"><span class="cl">cfssl gencert <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>   -ca<span class="o">=</span>/etc/kubernetes/pki/ca.pem <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>   -ca-key<span class="o">=</span>/etc/kubernetes/pki/ca-key.pem <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>   -config<span class="o">=</span>ca-config.json <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>   -profile<span class="o">=</span>kubernetes <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>   manager-csr.json <span class="p">|</span> cfssljson -bare /etc/kubernetes/pki/controller-manager
</span></span><span class="line"><span class="cl"><span class="c1"># 设置集群信息，集群名：kubernetes  server：https://192.168.43.200:8443</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 如果不是高可用集群，192.168.43.200:8443改为master01的地址，8443改为apiserver的端口，默认是6443</span>
</span></span><span class="line"><span class="cl">kubectl config set-cluster kubernetes <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>     --certificate-authority<span class="o">=</span>/etc/kubernetes/pki/ca.pem <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>     --embed-certs<span class="o">=</span><span class="nb">true</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>     --server<span class="o">=</span>https://192.168.43.200:8443 <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>     --kubeconfig<span class="o">=</span>/etc/kubernetes/controller-manager.kubeconfig
</span></span><span class="line"><span class="cl"><span class="c1"># 设置上下文信息，context为system:kube-controller-manager@kubernetes</span>
</span></span><span class="line"><span class="cl">kubectl config set-context system:kube-controller-manager@kubernetes <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --cluster<span class="o">=</span>kubernetes <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --user<span class="o">=</span>system:kube-controller-manager <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --kubeconfig<span class="o">=</span>/etc/kubernetes/controller-manager.kubeconfig
</span></span><span class="line"><span class="cl"><span class="c1"># 设置用户认证信息，用户为system:kube-controller-manager</span>
</span></span><span class="line"><span class="cl">kubectl config set-credentials system:kube-controller-manager <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>     --client-certificate<span class="o">=</span>/etc/kubernetes/pki/controller-manager.pem <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>     --client-key<span class="o">=</span>/etc/kubernetes/pki/controller-manager-key.pem <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>     --embed-certs<span class="o">=</span><span class="nb">true</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>     --kubeconfig<span class="o">=</span>/etc/kubernetes/controller-manager.kubeconfig
</span></span><span class="line"><span class="cl"><span class="c1"># 设置默认集群环境</span>
</span></span><span class="line"><span class="cl">kubectl config use-context system:kube-controller-manager@kubernetes <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>     --kubeconfig<span class="o">=</span>/etc/kubernetes/controller-manager.kubeconfig
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 生成scheduler证书</span>
</span></span><span class="line"><span class="cl">cfssl gencert <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>   -ca<span class="o">=</span>/etc/kubernetes/pki/ca.pem <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>   -ca-key<span class="o">=</span>/etc/kubernetes/pki/ca-key.pem <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>   -config<span class="o">=</span>ca-config.json <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>   -profile<span class="o">=</span>kubernetes <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>   scheduler-csr.json <span class="p">|</span> cfssljson -bare /etc/kubernetes/pki/scheduler
</span></span><span class="line"><span class="cl"><span class="c1"># 同样的，scheduler需要和controller-manager设置相同的集群上下文等信息</span>
</span></span><span class="line"><span class="cl">kubectl config set-cluster kubernetes <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>     --certificate-authority<span class="o">=</span>/etc/kubernetes/pki/ca.pem <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>     --embed-certs<span class="o">=</span><span class="nb">true</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>     --server<span class="o">=</span>https://192.168.43.200:8443 <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>     --kubeconfig<span class="o">=</span>/etc/kubernetes/scheduler.kubeconfig
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">kubectl config set-credentials system:kube-scheduler <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>     --client-certificate<span class="o">=</span>/etc/kubernetes/pki/scheduler.pem <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>     --client-key<span class="o">=</span>/etc/kubernetes/pki/scheduler-key.pem <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>     --embed-certs<span class="o">=</span><span class="nb">true</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>     --kubeconfig<span class="o">=</span>/etc/kubernetes/scheduler.kubeconfig
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">kubectl config set-context system:kube-scheduler@kubernetes <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>     --cluster<span class="o">=</span>kubernetes <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>     --user<span class="o">=</span>system:kube-scheduler <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>     --kubeconfig<span class="o">=</span>/etc/kubernetes/scheduler.kubeconfig
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">kubectl config use-context system:kube-scheduler@kubernetes <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>     --kubeconfig<span class="o">=</span>/etc/kubernetes/scheduler.kubeconfig
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 配置admin证书，以及集群上下文等信息</span>
</span></span><span class="line"><span class="cl">cfssl gencert <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>   -ca<span class="o">=</span>/etc/kubernetes/pki/ca.pem <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>   -ca-key<span class="o">=</span>/etc/kubernetes/pki/ca-key.pem <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>   -config<span class="o">=</span>ca-config.json <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>   -profile<span class="o">=</span>kubernetes <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>   admin-csr.json <span class="p">|</span> cfssljson -bare /etc/kubernetes/pki/admin
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">kubectl config set-cluster kubernetes --certificate-authority<span class="o">=</span>/etc/kubernetes/pki/ca.pem --embed-certs<span class="o">=</span><span class="nb">true</span> --server<span class="o">=</span>https://192.168.43.200:8443 --kubeconfig<span class="o">=</span>/etc/kubernetes/admin.kubeconfig
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">kubectl config set-credentials kubernetes-admin --client-certificate<span class="o">=</span>/etc/kubernetes/pki/admin.pem --client-key<span class="o">=</span>/etc/kubernetes/pki/admin-key.pem --embed-certs<span class="o">=</span><span class="nb">true</span> --kubeconfig<span class="o">=</span>/etc/kubernetes/admin.kubeconfig
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">kubectl config set-context kubernetes-admin@kubernetes --cluster<span class="o">=</span>kubernetes --user<span class="o">=</span>kubernetes-admin --kubeconfig<span class="o">=</span>/etc/kubernetes/admin.kubeconfig
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">kubectl config use-context kubernetes-admin@kubernetes --kubeconfig<span class="o">=</span>/etc/kubernetes/admin.kubeconfig
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 创建ServiceAccount密钥对</span>
</span></span><span class="line"><span class="cl">openssl genrsa -out /etc/kubernetes/pki/sa.key <span class="m">2048</span>
</span></span><span class="line"><span class="cl">openssl rsa -in /etc/kubernetes/pki/sa.key -pubout -out /etc/kubernetes/pki/sa.pub
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 拷贝证书到其他master节点</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> i in k8s-master02 k8s-master03<span class="p">;</span> <span class="k">do</span> 
</span></span><span class="line"><span class="cl">    <span class="k">for</span> FILE in <span class="k">$(</span>ls /etc/kubernetes/pki <span class="p">|</span> grep -v etcd<span class="k">)</span><span class="p">;</span> <span class="k">do</span> 
</span></span><span class="line"><span class="cl">        scp /etc/kubernetes/pki/<span class="si">${</span><span class="nv">FILE</span><span class="si">}</span> <span class="nv">$i</span>:/etc/kubernetes/pki/<span class="si">${</span><span class="nv">FILE</span><span class="si">}</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">done</span><span class="p">;</span> 
</span></span><span class="line"><span class="cl">    <span class="k">for</span> FILE in admin.kubeconfig controller-manager.kubeconfig scheduler.kubeconfig<span class="p">;</span> <span class="k">do</span> 
</span></span><span class="line"><span class="cl">        scp /etc/kubernetes/<span class="si">${</span><span class="nv">FILE</span><span class="si">}</span> <span class="nv">$i</span>:/etc/kubernetes/<span class="si">${</span><span class="nv">FILE</span><span class="si">}</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">done</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="k">done</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 查看证书数量是否为23</span>
</span></span><span class="line"><span class="cl">ls /etc/kubernetes/pki/ <span class="p">|</span> wc -l
</span></span><span class="line"><span class="cl"><span class="m">23</span>
</span></span></code></pre></div><h3 id="etcd集群master">ETCD集群(Master)<a hidden class="anchor" aria-hidden="true" href="#etcd集群master">#</a></h3>
<blockquote>
<p>如果Etcd集群服务器和Kubernetes集群服务器不重合（即独立的Etcd集群），需要根据实际情况配置集群IP。</p>
</blockquote>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># master01</span>
</span></span><span class="line"><span class="cl">cat &lt;&lt; <span class="s2">&#34;EOF&#34;</span> &gt; /etc/etcd/etcd.config.yml
</span></span><span class="line"><span class="cl">name: <span class="s1">&#39;k8s-master01&#39;</span>
</span></span><span class="line"><span class="cl">data-dir: /var/lib/etcd
</span></span><span class="line"><span class="cl">wal-dir: /var/lib/etcd/wal
</span></span><span class="line"><span class="cl">snapshot-count: <span class="m">5000</span>
</span></span><span class="line"><span class="cl">heartbeat-interval: <span class="m">100</span>
</span></span><span class="line"><span class="cl">election-timeout: <span class="m">1000</span>
</span></span><span class="line"><span class="cl">quota-backend-bytes: <span class="m">0</span>
</span></span><span class="line"><span class="cl">listen-peer-urls: <span class="s1">&#39;https://192.168.43.201:2380&#39;</span>
</span></span><span class="line"><span class="cl">listen-client-urls: <span class="s1">&#39;https://192.168.43.201:2379,http://127.0.0.1:2379&#39;</span>
</span></span><span class="line"><span class="cl">max-snapshots: <span class="m">3</span>
</span></span><span class="line"><span class="cl">max-wals: <span class="m">5</span>
</span></span><span class="line"><span class="cl">cors:
</span></span><span class="line"><span class="cl">initial-advertise-peer-urls: <span class="s1">&#39;https://192.168.43.201:2380&#39;</span>
</span></span><span class="line"><span class="cl">advertise-client-urls: <span class="s1">&#39;https://192.168.43.201:2379&#39;</span>
</span></span><span class="line"><span class="cl">discovery:
</span></span><span class="line"><span class="cl">discovery-fallback: <span class="s1">&#39;proxy&#39;</span>
</span></span><span class="line"><span class="cl">discovery-proxy:
</span></span><span class="line"><span class="cl">discovery-srv:
</span></span><span class="line"><span class="cl">initial-cluster: <span class="s1">&#39;k8s-master01=https://192.168.43.201:2380,k8s-master02=https://192.168.43.202:2380,k8s-master03=https://192.168.43.203:2380&#39;</span>
</span></span><span class="line"><span class="cl">initial-cluster-token: <span class="s1">&#39;etcd-k8s-cluster&#39;</span>
</span></span><span class="line"><span class="cl">initial-cluster-state: <span class="s1">&#39;new&#39;</span>
</span></span><span class="line"><span class="cl">strict-reconfig-check: <span class="nb">false</span>
</span></span><span class="line"><span class="cl">enable-v2: <span class="nb">true</span>
</span></span><span class="line"><span class="cl">enable-pprof: <span class="nb">true</span>
</span></span><span class="line"><span class="cl">proxy: <span class="s1">&#39;off&#39;</span>
</span></span><span class="line"><span class="cl">proxy-failure-wait: <span class="m">5000</span>
</span></span><span class="line"><span class="cl">proxy-refresh-interval: <span class="m">30000</span>
</span></span><span class="line"><span class="cl">proxy-dial-timeout: <span class="m">1000</span>
</span></span><span class="line"><span class="cl">proxy-write-timeout: <span class="m">5000</span>
</span></span><span class="line"><span class="cl">proxy-read-timeout: <span class="m">0</span>
</span></span><span class="line"><span class="cl">client-transport-security:
</span></span><span class="line"><span class="cl">  cert-file: <span class="s1">&#39;/etc/kubernetes/pki/etcd/etcd.pem&#39;</span>
</span></span><span class="line"><span class="cl">  key-file: <span class="s1">&#39;/etc/kubernetes/pki/etcd/etcd-key.pem&#39;</span>
</span></span><span class="line"><span class="cl">  client-cert-auth: <span class="nb">true</span>
</span></span><span class="line"><span class="cl">  trusted-ca-file: <span class="s1">&#39;/etc/kubernetes/pki/etcd/etcd-ca.pem&#39;</span>
</span></span><span class="line"><span class="cl">  auto-tls: <span class="nb">true</span>
</span></span><span class="line"><span class="cl">peer-transport-security:
</span></span><span class="line"><span class="cl">  cert-file: <span class="s1">&#39;/etc/kubernetes/pki/etcd/etcd.pem&#39;</span>
</span></span><span class="line"><span class="cl">  key-file: <span class="s1">&#39;/etc/kubernetes/pki/etcd/etcd-key.pem&#39;</span>
</span></span><span class="line"><span class="cl">  peer-client-cert-auth: <span class="nb">true</span>
</span></span><span class="line"><span class="cl">  trusted-ca-file: <span class="s1">&#39;/etc/kubernetes/pki/etcd/etcd-ca.pem&#39;</span>
</span></span><span class="line"><span class="cl">  auto-tls: <span class="nb">true</span>
</span></span><span class="line"><span class="cl">debug: <span class="nb">false</span>
</span></span><span class="line"><span class="cl">log-package-levels:
</span></span><span class="line"><span class="cl">log-outputs: <span class="o">[</span>default<span class="o">]</span>
</span></span><span class="line"><span class="cl">force-new-cluster: <span class="nb">false</span>
</span></span><span class="line"><span class="cl">EOF
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># master02</span>
</span></span><span class="line"><span class="cl">cat &lt;&lt; <span class="s2">&#34;EOF&#34;</span> &gt; /etc/etcd/etcd.config.yml
</span></span><span class="line"><span class="cl">name: <span class="s1">&#39;k8s-master02&#39;</span>
</span></span><span class="line"><span class="cl">data-dir: /var/lib/etcd
</span></span><span class="line"><span class="cl">wal-dir: /var/lib/etcd/wal
</span></span><span class="line"><span class="cl">snapshot-count: <span class="m">5000</span>
</span></span><span class="line"><span class="cl">heartbeat-interval: <span class="m">100</span>
</span></span><span class="line"><span class="cl">election-timeout: <span class="m">1000</span>
</span></span><span class="line"><span class="cl">quota-backend-bytes: <span class="m">0</span>
</span></span><span class="line"><span class="cl">listen-peer-urls: <span class="s1">&#39;https://192.168.43.202:2380&#39;</span>
</span></span><span class="line"><span class="cl">listen-client-urls: <span class="s1">&#39;https://192.168.43.202:2379,http://127.0.0.1:2379&#39;</span>
</span></span><span class="line"><span class="cl">max-snapshots: <span class="m">3</span>
</span></span><span class="line"><span class="cl">max-wals: <span class="m">5</span>
</span></span><span class="line"><span class="cl">cors:
</span></span><span class="line"><span class="cl">initial-advertise-peer-urls: <span class="s1">&#39;https://192.168.43.202:2380&#39;</span>
</span></span><span class="line"><span class="cl">advertise-client-urls: <span class="s1">&#39;https://192.168.43.202:2379&#39;</span>
</span></span><span class="line"><span class="cl">discovery:
</span></span><span class="line"><span class="cl">discovery-fallback: <span class="s1">&#39;proxy&#39;</span>
</span></span><span class="line"><span class="cl">discovery-proxy:
</span></span><span class="line"><span class="cl">discovery-srv:
</span></span><span class="line"><span class="cl">initial-cluster: <span class="s1">&#39;k8s-master01=https://192.168.43.201:2380,k8s-master02=https://192.168.43.202:2380,k8s-master03=https://192.168.43.203:2380&#39;</span>
</span></span><span class="line"><span class="cl">initial-cluster-token: <span class="s1">&#39;etcd-k8s-cluster&#39;</span>
</span></span><span class="line"><span class="cl">initial-cluster-state: <span class="s1">&#39;new&#39;</span>
</span></span><span class="line"><span class="cl">strict-reconfig-check: <span class="nb">false</span>
</span></span><span class="line"><span class="cl">enable-v2: <span class="nb">true</span>
</span></span><span class="line"><span class="cl">enable-pprof: <span class="nb">true</span>
</span></span><span class="line"><span class="cl">proxy: <span class="s1">&#39;off&#39;</span>
</span></span><span class="line"><span class="cl">proxy-failure-wait: <span class="m">5000</span>
</span></span><span class="line"><span class="cl">proxy-refresh-interval: <span class="m">30000</span>
</span></span><span class="line"><span class="cl">proxy-dial-timeout: <span class="m">1000</span>
</span></span><span class="line"><span class="cl">proxy-write-timeout: <span class="m">5000</span>
</span></span><span class="line"><span class="cl">proxy-read-timeout: <span class="m">0</span>
</span></span><span class="line"><span class="cl">client-transport-security:
</span></span><span class="line"><span class="cl">  cert-file: <span class="s1">&#39;/etc/kubernetes/pki/etcd/etcd.pem&#39;</span>
</span></span><span class="line"><span class="cl">  key-file: <span class="s1">&#39;/etc/kubernetes/pki/etcd/etcd-key.pem&#39;</span>
</span></span><span class="line"><span class="cl">  client-cert-auth: <span class="nb">true</span>
</span></span><span class="line"><span class="cl">  trusted-ca-file: <span class="s1">&#39;/etc/kubernetes/pki/etcd/etcd-ca.pem&#39;</span>
</span></span><span class="line"><span class="cl">  auto-tls: <span class="nb">true</span>
</span></span><span class="line"><span class="cl">peer-transport-security:
</span></span><span class="line"><span class="cl">  cert-file: <span class="s1">&#39;/etc/kubernetes/pki/etcd/etcd.pem&#39;</span>
</span></span><span class="line"><span class="cl">  key-file: <span class="s1">&#39;/etc/kubernetes/pki/etcd/etcd-key.pem&#39;</span>
</span></span><span class="line"><span class="cl">  peer-client-cert-auth: <span class="nb">true</span>
</span></span><span class="line"><span class="cl">  trusted-ca-file: <span class="s1">&#39;/etc/kubernetes/pki/etcd/etcd-ca.pem&#39;</span>
</span></span><span class="line"><span class="cl">  auto-tls: <span class="nb">true</span>
</span></span><span class="line"><span class="cl">debug: <span class="nb">false</span>
</span></span><span class="line"><span class="cl">log-package-levels:
</span></span><span class="line"><span class="cl">log-outputs: <span class="o">[</span>default<span class="o">]</span>
</span></span><span class="line"><span class="cl">force-new-cluster: <span class="nb">false</span>
</span></span><span class="line"><span class="cl">EOF
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># master03</span>
</span></span><span class="line"><span class="cl">cat &lt;&lt; <span class="s2">&#34;EOF&#34;</span> &gt; /etc/etcd/etcd.config.yml
</span></span><span class="line"><span class="cl">name: <span class="s1">&#39;k8s-master03&#39;</span>
</span></span><span class="line"><span class="cl">data-dir: /var/lib/etcd
</span></span><span class="line"><span class="cl">wal-dir: /var/lib/etcd/wal
</span></span><span class="line"><span class="cl">snapshot-count: <span class="m">5000</span>
</span></span><span class="line"><span class="cl">heartbeat-interval: <span class="m">100</span>
</span></span><span class="line"><span class="cl">election-timeout: <span class="m">1000</span>
</span></span><span class="line"><span class="cl">quota-backend-bytes: <span class="m">0</span>
</span></span><span class="line"><span class="cl">listen-peer-urls: <span class="s1">&#39;https://192.168.43.203:2380&#39;</span>
</span></span><span class="line"><span class="cl">listen-client-urls: <span class="s1">&#39;https://192.168.43.203:2379,http://127.0.0.1:2379&#39;</span>
</span></span><span class="line"><span class="cl">max-snapshots: <span class="m">3</span>
</span></span><span class="line"><span class="cl">max-wals: <span class="m">5</span>
</span></span><span class="line"><span class="cl">cors:
</span></span><span class="line"><span class="cl">initial-advertise-peer-urls: <span class="s1">&#39;https://192.168.43.203:2380&#39;</span>
</span></span><span class="line"><span class="cl">advertise-client-urls: <span class="s1">&#39;https://192.168.43.203:2379&#39;</span>
</span></span><span class="line"><span class="cl">discovery:
</span></span><span class="line"><span class="cl">discovery-fallback: <span class="s1">&#39;proxy&#39;</span>
</span></span><span class="line"><span class="cl">discovery-proxy:
</span></span><span class="line"><span class="cl">discovery-srv:
</span></span><span class="line"><span class="cl">initial-cluster: <span class="s1">&#39;k8s-master01=https://192.168.43.201:2380,k8s-master02=https://192.168.43.202:2380,k8s-master03=https://192.168.43.203:2380&#39;</span>
</span></span><span class="line"><span class="cl">initial-cluster-token: <span class="s1">&#39;etcd-k8s-cluster&#39;</span>
</span></span><span class="line"><span class="cl">initial-cluster-state: <span class="s1">&#39;new&#39;</span>
</span></span><span class="line"><span class="cl">strict-reconfig-check: <span class="nb">false</span>
</span></span><span class="line"><span class="cl">enable-v2: <span class="nb">true</span>
</span></span><span class="line"><span class="cl">enable-pprof: <span class="nb">true</span>
</span></span><span class="line"><span class="cl">proxy: <span class="s1">&#39;off&#39;</span>
</span></span><span class="line"><span class="cl">proxy-failure-wait: <span class="m">5000</span>
</span></span><span class="line"><span class="cl">proxy-refresh-interval: <span class="m">30000</span>
</span></span><span class="line"><span class="cl">proxy-dial-timeout: <span class="m">1000</span>
</span></span><span class="line"><span class="cl">proxy-write-timeout: <span class="m">5000</span>
</span></span><span class="line"><span class="cl">proxy-read-timeout: <span class="m">0</span>
</span></span><span class="line"><span class="cl">client-transport-security:
</span></span><span class="line"><span class="cl">  cert-file: <span class="s1">&#39;/etc/kubernetes/pki/etcd/etcd.pem&#39;</span>
</span></span><span class="line"><span class="cl">  key-file: <span class="s1">&#39;/etc/kubernetes/pki/etcd/etcd-key.pem&#39;</span>
</span></span><span class="line"><span class="cl">  client-cert-auth: <span class="nb">true</span>
</span></span><span class="line"><span class="cl">  trusted-ca-file: <span class="s1">&#39;/etc/kubernetes/pki/etcd/etcd-ca.pem&#39;</span>
</span></span><span class="line"><span class="cl">  auto-tls: <span class="nb">true</span>
</span></span><span class="line"><span class="cl">peer-transport-security:
</span></span><span class="line"><span class="cl">  cert-file: <span class="s1">&#39;/etc/kubernetes/pki/etcd/etcd.pem&#39;</span>
</span></span><span class="line"><span class="cl">  key-file: <span class="s1">&#39;/etc/kubernetes/pki/etcd/etcd-key.pem&#39;</span>
</span></span><span class="line"><span class="cl">  peer-client-cert-auth: <span class="nb">true</span>
</span></span><span class="line"><span class="cl">  trusted-ca-file: <span class="s1">&#39;/etc/kubernetes/pki/etcd/etcd-ca.pem&#39;</span>
</span></span><span class="line"><span class="cl">  auto-tls: <span class="nb">true</span>
</span></span><span class="line"><span class="cl">debug: <span class="nb">false</span>
</span></span><span class="line"><span class="cl">log-package-levels:
</span></span><span class="line"><span class="cl">log-outputs: <span class="o">[</span>default<span class="o">]</span>
</span></span><span class="line"><span class="cl">force-new-cluster: <span class="nb">false</span>
</span></span><span class="line"><span class="cl">EOF
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 在所有master节点创建etcd服务并启动</span>
</span></span><span class="line"><span class="cl">cat &lt;&lt; <span class="s2">&#34;EOF&#34;</span> &gt; /usr/lib/systemd/system/etcd.service
</span></span><span class="line"><span class="cl"><span class="o">[</span>Unit<span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="nv">Description</span><span class="o">=</span>Etcd Service
</span></span><span class="line"><span class="cl"><span class="nv">Documentation</span><span class="o">=</span>https://coreos.com/etcd/docs/latest/
</span></span><span class="line"><span class="cl"><span class="nv">After</span><span class="o">=</span>network.target
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">[</span>Service<span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="nv">Type</span><span class="o">=</span>notify
</span></span><span class="line"><span class="cl"><span class="nv">ExecStart</span><span class="o">=</span>/usr/local/bin/etcd --config-file<span class="o">=</span>/etc/etcd/etcd.config.yml
</span></span><span class="line"><span class="cl"><span class="nv">Restart</span><span class="o">=</span>on-failure
</span></span><span class="line"><span class="cl"><span class="nv">RestartSec</span><span class="o">=</span><span class="m">10</span>
</span></span><span class="line"><span class="cl"><span class="nv">LimitNOFILE</span><span class="o">=</span><span class="m">65536</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">[</span>Install<span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="nv">WantedBy</span><span class="o">=</span>multi-user.target
</span></span><span class="line"><span class="cl"><span class="nv">Alias</span><span class="o">=</span>etcd3.service
</span></span><span class="line"><span class="cl">EOF
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 所有Master节点创建etcd证书目录并链接证书，否则启动会失败</span>
</span></span><span class="line"><span class="cl">mkdir /etc/kubernetes/pki/etcd
</span></span><span class="line"><span class="cl">ln -s /etc/etcd/ssl/* /etc/kubernetes/pki/etcd/
</span></span><span class="line"><span class="cl"><span class="c1"># 启动</span>
</span></span><span class="line"><span class="cl">systemctl daemon-reload <span class="o">&amp;&amp;</span> systemctl <span class="nb">enable</span> --now etcd
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 查看etcd集群状态如下即可</span>
</span></span><span class="line"><span class="cl"><span class="nv">ETCDCTL_API</span><span class="o">=</span><span class="m">3</span> etcdctl --endpoints<span class="o">=</span><span class="s2">&#34;192.168.43.203:2379,192.168.43.202:2379,192.168.43.201:2379&#34;</span> --cacert<span class="o">=</span>/etc/kubernetes/pki/etcd/etcd-ca.pem --cert<span class="o">=</span>/etc/kubernetes/pki/etcd/etcd.pem --key<span class="o">=</span>/etc/kubernetes/pki/etcd/etcd-key.pem  endpoint status --write-out<span class="o">=</span>table
</span></span><span class="line"><span class="cl">+---------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
</span></span><span class="line"><span class="cl"><span class="p">|</span>      ENDPOINT       <span class="p">|</span>        ID        <span class="p">|</span> VERSION <span class="p">|</span> DB SIZE <span class="p">|</span> IS LEADER <span class="p">|</span> IS LEARNER <span class="p">|</span> RAFT TERM <span class="p">|</span> RAFT INDEX <span class="p">|</span> RAFT APPLIED INDEX <span class="p">|</span> ERRORS <span class="p">|</span>
</span></span><span class="line"><span class="cl">+---------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
</span></span><span class="line"><span class="cl"><span class="p">|</span> 192.168.43.203:2379 <span class="p">|</span>   fd1372a073304e <span class="p">|</span>   3.5.1 <span class="p">|</span>   <span class="m">20</span> kB <span class="p">|</span>     <span class="nb">false</span> <span class="p">|</span>      <span class="nb">false</span> <span class="p">|</span>         <span class="m">2</span> <span class="p">|</span>          <span class="m">9</span> <span class="p">|</span>                  <span class="m">9</span> <span class="p">|</span>        <span class="p">|</span>
</span></span><span class="line"><span class="cl"><span class="p">|</span> 192.168.43.202:2379 <span class="p">|</span> 837ce9c47e0719eb <span class="p">|</span>   3.5.1 <span class="p">|</span>   <span class="m">20</span> kB <span class="p">|</span>     <span class="nb">false</span> <span class="p">|</span>      <span class="nb">false</span> <span class="p">|</span>         <span class="m">2</span> <span class="p">|</span>          <span class="m">9</span> <span class="p">|</span>                  <span class="m">9</span> <span class="p">|</span>        <span class="p">|</span>
</span></span><span class="line"><span class="cl"><span class="p">|</span> 192.168.43.201:2379 <span class="p">|</span> e9bf8d99824c9061 <span class="p">|</span>   3.5.1 <span class="p">|</span>   <span class="m">20</span> kB <span class="p">|</span>      <span class="nb">true</span> <span class="p">|</span>      <span class="nb">false</span> <span class="p">|</span>         <span class="m">2</span> <span class="p">|</span>          <span class="m">9</span> <span class="p">|</span>                  <span class="m">9</span> <span class="p">|</span>        <span class="p">|</span>
</span></span><span class="line"><span class="cl">+---------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
</span></span></code></pre></div><h3 id="高可用组件master">高可用组件(Master)<a hidden class="anchor" aria-hidden="true" href="#高可用组件master">#</a></h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># 所有master节点安装Keepalived和haproxy，并创建配置文件目录</span>
</span></span><span class="line"><span class="cl">yum install keepalived haproxy -y
</span></span><span class="line"><span class="cl">mkdir /etc/haproxy
</span></span><span class="line"><span class="cl">mkdir /etc/keepalived
</span></span><span class="line"><span class="cl"><span class="c1"># 为所有master节点添加haproxy配置，配置都一样，检查最后三行主机名和IP地址对应上就行</span>
</span></span><span class="line"><span class="cl">cat &lt;&lt; <span class="s2">&#34;EOF&#34;</span> &gt; /etc/haproxy/haproxy.cfg
</span></span><span class="line"><span class="cl">global
</span></span><span class="line"><span class="cl">  maxconn  <span class="m">2000</span>
</span></span><span class="line"><span class="cl">  ulimit-n  <span class="m">16384</span>
</span></span><span class="line"><span class="cl">  log  127.0.0.1 local0 err
</span></span><span class="line"><span class="cl">  stats timeout 30s
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">defaults
</span></span><span class="line"><span class="cl">  log global
</span></span><span class="line"><span class="cl">  mode  http
</span></span><span class="line"><span class="cl">  option  httplog
</span></span><span class="line"><span class="cl">  timeout connect <span class="m">5000</span>
</span></span><span class="line"><span class="cl">  timeout client  <span class="m">50000</span>
</span></span><span class="line"><span class="cl">  timeout server  <span class="m">50000</span>
</span></span><span class="line"><span class="cl">  timeout http-request 15s
</span></span><span class="line"><span class="cl">  timeout http-keep-alive 15s
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">frontend monitor-in
</span></span><span class="line"><span class="cl">  <span class="nb">bind</span> *:33305
</span></span><span class="line"><span class="cl">  mode http
</span></span><span class="line"><span class="cl">  option httplog
</span></span><span class="line"><span class="cl">  monitor-uri /monitor
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">frontend k8s-master
</span></span><span class="line"><span class="cl">  <span class="nb">bind</span> 0.0.0.0:8443
</span></span><span class="line"><span class="cl">  <span class="nb">bind</span> 127.0.0.1:8443
</span></span><span class="line"><span class="cl">  mode tcp
</span></span><span class="line"><span class="cl">  option tcplog
</span></span><span class="line"><span class="cl">  tcp-request inspect-delay 5s
</span></span><span class="line"><span class="cl">  default_backend k8s-master
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">backend k8s-master
</span></span><span class="line"><span class="cl">  mode tcp
</span></span><span class="line"><span class="cl">  option tcplog
</span></span><span class="line"><span class="cl">  option tcp-check
</span></span><span class="line"><span class="cl">  balance roundrobin
</span></span><span class="line"><span class="cl">  default-server inter 10s downinter 5s rise <span class="m">2</span> fall <span class="m">2</span> slowstart 60s maxconn <span class="m">250</span> maxqueue <span class="m">256</span> weight <span class="m">100</span>
</span></span><span class="line"><span class="cl">  server k8s-master01  192.168.43.201:6443  check
</span></span><span class="line"><span class="cl">  server k8s-master02  192.168.43.202:6443  check
</span></span><span class="line"><span class="cl">  server k8s-master03  192.168.43.203:6443  check
</span></span><span class="line"><span class="cl">EOF
</span></span><span class="line"><span class="cl"><span class="c1"># keepalived配置不一样，注意区分网卡名、IP地址和虚拟IP地址</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 检查服务器网卡名</span>
</span></span><span class="line"><span class="cl">ip a 或 ifconfig
</span></span><span class="line"><span class="cl"><span class="c1"># k8s-master01 Keepalived配置</span>
</span></span><span class="line"><span class="cl">cat &lt;&lt; <span class="s2">&#34;EOF&#34;</span> &gt; /etc/keepalived/keepalived.conf
</span></span><span class="line"><span class="cl">! Configuration File <span class="k">for</span> keepalived
</span></span><span class="line"><span class="cl">global_defs <span class="o">{</span>
</span></span><span class="line"><span class="cl">    router_id LVS_DEVEL
</span></span><span class="line"><span class="cl">    script_user root
</span></span><span class="line"><span class="cl">    enable_script_security
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span><span class="line"><span class="cl">vrrp_script chk_apiserver <span class="o">{</span>
</span></span><span class="line"><span class="cl">    script <span class="s2">&#34;/etc/keepalived/check_apiserver.sh&#34;</span>
</span></span><span class="line"><span class="cl">    interval <span class="m">5</span>
</span></span><span class="line"><span class="cl">    weight -5
</span></span><span class="line"><span class="cl">    fall <span class="m">2</span>  
</span></span><span class="line"><span class="cl">    rise <span class="m">1</span>
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span><span class="line"><span class="cl">vrrp_instance VI_1 <span class="o">{</span>
</span></span><span class="line"><span class="cl">    state MASTER
</span></span><span class="line"><span class="cl">    interface ens33
</span></span><span class="line"><span class="cl">    mcast_src_ip 192.168.43.201
</span></span><span class="line"><span class="cl">    virtual_router_id <span class="m">51</span>
</span></span><span class="line"><span class="cl">    priority <span class="m">101</span>
</span></span><span class="line"><span class="cl">    advert_int <span class="m">2</span>
</span></span><span class="line"><span class="cl">    authentication <span class="o">{</span>
</span></span><span class="line"><span class="cl">        auth_type PASS
</span></span><span class="line"><span class="cl">        auth_pass K8SHA_KA_AUTH
</span></span><span class="line"><span class="cl">    <span class="o">}</span>
</span></span><span class="line"><span class="cl">    virtual_ipaddress <span class="o">{</span>
</span></span><span class="line"><span class="cl">        192.168.43.200
</span></span><span class="line"><span class="cl">    <span class="o">}</span>
</span></span><span class="line"><span class="cl">    track_script <span class="o">{</span>
</span></span><span class="line"><span class="cl">       chk_apiserver
</span></span><span class="line"><span class="cl">    <span class="o">}</span>
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span><span class="line"><span class="cl">EOF
</span></span><span class="line"><span class="cl"><span class="c1"># k8s-master02 Keepalived配置</span>
</span></span><span class="line"><span class="cl">cat &lt;&lt; <span class="s2">&#34;EOF&#34;</span> &gt; /etc/keepalived/keepalived.conf
</span></span><span class="line"><span class="cl">! Configuration File <span class="k">for</span> keepalived
</span></span><span class="line"><span class="cl">global_defs <span class="o">{</span>
</span></span><span class="line"><span class="cl">    router_id LVS_DEVEL
</span></span><span class="line"><span class="cl">    script_user root
</span></span><span class="line"><span class="cl">    enable_script_security
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span><span class="line"><span class="cl">vrrp_script chk_apiserver <span class="o">{</span>
</span></span><span class="line"><span class="cl">    script <span class="s2">&#34;/etc/keepalived/check_apiserver.sh&#34;</span>
</span></span><span class="line"><span class="cl">    interval <span class="m">5</span>
</span></span><span class="line"><span class="cl">    weight -5
</span></span><span class="line"><span class="cl">    fall <span class="m">2</span>  
</span></span><span class="line"><span class="cl">    rise <span class="m">1</span>
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span><span class="line"><span class="cl">vrrp_instance VI_1 <span class="o">{</span>
</span></span><span class="line"><span class="cl">    state BACKUP
</span></span><span class="line"><span class="cl">    interface ens33
</span></span><span class="line"><span class="cl">    mcast_src_ip 192.168.43.202
</span></span><span class="line"><span class="cl">    virtual_router_id <span class="m">51</span>
</span></span><span class="line"><span class="cl">    priority <span class="m">100</span>
</span></span><span class="line"><span class="cl">    advert_int <span class="m">2</span>
</span></span><span class="line"><span class="cl">    authentication <span class="o">{</span>
</span></span><span class="line"><span class="cl">        auth_type PASS
</span></span><span class="line"><span class="cl">        auth_pass K8SHA_KA_AUTH
</span></span><span class="line"><span class="cl">    <span class="o">}</span>
</span></span><span class="line"><span class="cl">    virtual_ipaddress <span class="o">{</span>
</span></span><span class="line"><span class="cl">        192.168.43.200
</span></span><span class="line"><span class="cl">    <span class="o">}</span>
</span></span><span class="line"><span class="cl">    track_script <span class="o">{</span>
</span></span><span class="line"><span class="cl">       chk_apiserver
</span></span><span class="line"><span class="cl">    <span class="o">}</span>
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span><span class="line"><span class="cl">EOF
</span></span><span class="line"><span class="cl"><span class="c1"># k8s-master03 Keepalived配置</span>
</span></span><span class="line"><span class="cl">cat &lt;&lt; <span class="s2">&#34;EOF&#34;</span> &gt; /etc/keepalived/keepalived.conf
</span></span><span class="line"><span class="cl">! Configuration File <span class="k">for</span> keepalived
</span></span><span class="line"><span class="cl">global_defs <span class="o">{</span>
</span></span><span class="line"><span class="cl">    router_id LVS_DEVEL
</span></span><span class="line"><span class="cl">    script_user root
</span></span><span class="line"><span class="cl">    enable_script_security
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span><span class="line"><span class="cl">vrrp_script chk_apiserver <span class="o">{</span>
</span></span><span class="line"><span class="cl">    script <span class="s2">&#34;/etc/keepalived/check_apiserver.sh&#34;</span>
</span></span><span class="line"><span class="cl">    interval <span class="m">5</span>
</span></span><span class="line"><span class="cl">    weight -5
</span></span><span class="line"><span class="cl">    fall <span class="m">2</span>  
</span></span><span class="line"><span class="cl">    rise <span class="m">1</span>
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span><span class="line"><span class="cl">vrrp_instance VI_1 <span class="o">{</span>
</span></span><span class="line"><span class="cl">    state BACKUP
</span></span><span class="line"><span class="cl">    interface ens33
</span></span><span class="line"><span class="cl">    mcast_src_ip 192.168.43.203
</span></span><span class="line"><span class="cl">    virtual_router_id <span class="m">51</span>
</span></span><span class="line"><span class="cl">    priority <span class="m">100</span>
</span></span><span class="line"><span class="cl">    advert_int <span class="m">2</span>
</span></span><span class="line"><span class="cl">    authentication <span class="o">{</span>
</span></span><span class="line"><span class="cl">        auth_type PASS
</span></span><span class="line"><span class="cl">        auth_pass K8SHA_KA_AUTH
</span></span><span class="line"><span class="cl">    <span class="o">}</span>
</span></span><span class="line"><span class="cl">    virtual_ipaddress <span class="o">{</span>
</span></span><span class="line"><span class="cl">        192.168.43.200
</span></span><span class="line"><span class="cl">    <span class="o">}</span>
</span></span><span class="line"><span class="cl">    track_script <span class="o">{</span>
</span></span><span class="line"><span class="cl">       chk_apiserver
</span></span><span class="line"><span class="cl">    <span class="o">}</span>
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span><span class="line"><span class="cl">EOF
</span></span><span class="line"><span class="cl"><span class="c1"># 所有master节点配置Keepalived健康检查脚本</span>
</span></span><span class="line"><span class="cl">cat &lt;&lt; <span class="s2">&#34;EOF&#34;</span> &gt; /etc/keepalived/check_apiserver.sh 
</span></span><span class="line"><span class="cl"><span class="c1">#!/bin/bash</span>
</span></span><span class="line"><span class="cl"><span class="nv">err</span><span class="o">=</span><span class="m">0</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> k in <span class="k">$(</span>seq <span class="m">1</span> 3<span class="k">)</span>
</span></span><span class="line"><span class="cl"><span class="k">do</span>
</span></span><span class="line"><span class="cl">    <span class="nv">check_code</span><span class="o">=</span><span class="k">$(</span>pgrep haproxy<span class="k">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="o">[[</span> <span class="nv">$check_code</span> <span class="o">==</span> <span class="s2">&#34;&#34;</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">        <span class="nv">err</span><span class="o">=</span><span class="k">$(</span>expr <span class="nv">$err</span> + 1<span class="k">)</span>
</span></span><span class="line"><span class="cl">        sleep <span class="m">1</span>
</span></span><span class="line"><span class="cl">        <span class="k">continue</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span>
</span></span><span class="line"><span class="cl">        <span class="nv">err</span><span class="o">=</span><span class="m">0</span>
</span></span><span class="line"><span class="cl">        <span class="nb">break</span>
</span></span><span class="line"><span class="cl">    <span class="k">fi</span>
</span></span><span class="line"><span class="cl"><span class="k">done</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="o">[[</span> <span class="nv">$err</span> !<span class="o">=</span> <span class="s2">&#34;0&#34;</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">    <span class="nb">echo</span> <span class="s2">&#34;systemctl stop keepalived&#34;</span>
</span></span><span class="line"><span class="cl">    /usr/bin/systemctl stop keepalived
</span></span><span class="line"><span class="cl">    <span class="nb">exit</span> <span class="m">1</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span>
</span></span><span class="line"><span class="cl">    <span class="nb">exit</span> <span class="m">0</span>
</span></span><span class="line"><span class="cl"><span class="k">fi</span>
</span></span><span class="line"><span class="cl">EOF
</span></span><span class="line"><span class="cl"><span class="c1"># 赋予可执行权限</span>
</span></span><span class="line"><span class="cl">chmod +x /etc/keepalived/check_apiserver.sh
</span></span><span class="line"><span class="cl"><span class="c1"># 启动haproxy和Keepalived并加入开机启动</span>
</span></span><span class="line"><span class="cl">systemctl daemon-reload <span class="o">&amp;&amp;</span> systemctl <span class="nb">enable</span> --now haproxy <span class="o">&amp;&amp;</span> systemctl <span class="nb">enable</span> --now keepalived
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 检查服务是否正常</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 这种告警可以忽略：Mar 06 14:03:35 k8s-master01 haproxy-systemd-wrapper[1981]: [WARNING] 064/140335 (1982) : config : frontend &#39;GLOBAL&#39; has no &#39;bind&#39;</span>
</span></span><span class="line"><span class="cl">systemctl status haproxy
</span></span><span class="line"><span class="cl">systemctl status keepalived
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 测试一波</span>
</span></span><span class="line"><span class="cl">telnet k8s-master-vip <span class="m">8443</span>
</span></span><span class="line"><span class="cl">ping k8s-master-vip
</span></span></code></pre></div><h3 id="配置集群组件">配置集群组件<a hidden class="anchor" aria-hidden="true" href="#配置集群组件">#</a></h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># 所有节点创建以下集群资源目录</span>
</span></span><span class="line"><span class="cl">mkdir -p /etc/kubernetes/manifests/ /etc/systemd/system/kubelet.service.d /var/lib/kubelet /var/log/kubernetes
</span></span></code></pre></div><p><strong>Apiserver(Master)</strong></p>
<ul>
<li>所有master节点配置kube-apiserver服务</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># service网段为10.96.0.0/12，可自行设置，其他参数按需配置即可</span>
</span></span><span class="line"><span class="cl"><span class="c1"># master01配置</span>
</span></span><span class="line"><span class="cl">cat &lt;&lt; <span class="s2">&#34;EOF&#34;</span> &gt; /usr/lib/systemd/system/kube-apiserver.service 
</span></span><span class="line"><span class="cl"><span class="o">[</span>Unit<span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="nv">Description</span><span class="o">=</span>Kubernetes API Server
</span></span><span class="line"><span class="cl"><span class="nv">Documentation</span><span class="o">=</span>https://github.com/kubernetes/kubernetes
</span></span><span class="line"><span class="cl"><span class="nv">After</span><span class="o">=</span>network.target
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">[</span>Service<span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="nv">ExecStart</span><span class="o">=</span>/usr/local/bin/kube-apiserver <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --v<span class="o">=</span><span class="m">2</span>  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --allow-privileged<span class="o">=</span><span class="nb">true</span>  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --bind-address<span class="o">=</span>0.0.0.0  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --secure-port<span class="o">=</span><span class="m">6443</span>  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --advertise-address<span class="o">=</span>192.168.43.201 <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --service-cluster-ip-range<span class="o">=</span>10.96.0.0/12  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --service-node-port-range<span class="o">=</span>30000-32767  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --etcd-servers<span class="o">=</span>https://192.168.43.201:2379,https://192.168.43.202:2379,https://192.168.43.203:2379 <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --etcd-cafile<span class="o">=</span>/etc/etcd/ssl/etcd-ca.pem  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --etcd-certfile<span class="o">=</span>/etc/etcd/ssl/etcd.pem  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --etcd-keyfile<span class="o">=</span>/etc/etcd/ssl/etcd-key.pem  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --client-ca-file<span class="o">=</span>/etc/kubernetes/pki/ca.pem  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --tls-cert-file<span class="o">=</span>/etc/kubernetes/pki/apiserver.pem  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --tls-private-key-file<span class="o">=</span>/etc/kubernetes/pki/apiserver-key.pem  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --kubelet-client-certificate<span class="o">=</span>/etc/kubernetes/pki/apiserver.pem  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --kubelet-client-key<span class="o">=</span>/etc/kubernetes/pki/apiserver-key.pem  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --service-account-key-file<span class="o">=</span>/etc/kubernetes/pki/sa.pub  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --service-account-signing-key-file<span class="o">=</span>/etc/kubernetes/pki/sa.key  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --service-account-issuer<span class="o">=</span>https://kubernetes.default.svc.cluster.local <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --kubelet-preferred-address-types<span class="o">=</span>InternalIP,ExternalIP,Hostname  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --enable-admission-plugins<span class="o">=</span>NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --authorization-mode<span class="o">=</span>Node,RBAC  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --enable-bootstrap-token-auth<span class="o">=</span><span class="nb">true</span>  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --requestheader-client-ca-file<span class="o">=</span>/etc/kubernetes/pki/front-proxy-ca.pem  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --proxy-client-cert-file<span class="o">=</span>/etc/kubernetes/pki/front-proxy-client.pem  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --proxy-client-key-file<span class="o">=</span>/etc/kubernetes/pki/front-proxy-client-key.pem  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --requestheader-allowed-names<span class="o">=</span>aggregator  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --requestheader-group-headers<span class="o">=</span>X-Remote-Group  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --requestheader-extra-headers-prefix<span class="o">=</span>X-Remote-Extra-  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --requestheader-username-headers<span class="o">=</span>X-Remote-User
</span></span><span class="line"><span class="cl">      <span class="c1"># --token-auth-file=/etc/kubernetes/token.csv</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nv">Restart</span><span class="o">=</span>on-failure
</span></span><span class="line"><span class="cl"><span class="nv">RestartSec</span><span class="o">=</span>10s
</span></span><span class="line"><span class="cl"><span class="nv">LimitNOFILE</span><span class="o">=</span><span class="m">65535</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">[</span>Install<span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="nv">WantedBy</span><span class="o">=</span>multi-user.target
</span></span><span class="line"><span class="cl">EOF
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># master02配置</span>
</span></span><span class="line"><span class="cl">cat &lt;&lt; <span class="s2">&#34;EOF&#34;</span> &gt;  /usr/lib/systemd/system/kube-apiserver.service 
</span></span><span class="line"><span class="cl"><span class="o">[</span>Unit<span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="nv">Description</span><span class="o">=</span>Kubernetes API Server
</span></span><span class="line"><span class="cl"><span class="nv">Documentation</span><span class="o">=</span>https://github.com/kubernetes/kubernetes
</span></span><span class="line"><span class="cl"><span class="nv">After</span><span class="o">=</span>network.target
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">[</span>Service<span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="nv">ExecStart</span><span class="o">=</span>/usr/local/bin/kube-apiserver <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --v<span class="o">=</span><span class="m">2</span>  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --allow-privileged<span class="o">=</span><span class="nb">true</span>  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --bind-address<span class="o">=</span>0.0.0.0  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --secure-port<span class="o">=</span><span class="m">6443</span>  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --advertise-address<span class="o">=</span>192.168.43.202 <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --service-cluster-ip-range<span class="o">=</span>10.96.0.0/12  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --service-node-port-range<span class="o">=</span>30000-32767  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --etcd-servers<span class="o">=</span>https://192.168.43.201:2379,https://192.168.43.202:2379,https://192.168.43.203:2379 <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --etcd-cafile<span class="o">=</span>/etc/etcd/ssl/etcd-ca.pem  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --etcd-certfile<span class="o">=</span>/etc/etcd/ssl/etcd.pem  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --etcd-keyfile<span class="o">=</span>/etc/etcd/ssl/etcd-key.pem  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --client-ca-file<span class="o">=</span>/etc/kubernetes/pki/ca.pem  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --tls-cert-file<span class="o">=</span>/etc/kubernetes/pki/apiserver.pem  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --tls-private-key-file<span class="o">=</span>/etc/kubernetes/pki/apiserver-key.pem  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --kubelet-client-certificate<span class="o">=</span>/etc/kubernetes/pki/apiserver.pem  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --kubelet-client-key<span class="o">=</span>/etc/kubernetes/pki/apiserver-key.pem  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --service-account-key-file<span class="o">=</span>/etc/kubernetes/pki/sa.pub  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --service-account-signing-key-file<span class="o">=</span>/etc/kubernetes/pki/sa.key  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --service-account-issuer<span class="o">=</span>https://kubernetes.default.svc.cluster.local <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --kubelet-preferred-address-types<span class="o">=</span>InternalIP,ExternalIP,Hostname  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --enable-admission-plugins<span class="o">=</span>NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --authorization-mode<span class="o">=</span>Node,RBAC  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --enable-bootstrap-token-auth<span class="o">=</span><span class="nb">true</span>  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --requestheader-client-ca-file<span class="o">=</span>/etc/kubernetes/pki/front-proxy-ca.pem  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --proxy-client-cert-file<span class="o">=</span>/etc/kubernetes/pki/front-proxy-client.pem  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --proxy-client-key-file<span class="o">=</span>/etc/kubernetes/pki/front-proxy-client-key.pem  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --requestheader-allowed-names<span class="o">=</span>aggregator  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --requestheader-group-headers<span class="o">=</span>X-Remote-Group  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --requestheader-extra-headers-prefix<span class="o">=</span>X-Remote-Extra-  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --requestheader-username-headers<span class="o">=</span>X-Remote-User
</span></span><span class="line"><span class="cl">      <span class="c1"># --token-auth-file=/etc/kubernetes/token.csv</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nv">Restart</span><span class="o">=</span>on-failure
</span></span><span class="line"><span class="cl"><span class="nv">RestartSec</span><span class="o">=</span>10s
</span></span><span class="line"><span class="cl"><span class="nv">LimitNOFILE</span><span class="o">=</span><span class="m">65535</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">[</span>Install<span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="nv">WantedBy</span><span class="o">=</span>multi-user.target
</span></span><span class="line"><span class="cl">EOF
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># master03配置</span>
</span></span><span class="line"><span class="cl">cat &lt;&lt; <span class="s2">&#34;EOF&#34;</span> &gt; /usr/lib/systemd/system/kube-apiserver.service 
</span></span><span class="line"><span class="cl"><span class="o">[</span>Unit<span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="nv">Description</span><span class="o">=</span>Kubernetes API Server
</span></span><span class="line"><span class="cl"><span class="nv">Documentation</span><span class="o">=</span>https://github.com/kubernetes/kubernetes
</span></span><span class="line"><span class="cl"><span class="nv">After</span><span class="o">=</span>network.target
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">[</span>Service<span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="nv">ExecStart</span><span class="o">=</span>/usr/local/bin/kube-apiserver <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --v<span class="o">=</span><span class="m">2</span>  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --allow-privileged<span class="o">=</span><span class="nb">true</span>  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --bind-address<span class="o">=</span>0.0.0.0  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --secure-port<span class="o">=</span><span class="m">6443</span>  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --advertise-address<span class="o">=</span>192.168.43.203 <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --service-cluster-ip-range<span class="o">=</span>10.96.0.0/12  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --service-node-port-range<span class="o">=</span>30000-32767  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --etcd-servers<span class="o">=</span>https://192.168.43.201:2379,https://192.168.43.202:2379,https://192.168.43.203:2379 <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --etcd-cafile<span class="o">=</span>/etc/etcd/ssl/etcd-ca.pem  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --etcd-certfile<span class="o">=</span>/etc/etcd/ssl/etcd.pem  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --etcd-keyfile<span class="o">=</span>/etc/etcd/ssl/etcd-key.pem  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --client-ca-file<span class="o">=</span>/etc/kubernetes/pki/ca.pem  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --tls-cert-file<span class="o">=</span>/etc/kubernetes/pki/apiserver.pem  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --tls-private-key-file<span class="o">=</span>/etc/kubernetes/pki/apiserver-key.pem  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --kubelet-client-certificate<span class="o">=</span>/etc/kubernetes/pki/apiserver.pem  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --kubelet-client-key<span class="o">=</span>/etc/kubernetes/pki/apiserver-key.pem  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --service-account-key-file<span class="o">=</span>/etc/kubernetes/pki/sa.pub  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --service-account-signing-key-file<span class="o">=</span>/etc/kubernetes/pki/sa.key  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --service-account-issuer<span class="o">=</span>https://kubernetes.default.svc.cluster.local <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --kubelet-preferred-address-types<span class="o">=</span>InternalIP,ExternalIP,Hostname  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --enable-admission-plugins<span class="o">=</span>NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --authorization-mode<span class="o">=</span>Node,RBAC  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --enable-bootstrap-token-auth<span class="o">=</span><span class="nb">true</span>  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --requestheader-client-ca-file<span class="o">=</span>/etc/kubernetes/pki/front-proxy-ca.pem  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --proxy-client-cert-file<span class="o">=</span>/etc/kubernetes/pki/front-proxy-client.pem  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --proxy-client-key-file<span class="o">=</span>/etc/kubernetes/pki/front-proxy-client-key.pem  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --requestheader-allowed-names<span class="o">=</span>aggregator  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --requestheader-group-headers<span class="o">=</span>X-Remote-Group  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --requestheader-extra-headers-prefix<span class="o">=</span>X-Remote-Extra-  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --requestheader-username-headers<span class="o">=</span>X-Remote-User
</span></span><span class="line"><span class="cl">      <span class="c1"># --token-auth-file=/etc/kubernetes/token.csv</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nv">Restart</span><span class="o">=</span>on-failure
</span></span><span class="line"><span class="cl"><span class="nv">RestartSec</span><span class="o">=</span>10s
</span></span><span class="line"><span class="cl"><span class="nv">LimitNOFILE</span><span class="o">=</span><span class="m">65535</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">[</span>Install<span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="nv">WantedBy</span><span class="o">=</span>multi-user.target
</span></span><span class="line"><span class="cl">EOF
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 启动kube-apiserver</span>
</span></span><span class="line"><span class="cl">systemctl daemon-reload <span class="o">&amp;&amp;</span> systemctl <span class="nb">enable</span> --now kube-apiserver
</span></span></code></pre></div><p><strong>ControllerManager(Master)</strong></p>
<p>所有Master节点配置kube-controller-manager服务，配置都一样</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># Pod网段是172.16.0.0/12，可按需更改，不要和其他在用网段冲突即可</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 给所有master节点配置服务</span>
</span></span><span class="line"><span class="cl">cat &lt;&lt; <span class="s2">&#34;EOF&#34;</span> &gt; /usr/lib/systemd/system/kube-controller-manager.service
</span></span><span class="line"><span class="cl"><span class="o">[</span>Unit<span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="nv">Description</span><span class="o">=</span>Kubernetes Controller Manager
</span></span><span class="line"><span class="cl"><span class="nv">Documentation</span><span class="o">=</span>https://github.com/kubernetes/kubernetes
</span></span><span class="line"><span class="cl"><span class="nv">After</span><span class="o">=</span>network.target
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">[</span>Service<span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="nv">ExecStart</span><span class="o">=</span>/usr/local/bin/kube-controller-manager <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --v<span class="o">=</span><span class="m">2</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --root-ca-file<span class="o">=</span>/etc/kubernetes/pki/ca.pem <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --cluster-signing-cert-file<span class="o">=</span>/etc/kubernetes/pki/ca.pem <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --cluster-signing-key-file<span class="o">=</span>/etc/kubernetes/pki/ca-key.pem <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --service-account-private-key-file<span class="o">=</span>/etc/kubernetes/pki/sa.key <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --kubeconfig<span class="o">=</span>/etc/kubernetes/controller-manager.kubeconfig <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --feature-gates<span class="o">=</span><span class="nv">LegacyServiceAccountTokenNoAutoGeneration</span><span class="o">=</span><span class="nb">false</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --leader-elect<span class="o">=</span><span class="nb">true</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --use-service-account-credentials<span class="o">=</span><span class="nb">true</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --node-monitor-grace-period<span class="o">=</span>40s <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --node-monitor-period<span class="o">=</span>5s <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --pod-eviction-timeout<span class="o">=</span>2m0s <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --controllers<span class="o">=</span>*,bootstrapsigner,tokencleaner <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --allocate-node-cidrs<span class="o">=</span><span class="nb">true</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --cluster-cidr<span class="o">=</span>172.16.0.0/12 <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --requestheader-client-ca-file<span class="o">=</span>/etc/kubernetes/pki/front-proxy-ca.pem <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --node-cidr-mask-size<span class="o">=</span><span class="m">24</span>
</span></span><span class="line"><span class="cl">      
</span></span><span class="line"><span class="cl"><span class="nv">Restart</span><span class="o">=</span>always
</span></span><span class="line"><span class="cl"><span class="nv">RestartSec</span><span class="o">=</span>10s
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">[</span>Install<span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="nv">WantedBy</span><span class="o">=</span>multi-user.target
</span></span><span class="line"><span class="cl">EOF
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 启动</span>
</span></span><span class="line"><span class="cl">systemctl daemon-reload <span class="o">&amp;&amp;</span> systemctl <span class="nb">enable</span> --now kube-controller-manager
</span></span></code></pre></div><p><strong>Scheduler(Master)</strong></p>
<p>所有Master节点配置kube-scheduler服务，配置都一样</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">cat &lt;&lt; <span class="s2">&#34;EOF&#34;</span> &gt; /usr/lib/systemd/system/kube-scheduler.service 
</span></span><span class="line"><span class="cl"><span class="o">[</span>Unit<span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="nv">Description</span><span class="o">=</span>Kubernetes Scheduler
</span></span><span class="line"><span class="cl"><span class="nv">Documentation</span><span class="o">=</span>https://github.com/kubernetes/kubernetes
</span></span><span class="line"><span class="cl"><span class="nv">After</span><span class="o">=</span>network.target
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">[</span>Service<span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="nv">ExecStart</span><span class="o">=</span>/usr/local/bin/kube-scheduler <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --v<span class="o">=</span><span class="m">2</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --leader-elect<span class="o">=</span><span class="nb">true</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --authentication-kubeconfig<span class="o">=</span>/etc/kubernetes/scheduler.kubeconfig <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --authorization-kubeconfig<span class="o">=</span>/etc/kubernetes/scheduler.kubeconfig <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>      --kubeconfig<span class="o">=</span>/etc/kubernetes/scheduler.kubeconfig
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nv">Restart</span><span class="o">=</span>always
</span></span><span class="line"><span class="cl"><span class="nv">RestartSec</span><span class="o">=</span>10s
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">[</span>Install<span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="nv">WantedBy</span><span class="o">=</span>multi-user.target
</span></span><span class="line"><span class="cl">EOF
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 启动</span>
</span></span><span class="line"><span class="cl">systemctl daemon-reload <span class="o">&amp;&amp;</span> systemctl <span class="nb">enable</span> --now kube-scheduler
</span></span></code></pre></div><p><strong>确认集群组件状态</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># 所有Master节点均检查，-l参数输出完整信息</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 如果有E开头的报错，需要排查解决一下，常见问题是IP冲突、证书错误</span>
</span></span><span class="line"><span class="cl"><span class="c1"># W告警可暂时忽略</span>
</span></span><span class="line"><span class="cl">systemctl status kube-apiserver -l
</span></span><span class="line"><span class="cl">systemctl status kube-controller-manager -l
</span></span><span class="line"><span class="cl">systemctl status kube-scheduler -l
</span></span></code></pre></div><h3 id="配置tls-bootstrapping">配置TLS Bootstrapping<a hidden class="anchor" aria-hidden="true" href="#配置tls-bootstrapping">#</a></h3>
<p>只需在master01节点配置，TLS Bootstrapping的官方说明请见：<a href="https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/">TLS Bootstrapping</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">cd</span> /root/k8s-ha-install/bootstrap
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 查看一下secret，name后要和token-id一致，token-id.token-secret和集群设置--token=对应上</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@k8s-master01 bootstrap<span class="o">]</span><span class="c1"># cat bootstrap.secret.yaml </span>
</span></span><span class="line"><span class="cl">apiVersion: v1
</span></span><span class="line"><span class="cl">kind: Secret
</span></span><span class="line"><span class="cl">metadata:
</span></span><span class="line"><span class="cl">  name: bootstrap-token-c8ad9c
</span></span><span class="line"><span class="cl">  namespace: kube-system
</span></span><span class="line"><span class="cl">type: bootstrap.kubernetes.io/token
</span></span><span class="line"><span class="cl">stringData:
</span></span><span class="line"><span class="cl">  description: <span class="s2">&#34;The default bootstrap token generated by &#39;kubelet &#39;.&#34;</span>
</span></span><span class="line"><span class="cl">  token-id: c8ad9c
</span></span><span class="line"><span class="cl">  token-secret: 2e4d610cf3e7426e
</span></span><span class="line"><span class="cl">  ...
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">kubectl config set-cluster kubernetes --certificate-authority<span class="o">=</span>/etc/kubernetes/pki/ca.pem --embed-certs<span class="o">=</span><span class="nb">true</span> --server<span class="o">=</span>https://192.168.43.200:8443 --kubeconfig<span class="o">=</span>/etc/kubernetes/bootstrap-kubelet.kubeconfig
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">kubectl config set-credentials tls-bootstrap-token-user --token<span class="o">=</span>c8ad9c.2e4d610cf3e7426e --kubeconfig<span class="o">=</span>/etc/kubernetes/bootstrap-kubelet.kubeconfig
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">kubectl config set-context tls-bootstrap-token-user@kubernetes --cluster<span class="o">=</span>kubernetes --user<span class="o">=</span>tls-bootstrap-token-user --kubeconfig<span class="o">=</span>/etc/kubernetes/bootstrap-kubelet.kubeconfig
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">kubectl config use-context tls-bootstrap-token-user@kubernetes --kubeconfig<span class="o">=</span>/etc/kubernetes/bootstrap-kubelet.kubeconfig
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 创建配置目录，拷贝admin.kubeconfig文件为config，授权kubectl，使得当前用户可以使用kubectl创建资源</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 其他master节点如果需要使用kubectl命令创建资源，也可以拷贝文件过去</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 下面是没有授权的情况</span>
</span></span><span class="line"><span class="cl"><span class="c1"># [root@k8s-master01 bootstrap]# kubectl get cs</span>
</span></span><span class="line"><span class="cl"><span class="c1"># The connection to the server localhost:8080 was refused - did you specify the right host or port?</span>
</span></span><span class="line"><span class="cl">mkdir -p /root/.kube<span class="p">;</span>cp /etc/kubernetes/admin.kubeconfig /root/.kube/config
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 授权后查看集群状态</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@k8s-master01 bootstrap<span class="o">]</span><span class="c1"># kubectl get cs</span>
</span></span><span class="line"><span class="cl">Warning: v1 ComponentStatus is deprecated in v1.19+
</span></span><span class="line"><span class="cl">NAME                 STATUS    MESSAGE                         ERROR
</span></span><span class="line"><span class="cl">scheduler            Healthy   ok
</span></span><span class="line"><span class="cl">controller-manager   Healthy   ok
</span></span><span class="line"><span class="cl">etcd-0               Healthy   <span class="o">{</span><span class="s2">&#34;health&#34;</span>:<span class="s2">&#34;true&#34;</span>,<span class="s2">&#34;reason&#34;</span>:<span class="s2">&#34;&#34;</span><span class="o">}</span>
</span></span><span class="line"><span class="cl">etcd-1               Healthy   <span class="o">{</span><span class="s2">&#34;health&#34;</span>:<span class="s2">&#34;true&#34;</span>,<span class="s2">&#34;reason&#34;</span>:<span class="s2">&#34;&#34;</span><span class="o">}</span>
</span></span><span class="line"><span class="cl">etcd-2               Healthy   <span class="o">{</span><span class="s2">&#34;health&#34;</span>:<span class="s2">&#34;true&#34;</span>,<span class="s2">&#34;reason&#34;</span>:<span class="s2">&#34;&#34;</span><span class="o">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 创建bootstrap-secret</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@k8s-master01 bootstrap<span class="o">]</span><span class="c1"># kubectl apply -f bootstrap.secret.yaml </span>
</span></span><span class="line"><span class="cl">secret/bootstrap-token-c8ad9c created
</span></span><span class="line"><span class="cl">clusterrolebinding.rbac.authorization.k8s.io/kubelet-bootstrap created
</span></span><span class="line"><span class="cl">clusterrolebinding.rbac.authorization.k8s.io/node-autoapprove-bootstrap created
</span></span><span class="line"><span class="cl">clusterrolebinding.rbac.authorization.k8s.io/node-autoapprove-certificate-rotation created
</span></span><span class="line"><span class="cl">clusterrole.rbac.authorization.k8s.io/system:kube-apiserver-to-kubelet created
</span></span><span class="line"><span class="cl">clusterrolebinding.rbac.authorization.k8s.io/system:kube-apiserver created
</span></span></code></pre></div><h3 id="配置kubelet">配置Kubelet<a hidden class="anchor" aria-hidden="true" href="#配置kubelet">#</a></h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># 从master01拷贝证书文件到其他节点</span>
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> /etc/kubernetes/
</span></span><span class="line"><span class="cl"><span class="k">for</span> i in k8s-master02 k8s-master03 k8s-node01 k8s-node02<span class="p">;</span> <span class="k">do</span>
</span></span><span class="line"><span class="cl">    ssh <span class="nv">$i</span> mkdir -p /etc/kubernetes/pki
</span></span><span class="line"><span class="cl">    <span class="k">for</span> FILE in pki/ca.pem pki/ca-key.pem pki/front-proxy-ca.pem bootstrap-kubelet.kubeconfig<span class="p">;</span> <span class="k">do</span>
</span></span><span class="line"><span class="cl">      scp /etc/kubernetes/<span class="nv">$FILE</span> <span class="nv">$i</span>:/etc/kubernetes/<span class="si">${</span><span class="nv">FILE</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">    <span class="k">done</span>
</span></span><span class="line"><span class="cl"><span class="k">done</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 所有节点配置kubelet服务</span>
</span></span><span class="line"><span class="cl">cat &lt;&lt; <span class="s2">&#34;EOF&#34;</span> &gt; /usr/lib/systemd/system/kubelet.service
</span></span><span class="line"><span class="cl"><span class="o">[</span>Unit<span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="nv">Description</span><span class="o">=</span>Kubernetes Kubelet
</span></span><span class="line"><span class="cl"><span class="nv">Documentation</span><span class="o">=</span>https://github.com/kubernetes/kubernetes
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">[</span>Service<span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="nv">ExecStart</span><span class="o">=</span>/usr/local/bin/kubelet
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nv">Restart</span><span class="o">=</span>always
</span></span><span class="line"><span class="cl"><span class="nv">StartLimitInterval</span><span class="o">=</span><span class="m">0</span>
</span></span><span class="line"><span class="cl"><span class="nv">RestartSec</span><span class="o">=</span><span class="m">10</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">[</span>Install<span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="nv">WantedBy</span><span class="o">=</span>multi-user.target
</span></span><span class="line"><span class="cl">EOF
</span></span><span class="line"><span class="cl"><span class="c1"># Runtime为Containerd，kubelet服务的配置文件</span>
</span></span><span class="line"><span class="cl">cat &lt;&lt; <span class="s2">&#34;EOF&#34;</span> &gt; /etc/systemd/system/kubelet.service.d/10-kubelet.conf
</span></span><span class="line"><span class="cl"><span class="o">[</span>Service<span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="nv">Environment</span><span class="o">=</span><span class="s2">&#34;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig --kubeconfig=/etc/kubernetes/kubelet.kubeconfig&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nv">Environment</span><span class="o">=</span><span class="s2">&#34;KUBELET_SYSTEM_ARGS=--container-runtime=remote --runtime-request-timeout=15m --container-runtime-endpoint=unix:///run/containerd/containerd.sock&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nv">Environment</span><span class="o">=</span><span class="s2">&#34;KUBELET_CONFIG_ARGS=--config=/etc/kubernetes/kubelet-conf.yml&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nv">Environment</span><span class="o">=</span><span class="s2">&#34;KUBELET_EXTRA_ARGS=--node-labels=node.kubernetes.io/node=&#39;&#39; &#34;</span>
</span></span><span class="line"><span class="cl"><span class="nv">ExecStart</span><span class="o">=</span>
</span></span><span class="line"><span class="cl"><span class="nv">ExecStart</span><span class="o">=</span>/usr/local/bin/kubelet <span class="nv">$KUBELET_KUBECONFIG_ARGS</span> <span class="nv">$KUBELET_CONFIG_ARGS</span> <span class="nv">$KUBELET_SYSTEM_ARGS</span> <span class="nv">$KUBELET_EXTRA_ARGS</span>
</span></span><span class="line"><span class="cl">EOF
</span></span><span class="line"><span class="cl"><span class="c1"># 创建kubelet配置文件，对应上面Environment配置KUBELET_CONFIG_ARGS</span>
</span></span><span class="line"><span class="cl"><span class="c1"># clusterDNS配置为service网段的第十个地址</span>
</span></span><span class="line"><span class="cl">cat &lt;&lt; <span class="s2">&#34;EOF&#34;</span> &gt; /etc/kubernetes/kubelet-conf.yml
</span></span><span class="line"><span class="cl">apiVersion: kubelet.config.k8s.io/v1beta1
</span></span><span class="line"><span class="cl">kind: KubeletConfiguration
</span></span><span class="line"><span class="cl">address: 0.0.0.0
</span></span><span class="line"><span class="cl">port: <span class="m">10250</span>
</span></span><span class="line"><span class="cl">readOnlyPort: <span class="m">10255</span>
</span></span><span class="line"><span class="cl">authentication:
</span></span><span class="line"><span class="cl">  anonymous:
</span></span><span class="line"><span class="cl">    enabled: <span class="nb">false</span>
</span></span><span class="line"><span class="cl">  webhook:
</span></span><span class="line"><span class="cl">    cacheTTL: 2m0s
</span></span><span class="line"><span class="cl">    enabled: <span class="nb">true</span>
</span></span><span class="line"><span class="cl">  x509:
</span></span><span class="line"><span class="cl">    clientCAFile: /etc/kubernetes/pki/ca.pem
</span></span><span class="line"><span class="cl">authorization:
</span></span><span class="line"><span class="cl">  mode: Webhook
</span></span><span class="line"><span class="cl">  webhook:
</span></span><span class="line"><span class="cl">    cacheAuthorizedTTL: 5m0s
</span></span><span class="line"><span class="cl">    cacheUnauthorizedTTL: 30s
</span></span><span class="line"><span class="cl">cgroupDriver: systemd
</span></span><span class="line"><span class="cl">cgroupsPerQOS: <span class="nb">true</span>
</span></span><span class="line"><span class="cl">clusterDNS:
</span></span><span class="line"><span class="cl">- 10.96.0.10
</span></span><span class="line"><span class="cl">clusterDomain: cluster.local
</span></span><span class="line"><span class="cl">containerLogMaxFiles: <span class="m">5</span>
</span></span><span class="line"><span class="cl">containerLogMaxSize: 10Mi
</span></span><span class="line"><span class="cl">contentType: application/vnd.kubernetes.protobuf
</span></span><span class="line"><span class="cl">cpuCFSQuota: <span class="nb">true</span>
</span></span><span class="line"><span class="cl">cpuManagerPolicy: none
</span></span><span class="line"><span class="cl">cpuManagerReconcilePeriod: 10s
</span></span><span class="line"><span class="cl">enableControllerAttachDetach: <span class="nb">true</span>
</span></span><span class="line"><span class="cl">enableDebuggingHandlers: <span class="nb">true</span>
</span></span><span class="line"><span class="cl">enforceNodeAllocatable:
</span></span><span class="line"><span class="cl">- pods
</span></span><span class="line"><span class="cl">eventBurst: <span class="m">10</span>
</span></span><span class="line"><span class="cl">eventRecordQPS: <span class="m">5</span>
</span></span><span class="line"><span class="cl">evictionHard:
</span></span><span class="line"><span class="cl">  imagefs.available: 15%
</span></span><span class="line"><span class="cl">  memory.available: 100Mi
</span></span><span class="line"><span class="cl">  nodefs.available: 10%
</span></span><span class="line"><span class="cl">  nodefs.inodesFree: 5%
</span></span><span class="line"><span class="cl">evictionPressureTransitionPeriod: 5m0s
</span></span><span class="line"><span class="cl">failSwapOn: <span class="nb">true</span>
</span></span><span class="line"><span class="cl">fileCheckFrequency: 20s
</span></span><span class="line"><span class="cl">hairpinMode: promiscuous-bridge
</span></span><span class="line"><span class="cl">healthzBindAddress: 127.0.0.1
</span></span><span class="line"><span class="cl">healthzPort: <span class="m">10248</span>
</span></span><span class="line"><span class="cl">httpCheckFrequency: 20s
</span></span><span class="line"><span class="cl">imageGCHighThresholdPercent: <span class="m">85</span>
</span></span><span class="line"><span class="cl">imageGCLowThresholdPercent: <span class="m">80</span>
</span></span><span class="line"><span class="cl">imageMinimumGCAge: 2m0s
</span></span><span class="line"><span class="cl">iptablesDropBit: <span class="m">15</span>
</span></span><span class="line"><span class="cl">iptablesMasqueradeBit: <span class="m">14</span>
</span></span><span class="line"><span class="cl">kubeAPIBurst: <span class="m">10</span>
</span></span><span class="line"><span class="cl">kubeAPIQPS: <span class="m">5</span>
</span></span><span class="line"><span class="cl">makeIPTablesUtilChains: <span class="nb">true</span>
</span></span><span class="line"><span class="cl">maxOpenFiles: <span class="m">1000000</span>
</span></span><span class="line"><span class="cl">maxPods: <span class="m">110</span>
</span></span><span class="line"><span class="cl">nodeStatusUpdateFrequency: 10s
</span></span><span class="line"><span class="cl">oomScoreAdj: -999
</span></span><span class="line"><span class="cl">podPidsLimit: -1
</span></span><span class="line"><span class="cl">registryBurst: <span class="m">10</span>
</span></span><span class="line"><span class="cl">registryPullQPS: <span class="m">5</span>
</span></span><span class="line"><span class="cl">resolvConf: /etc/resolv.conf
</span></span><span class="line"><span class="cl">rotateCertificates: <span class="nb">true</span>
</span></span><span class="line"><span class="cl">runtimeRequestTimeout: 2m0s
</span></span><span class="line"><span class="cl">serializeImagePulls: <span class="nb">true</span>
</span></span><span class="line"><span class="cl">staticPodPath: /etc/kubernetes/manifests
</span></span><span class="line"><span class="cl">streamingConnectionIdleTimeout: 4h0m0s
</span></span><span class="line"><span class="cl">syncFrequency: 1m0s
</span></span><span class="line"><span class="cl">volumeStatsAggPeriod: 1m0s
</span></span><span class="line"><span class="cl">EOF
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 启动服务</span>
</span></span><span class="line"><span class="cl">systemctl daemon-reload <span class="o">&amp;&amp;</span> systemctl <span class="nb">enable</span> --now kubelet
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 以k8s-master01为例查看运行日志和状态，只有CNI一处报错表示配置正确，后面CNI配置好后就会正常</span>
</span></span><span class="line"><span class="cl">tail -f /var/log/messages
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">Apr <span class="m">21</span> 16:15:56 k8s-master01 kubelet: E0421 16:15:56.608747    <span class="m">7169</span> kubelet.go:2475<span class="o">]</span> <span class="s2">&#34;Container runtime network not ready&#34;</span> <span class="nv">networkReady</span><span class="o">=</span><span class="s2">&#34;NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized&#34;</span>
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@k8s-master01 ~<span class="o">]</span><span class="c1"># systemctl status kubelet</span>
</span></span><span class="line"><span class="cl">● kubelet.service - Kubernetes Kubelet
</span></span><span class="line"><span class="cl">   Loaded: loaded <span class="o">(</span>/usr/lib/systemd/system/kubelet.service<span class="p">;</span> enabled<span class="p">;</span> vendor preset: disabled<span class="o">)</span>
</span></span><span class="line"><span class="cl">  Drop-In: /etc/systemd/system/kubelet.service.d
</span></span><span class="line"><span class="cl">           └─10-kubelet.conf
</span></span><span class="line"><span class="cl">   Active: active <span class="o">(</span>running<span class="o">)</span> since 五 2023-04-21 16:09:00 CST<span class="p">;</span> 6min ago
</span></span><span class="line"><span class="cl">     Docs: https://github.com/kubernetes/kubernetes
</span></span><span class="line"><span class="cl"> Main PID: <span class="m">7169</span> <span class="o">(</span>kubelet<span class="o">)</span>
</span></span><span class="line"><span class="cl">    Tasks: <span class="m">11</span>
</span></span><span class="line"><span class="cl">   Memory: 40.6M
</span></span><span class="line"><span class="cl">   CGroup: /system.slice/kubelet.service
</span></span><span class="line"><span class="cl">           └─7169 /usr/local/bin/kubelet --bootstrap-kubeconfig<span class="o">=</span>/etc/kubernetes/bootstrap-kubelet.kubeconfig --kubeconfig<span class="o">=</span>/etc/kuberne...
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">4月 <span class="m">21</span> 16:14:26 k8s-master01 kubelet<span class="o">[</span>7169<span class="o">]</span>: E0421 16:14:26.578931    <span class="m">7169</span> kubelet.go:2475<span class="o">]</span> <span class="s2">&#34;Container runtime network not read...alized&#34;</span>
</span></span><span class="line"><span class="cl">4月 <span class="m">21</span> 16:14:31 k8s-master01 kubelet<span class="o">[</span>7169<span class="o">]</span>: E0421 16:14:31.580077    <span class="m">7169</span> kubelet.go:2475<span class="o">]</span> <span class="s2">&#34;Container runtime network not read...alized&#34;</span>
</span></span><span class="line"><span class="cl">....
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 并且此时节点状态应该是可查询且处于NotReady，CNI配置后就会ready</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@k8s-master01 ~<span class="o">]</span><span class="c1"># kubectl get node</span>
</span></span><span class="line"><span class="cl">NAME           STATUS     ROLES    AGE     VERSION
</span></span><span class="line"><span class="cl">k8s-master01   NotReady   &lt;none&gt;   7m12s   v1.26.4
</span></span><span class="line"><span class="cl">k8s-master02   NotReady   &lt;none&gt;   7m11s   v1.26.4
</span></span><span class="line"><span class="cl">k8s-master03   NotReady   &lt;none&gt;   7m10s   v1.26.4
</span></span><span class="line"><span class="cl">k8s-node01     NotReady   &lt;none&gt;   7m24s   v1.26.4
</span></span><span class="line"><span class="cl">k8s-node02     NotReady   &lt;none&gt;   7m12s   v1.26.4
</span></span></code></pre></div><h3 id="配置kube-proxy">配置Kube-proxy<a hidden class="anchor" aria-hidden="true" href="#配置kube-proxy">#</a></h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># 只需在k8s-master01上执行</span>
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> /root/k8s-ha-install
</span></span><span class="line"><span class="cl">kubectl -n kube-system create serviceaccount kube-proxy
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">kubectl create clusterrolebinding system:kube-proxy --clusterrole system:node-proxier --serviceaccount kube-system:kube-proxy
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nv">SECRET</span><span class="o">=</span><span class="k">$(</span>kubectl -n kube-system get sa/kube-proxy <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --output<span class="o">=</span><span class="nv">jsonpath</span><span class="o">=</span><span class="s1">&#39;{.secrets[0].name}&#39;</span><span class="k">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nv">JWT_TOKEN</span><span class="o">=</span><span class="k">$(</span>kubectl -n kube-system get secret/<span class="nv">$SECRET</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>--output<span class="o">=</span><span class="nv">jsonpath</span><span class="o">=</span><span class="s1">&#39;{.data.token}&#39;</span> <span class="p">|</span> base64 -d<span class="k">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nv">PKI_DIR</span><span class="o">=</span>/etc/kubernetes/pki
</span></span><span class="line"><span class="cl"><span class="nv">K8S_DIR</span><span class="o">=</span>/etc/kubernetes
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">kubectl config set-cluster kubernetes --certificate-authority<span class="o">=</span>/etc/kubernetes/pki/ca.pem --embed-certs<span class="o">=</span><span class="nb">true</span>     --server<span class="o">=</span>https://192.168.43.200:8443 --kubeconfig<span class="o">=</span><span class="si">${</span><span class="nv">K8S_DIR</span><span class="si">}</span>/kube-proxy.kubeconfig
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">kubectl config set-credentials kubernetes --token<span class="o">=</span><span class="si">${</span><span class="nv">JWT_TOKEN</span><span class="si">}</span> --kubeconfig<span class="o">=</span>/etc/kubernetes/kube-proxy.kubeconfig
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">kubectl config set-context kubernetes --cluster<span class="o">=</span>kubernetes --user<span class="o">=</span>kubernetes --kubeconfig<span class="o">=</span>/etc/kubernetes/kube-proxy.kubeconfig
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">kubectl config use-context kubernetes --kubeconfig<span class="o">=</span>/etc/kubernetes/kube-proxy.kubeconfig
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 从k8s-master01将kubeconfig拷贝到其他节点</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> i in k8s-master02 k8s-master03 k8s-node01 k8s-node02<span class="p">;</span> <span class="k">do</span>
</span></span><span class="line"><span class="cl">     scp /etc/kubernetes/kube-proxy.kubeconfig <span class="nv">$i</span>:/etc/kubernetes/kube-proxy.kubeconfig
</span></span><span class="line"><span class="cl"><span class="k">done</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 所有节点配置kube-proxy服务</span>
</span></span><span class="line"><span class="cl">cat &lt;&lt; <span class="s2">&#34;EOF&#34;</span> &gt; /usr/lib/systemd/system/kube-proxy.service
</span></span><span class="line"><span class="cl"><span class="o">[</span>Unit<span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="nv">Description</span><span class="o">=</span>Kubernetes Kube Proxy
</span></span><span class="line"><span class="cl"><span class="nv">Documentation</span><span class="o">=</span>https://github.com/kubernetes/kubernetes
</span></span><span class="line"><span class="cl"><span class="nv">After</span><span class="o">=</span>network.target
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">[</span>Service<span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="nv">ExecStart</span><span class="o">=</span>/usr/local/bin/kube-proxy <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --config<span class="o">=</span>/etc/kubernetes/kube-proxy.yaml <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --v<span class="o">=</span><span class="m">2</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nv">Restart</span><span class="o">=</span>always
</span></span><span class="line"><span class="cl"><span class="nv">RestartSec</span><span class="o">=</span>10s
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">[</span>Install<span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="nv">WantedBy</span><span class="o">=</span>multi-user.target
</span></span><span class="line"><span class="cl">EOF
</span></span><span class="line"><span class="cl"><span class="c1"># 所有节点配置kube-proxy.yaml,clusterCIDR为pod网段</span>
</span></span><span class="line"><span class="cl">cat &lt;&lt; <span class="s2">&#34;EOF&#34;</span> &gt; /etc/kubernetes/kube-proxy.yaml
</span></span><span class="line"><span class="cl">apiVersion: kubeproxy.config.k8s.io/v1alpha1
</span></span><span class="line"><span class="cl">bindAddress: 0.0.0.0
</span></span><span class="line"><span class="cl">clientConnection:
</span></span><span class="line"><span class="cl">  acceptContentTypes: <span class="s2">&#34;&#34;</span>
</span></span><span class="line"><span class="cl">  burst: <span class="m">10</span>
</span></span><span class="line"><span class="cl">  contentType: application/vnd.kubernetes.protobuf
</span></span><span class="line"><span class="cl">  kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig
</span></span><span class="line"><span class="cl">  qps: <span class="m">5</span>
</span></span><span class="line"><span class="cl">clusterCIDR: 172.16.0.0/12 
</span></span><span class="line"><span class="cl">configSyncPeriod: 15m0s
</span></span><span class="line"><span class="cl">conntrack:
</span></span><span class="line"><span class="cl">  max: null
</span></span><span class="line"><span class="cl">  maxPerCore: <span class="m">32768</span>
</span></span><span class="line"><span class="cl">  min: <span class="m">131072</span>
</span></span><span class="line"><span class="cl">  tcpCloseWaitTimeout: 1h0m0s
</span></span><span class="line"><span class="cl">  tcpEstablishedTimeout: 24h0m0s
</span></span><span class="line"><span class="cl">enableProfiling: <span class="nb">false</span>
</span></span><span class="line"><span class="cl">healthzBindAddress: 0.0.0.0:10256
</span></span><span class="line"><span class="cl">hostnameOverride: <span class="s2">&#34;&#34;</span>
</span></span><span class="line"><span class="cl">iptables:
</span></span><span class="line"><span class="cl">  masqueradeAll: <span class="nb">false</span>
</span></span><span class="line"><span class="cl">  masqueradeBit: <span class="m">14</span>
</span></span><span class="line"><span class="cl">  minSyncPeriod: 0s
</span></span><span class="line"><span class="cl">  syncPeriod: 30s
</span></span><span class="line"><span class="cl">ipvs:
</span></span><span class="line"><span class="cl">  masqueradeAll: <span class="nb">true</span>
</span></span><span class="line"><span class="cl">  minSyncPeriod: 5s
</span></span><span class="line"><span class="cl">  scheduler: <span class="s2">&#34;rr&#34;</span>
</span></span><span class="line"><span class="cl">  syncPeriod: 30s
</span></span><span class="line"><span class="cl">kind: KubeProxyConfiguration
</span></span><span class="line"><span class="cl">metricsBindAddress: 127.0.0.1:10249
</span></span><span class="line"><span class="cl">mode: <span class="s2">&#34;ipvs&#34;</span>
</span></span><span class="line"><span class="cl">nodePortAddresses: null
</span></span><span class="line"><span class="cl">oomScoreAdj: -999
</span></span><span class="line"><span class="cl">portRange: <span class="s2">&#34;&#34;</span>
</span></span><span class="line"><span class="cl">udpIdleTimeout: 250ms
</span></span><span class="line"><span class="cl">EOF
</span></span><span class="line"><span class="cl"><span class="c1"># 启动</span>
</span></span><span class="line"><span class="cl">systemctl daemon-reload <span class="o">&amp;&amp;</span> systemctl <span class="nb">enable</span> --now kube-proxy
</span></span></code></pre></div><h3 id="配置calico">配置Calico<a hidden class="anchor" aria-hidden="true" href="#配置calico">#</a></h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># k8s-master01上执行，更改calico中Pod网段为自己的</span>
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> /root/k8s-ha-install/calico/
</span></span><span class="line"><span class="cl">sed -i <span class="s2">&#34;s#POD_CIDR#172.16.0.0/12#g&#34;</span> calico.yaml
</span></span><span class="line"><span class="cl"><span class="c1"># 检查是否更改成功</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@k8s-master01 calico<span class="o">]</span><span class="c1"># grep &#34;IPV4POOL_CIDR&#34; calico.yaml  -A 1</span>
</span></span><span class="line"><span class="cl">            - name: CALICO_IPV4POOL_CIDR
</span></span><span class="line"><span class="cl">              value: <span class="s2">&#34;172.16.0.0/12&#34;</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 应用calico</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@k8s-master01 calico<span class="o">]</span><span class="c1"># kubectl apply -f calico.yaml</span>
</span></span><span class="line"><span class="cl">poddisruptionbudget.policy/calico-kube-controllers created
</span></span><span class="line"><span class="cl">poddisruptionbudget.policy/calico-typha created
</span></span><span class="line"><span class="cl">serviceaccount/calico-kube-controllers created
</span></span><span class="line"><span class="cl">serviceaccount/calico-node created
</span></span><span class="line"><span class="cl">configmap/calico-config created
</span></span><span class="line"><span class="cl">customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
</span></span><span class="line"><span class="cl">customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
</span></span><span class="line"><span class="cl">customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
</span></span><span class="line"><span class="cl">customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
</span></span><span class="line"><span class="cl">customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
</span></span><span class="line"><span class="cl">customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
</span></span><span class="line"><span class="cl">customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
</span></span><span class="line"><span class="cl">customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
</span></span><span class="line"><span class="cl">customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
</span></span><span class="line"><span class="cl">customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
</span></span><span class="line"><span class="cl">customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
</span></span><span class="line"><span class="cl">customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
</span></span><span class="line"><span class="cl">customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
</span></span><span class="line"><span class="cl">customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
</span></span><span class="line"><span class="cl">customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
</span></span><span class="line"><span class="cl">customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
</span></span><span class="line"><span class="cl">customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
</span></span><span class="line"><span class="cl">clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
</span></span><span class="line"><span class="cl">clusterrole.rbac.authorization.k8s.io/calico-node created
</span></span><span class="line"><span class="cl">clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
</span></span><span class="line"><span class="cl">clusterrolebinding.rbac.authorization.k8s.io/calico-node created
</span></span><span class="line"><span class="cl">service/calico-typha created
</span></span><span class="line"><span class="cl">daemonset.apps/calico-node created
</span></span><span class="line"><span class="cl">deployment.apps/calico-kube-controllers created
</span></span><span class="line"><span class="cl">deployment.apps/calico-typha created
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># （中间状态）在calico生效过程中查看各节点污点状态，发现均有不可调度污点，不必管他，等待calico生效即可，这种污点是kubernetes集群保护机制，在节点处于not ready状态时，节点不可调度</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@k8s-master01 calico<span class="o">]</span><span class="c1"># kubectl describe node| grep Taints:</span>
</span></span><span class="line"><span class="cl">Taints:             node.kubernetes.io/not-ready:NoSchedule
</span></span><span class="line"><span class="cl">Taints:             node.kubernetes.io/not-ready:NoSchedule
</span></span><span class="line"><span class="cl">Taints:             node.kubernetes.io/not-ready:NoSchedule
</span></span><span class="line"><span class="cl">Taints:             node.kubernetes.io/not-ready:NoSchedule
</span></span><span class="line"><span class="cl">Taints:             node.kubernetes.io/not-ready:NoSchedule
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># calico Pod处于Running后集群网络将建立，node将处于Ready状态，calico建立网络取决于电脑性能（硬件和网络环境），一般几分钟即可完成，性能差的可能会花费更长时间</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@k8s-master01 calico<span class="o">]</span><span class="c1"># kubectl get po -A</span>
</span></span><span class="line"><span class="cl">NAMESPACE     NAME                                       READY   STATUS    RESTARTS      AGE
</span></span><span class="line"><span class="cl">kube-system   calico-kube-controllers-6bd6b69df9-5nwkl   1/1     Running   <span class="m">0</span>             10m
</span></span><span class="line"><span class="cl">kube-system   calico-node-8cvcd                          1/1     Running   <span class="m">0</span>             10m
</span></span><span class="line"><span class="cl">kube-system   calico-node-96mx8                          1/1     Running   <span class="m">1</span> <span class="o">(</span>36s ago<span class="o">)</span>   10m
</span></span><span class="line"><span class="cl">kube-system   calico-node-f49rb                          1/1     Running   <span class="m">0</span>             10m
</span></span><span class="line"><span class="cl">kube-system   calico-node-rgs7f                          1/1     Running   <span class="m">0</span>             10m
</span></span><span class="line"><span class="cl">kube-system   calico-node-vzrls                          1/1     Running   <span class="m">0</span>             10m
</span></span><span class="line"><span class="cl">kube-system   calico-typha-77fc8866f5-h9bsj              1/1     Running   <span class="m">0</span>             10m
</span></span><span class="line"><span class="cl"><span class="c1"># 查看集群状态和资源</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@k8s-master01 calico<span class="o">]</span><span class="c1"># kubectl get node</span>
</span></span><span class="line"><span class="cl">NAME           STATUS   ROLES    AGE   VERSION
</span></span><span class="line"><span class="cl">k8s-master01   Ready    &lt;none&gt;   23m   v1.26.4
</span></span><span class="line"><span class="cl">k8s-master02   Ready    &lt;none&gt;   23m   v1.26.4
</span></span><span class="line"><span class="cl">k8s-master03   Ready    &lt;none&gt;   23m   v1.26.4
</span></span><span class="line"><span class="cl">k8s-node01     Ready    &lt;none&gt;   23m   v1.26.4
</span></span><span class="line"><span class="cl">k8s-node02     Ready    &lt;none&gt;   23m   v1.26.4
</span></span></code></pre></div><h3 id="配置coredns">配置CoreDNS<a hidden class="anchor" aria-hidden="true" href="#配置coredns">#</a></h3>
<ul>
<li>在k8s-master01操作</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># 如果更改了k8s service的网段需要将coredns的serviceIP改成k8s service网段的第十个IP</span>
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> /root/k8s-ha-install/
</span></span><span class="line"><span class="cl"><span class="nv">COREDNS_SERVICE_IP</span><span class="o">=</span><span class="sb">`</span>kubectl get svc <span class="p">|</span> grep kubernetes <span class="p">|</span> awk <span class="s1">&#39;{print $3}&#39;</span><span class="sb">`</span><span class="m">0</span>
</span></span><span class="line"><span class="cl">sed -i <span class="s2">&#34;s#KUBEDNS_SERVICE_IP#</span><span class="si">${</span><span class="nv">COREDNS_SERVICE_IP</span><span class="si">}</span><span class="s2">#g&#34;</span> CoreDNS/coredns.yaml
</span></span><span class="line"><span class="cl">kubectl apply -f CoreDNS/coredns.yaml 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@k8s-master01 k8s-ha-install<span class="o">]</span><span class="c1"># kubectl get pod -A | grep coredns</span>
</span></span><span class="line"><span class="cl">kube-system   coredns-5db5696c7-tsqts                    1/1     Running   <span class="m">0</span>          80s
</span></span></code></pre></div><h2 id="配置metrics">配置Metrics<a hidden class="anchor" aria-hidden="true" href="#配置metrics">#</a></h2>
<ul>
<li>在k8s-master01操作</li>
</ul>
<p>在新版的Kubernetes中系统资源的采集均使用Metrics-server，可以通过Metrics采集节点和Pod的内存、磁盘、CPU和网络的使用率。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">cd</span> /root/k8s-ha-install/metrics-server
</span></span><span class="line"><span class="cl">kubectl  create -f . 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 等待metrics-server部署好后，便可使用</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@k8s-master01 metrics-server<span class="o">]</span><span class="c1"># kubectl top nodes</span>
</span></span><span class="line"><span class="cl">NAME           CPU<span class="o">(</span>cores<span class="o">)</span>   CPU%   MEMORY<span class="o">(</span>bytes<span class="o">)</span>   MEMORY%   
</span></span><span class="line"><span class="cl">k8s-master01   186m         9%     1094Mi          58%       
</span></span><span class="line"><span class="cl">k8s-master02   192m         9%     1176Mi          62%       
</span></span><span class="line"><span class="cl">k8s-master03   176m         8%     1123Mi          60%       
</span></span><span class="line"><span class="cl">k8s-node01     72m          3%     463Mi           24%       
</span></span><span class="line"><span class="cl">k8s-node02     66m          3%     472Mi           25%  
</span></span></code></pre></div><h2 id="配置dashboard">配置Dashboard<a hidden class="anchor" aria-hidden="true" href="#配置dashboard">#</a></h2>
<p>Dashboard是一个展示Kubernetes集群资源和Pod日志，甚至可以执行容器命令的web控制台。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># 直接部署即可</span>
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> /root/k8s-ha-install/dashboard/
</span></span><span class="line"><span class="cl">kubectl apply -f .
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># 查看dashboard端口，默认是NodePort模式，访问集群内任意节点的32486端口即可</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@k8s-master01 ~<span class="o">]</span><span class="c1"># kubectl get svc -A | grep dash</span>
</span></span><span class="line"><span class="cl">kubernetes-dashboard   dashboard-metrics-scraper   ClusterIP   10.111.39.8      &lt;none&gt;        8000/TCP                 12m
</span></span><span class="line"><span class="cl">kubernetes-dashboard   kubernetes-dashboard        NodePort    10.98.143.126    &lt;none&gt;        443:32486/TCP            12m
</span></span></code></pre></div><p>访问dashboard：<a href="https://%E9%9B%86%E7%BE%A4%E5%86%85%E4%BB%BB%E6%84%8F%E8%8A%82%E7%82%B9IP:32486">https://集群内任意节点IP:32486</a></p>
<p>发现提示隐私设置错误的问题，解决方法是在Chrome浏览器启动参数加入<code>--test-type --ignore-certificate-errors</code>，再访问就没有这个提示</p>
<p><img loading="lazy" src="https://deemoprobe.oss-cn-shanghai.aliyuncs.com/images/20211213154024.png" alt="20211213154024"  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># 获取登陆令牌（token）</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@k8s-master01 dashboard<span class="o">]</span><span class="c1"># kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk &#39;{print $1}&#39;)</span>
</span></span><span class="line"><span class="cl">Name:         admin-user-token-cj2kt
</span></span><span class="line"><span class="cl">Namespace:    kube-system
</span></span><span class="line"><span class="cl">Labels:       &lt;none&gt;
</span></span><span class="line"><span class="cl">Annotations:  kubernetes.io/service-account.name: admin-user
</span></span><span class="line"><span class="cl">              kubernetes.io/service-account.uid: c86fbde2-36ea-4dd3-94fd-8ce8012fdf22
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Type:  kubernetes.io/service-account-token
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nv">Data</span>
</span></span><span class="line"><span class="cl"><span class="o">====</span>
</span></span><span class="line"><span class="cl">ca.crt:     <span class="m">1411</span> bytes
</span></span><span class="line"><span class="cl">namespace:  <span class="m">11</span> bytes
</span></span><span class="line"><span class="cl">token:      eyJhbGciOiJSUzI1NiIsImtpZCI6ImFPeklobHBkNVRzZzZYVF9nbG5BMTgwOHdvMUNkV2FGbW1wdmUzZzdJRXcifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLWNqMmt0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJjODZmYmRlMi0zNmVhLTRkZDMtOTRmZC04Y2U4MDEyZmRmMjIiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.YgxIsaxR-hfyT9YLGdszggQ0Rvoc4SvyqswgvHz2ySc27q8lAQ7EJxhze3bhrdTL79z3J30T6vmuA5Be3kq_c2r42r2Iy-pC92t8xTISlPWEl7JfSg8GSbX2-UxUM_wqCmbMO3RWGYW5FpzrJ2pSVaeIGlu2JmYTugtS50LCFi87DmP2tDAKLQfh1NRylpEPI1AJPbl41E2wyDBUlS86YF_glUnQxyDCyrf2wJ2Akjqe7If2b9tAXHbSZBcQJFGHENymYhdBW6QObmTRxUsaOX9wdTToFcoHr-FaE4LcP9KuXhxP-gNNyVN7HN0k0WbhAp6CBIoypFCVLIN96EvNIg
</span></span></code></pre></div><ul>
<li>选择<code>ALL namespace</code>，可以查看如下图</li>
</ul>
<p><a href="https://deemoprobe.oss-cn-shanghai.aliyuncs.com/images/20230421165240.png"><img loading="lazy" src="https://deemoprobe.oss-cn-shanghai.aliyuncs.com/images/20230421165240.png" alt="1"  />
</a></p>
<h2 id="集群优化可选">集群优化(可选)<a hidden class="anchor" aria-hidden="true" href="#集群优化可选">#</a></h2>
<p>Docker可在<code>/etc/docker/daemon.json</code>自定义优化配置，所有配置可见：<a href="https://docs.docker.com/engine/reference/commandline/dockerd/#daemon-configuration-file">官方docker configuration</a>，docker常用优化配置见下方注释说明。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># （！！！如果使用docker作为Runtime的话）优化docker配置</span>
</span></span><span class="line"><span class="cl"><span class="c1"># /etc/docker/daemon.json文件，按需配置，不需要全部都照抄，使用时删除注释，因为JSON文件不支持注释</span>
</span></span><span class="line"><span class="cl"><span class="o">{</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;exec-opts&#34;</span>: <span class="o">[</span><span class="s2">&#34;native.cgroupdriver=systemd&#34;</span><span class="o">]</span>, <span class="c1"># cgroups驱动</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;registry-mirrors&#34;</span>: <span class="o">[</span><span class="s2">&#34;https://ynirk4k5.mirror.aliyuncs.com&#34;</span><span class="o">]</span>, <span class="c1"># 镜像加速器地址</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;allow-nondistributable-artifacts&#34;</span>: <span class="o">[]</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;api-cors-header&#34;</span>: <span class="s2">&#34;&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;authorization-plugins&#34;</span>: <span class="o">[]</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;bip&#34;</span>: <span class="s2">&#34;&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;bridge&#34;</span>: <span class="s2">&#34;&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;cgroup-parent&#34;</span>: <span class="s2">&#34;&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;cluster-advertise&#34;</span>: <span class="s2">&#34;&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;cluster-store&#34;</span>: <span class="s2">&#34;&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;cluster-store-opts&#34;</span>: <span class="o">{}</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;containerd&#34;</span>: <span class="s2">&#34;/run/containerd/containerd.sock&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;containerd-namespace&#34;</span>: <span class="s2">&#34;docker&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;data-root&#34;</span>: <span class="s2">&#34;&#34;</span>, <span class="c1"># 数据根目录，大量docker镜像可能会占用较大存储，可以设置系统盘外的挂载盘</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;debug&#34;</span>: true,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;default-address-pools&#34;</span>: <span class="o">[</span>
</span></span><span class="line"><span class="cl">    <span class="o">{</span>
</span></span><span class="line"><span class="cl">      <span class="s2">&#34;base&#34;</span>: <span class="s2">&#34;172.30.0.0/16&#34;</span>,
</span></span><span class="line"><span class="cl">      <span class="s2">&#34;size&#34;</span>: <span class="m">24</span>
</span></span><span class="line"><span class="cl">    <span class="o">}</span>,
</span></span><span class="line"><span class="cl">    <span class="o">{</span>
</span></span><span class="line"><span class="cl">      <span class="s2">&#34;base&#34;</span>: <span class="s2">&#34;172.31.0.0/16&#34;</span>,
</span></span><span class="line"><span class="cl">      <span class="s2">&#34;size&#34;</span>: <span class="m">24</span>
</span></span><span class="line"><span class="cl">    <span class="o">}</span>
</span></span><span class="line"><span class="cl">  <span class="o">]</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;default-cgroupns-mode&#34;</span>: <span class="s2">&#34;private&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;default-gateway&#34;</span>: <span class="s2">&#34;&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;default-gateway-v6&#34;</span>: <span class="s2">&#34;&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;default-runtime&#34;</span>: <span class="s2">&#34;runc&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;default-shm-size&#34;</span>: <span class="s2">&#34;64M&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;default-ulimits&#34;</span>: <span class="o">{</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;nofile&#34;</span>: <span class="o">{</span>
</span></span><span class="line"><span class="cl">      <span class="s2">&#34;Hard&#34;</span>: 64000,
</span></span><span class="line"><span class="cl">      <span class="s2">&#34;Name&#34;</span>: <span class="s2">&#34;nofile&#34;</span>,
</span></span><span class="line"><span class="cl">      <span class="s2">&#34;Soft&#34;</span>: <span class="m">64000</span>
</span></span><span class="line"><span class="cl">    <span class="o">}</span>
</span></span><span class="line"><span class="cl">  <span class="o">}</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;dns&#34;</span>: <span class="o">[]</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;dns-opts&#34;</span>: <span class="o">[]</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;dns-search&#34;</span>: <span class="o">[]</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;exec-root&#34;</span>: <span class="s2">&#34;&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;experimental&#34;</span>: false,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;features&#34;</span>: <span class="o">{}</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;fixed-cidr&#34;</span>: <span class="s2">&#34;&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;fixed-cidr-v6&#34;</span>: <span class="s2">&#34;&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;group&#34;</span>: <span class="s2">&#34;&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;hosts&#34;</span>: <span class="o">[]</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;icc&#34;</span>: false,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;init&#34;</span>: false,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;init-path&#34;</span>: <span class="s2">&#34;/usr/libexec/docker-init&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;insecure-registries&#34;</span>: <span class="o">[]</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;ip&#34;</span>: <span class="s2">&#34;0.0.0.0&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;ip-forward&#34;</span>: false,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;ip-masq&#34;</span>: false,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;iptables&#34;</span>: false,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;ip6tables&#34;</span>: false,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;ipv6&#34;</span>: false,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;labels&#34;</span>: <span class="o">[]</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;live-restore&#34;</span>: true, <span class="c1"># docker进程宕机时容器依然保持存活</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;log-driver&#34;</span>: <span class="s2">&#34;json-file&#34;</span>, <span class="c1"># 日志格式</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;log-level&#34;</span>: <span class="s2">&#34;&#34;</span>, <span class="c1"># 日志级别</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;log-opts&#34;</span>: <span class="o">{</span> <span class="c1"># 日志优化</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;cache-disabled&#34;</span>: <span class="s2">&#34;false&#34;</span>,
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;cache-max-file&#34;</span>: <span class="s2">&#34;5&#34;</span>,
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;cache-max-size&#34;</span>: <span class="s2">&#34;20m&#34;</span>,
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;cache-compress&#34;</span>: <span class="s2">&#34;true&#34;</span>,
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;env&#34;</span>: <span class="s2">&#34;os,customer&#34;</span>,
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;labels&#34;</span>: <span class="s2">&#34;somelabel&#34;</span>,
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;max-file&#34;</span>: <span class="s2">&#34;5&#34;</span>, <span class="c1"># 最大日志数量</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;max-size&#34;</span>: <span class="s2">&#34;10m&#34;</span> <span class="c1"># 保存的最大日志大小</span>
</span></span><span class="line"><span class="cl">  <span class="o">}</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;max-concurrent-downloads&#34;</span>: 3, <span class="c1"># pull下载并发数</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;max-concurrent-uploads&#34;</span>: 5, <span class="c1"># push上传并发数</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;max-download-attempts&#34;</span>: 5,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;mtu&#34;</span>: 0,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;no-new-privileges&#34;</span>: false,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;node-generic-resources&#34;</span>: <span class="o">[</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;NVIDIA-GPU=UUID1&#34;</span>,
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;NVIDIA-GPU=UUID2&#34;</span>
</span></span><span class="line"><span class="cl">  <span class="o">]</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;oom-score-adjust&#34;</span>: -500,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;pidfile&#34;</span>: <span class="s2">&#34;&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;raw-logs&#34;</span>: false,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;runtimes&#34;</span>: <span class="o">{</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;cc-runtime&#34;</span>: <span class="o">{</span>
</span></span><span class="line"><span class="cl">      <span class="s2">&#34;path&#34;</span>: <span class="s2">&#34;/usr/bin/cc-runtime&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="o">}</span>,
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;custom&#34;</span>: <span class="o">{</span>
</span></span><span class="line"><span class="cl">      <span class="s2">&#34;path&#34;</span>: <span class="s2">&#34;/usr/local/bin/my-runc-replacement&#34;</span>,
</span></span><span class="line"><span class="cl">      <span class="s2">&#34;runtimeArgs&#34;</span>: <span class="o">[</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;--debug&#34;</span>
</span></span><span class="line"><span class="cl">      <span class="o">]</span>
</span></span><span class="line"><span class="cl">    <span class="o">}</span>
</span></span><span class="line"><span class="cl">  <span class="o">}</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;seccomp-profile&#34;</span>: <span class="s2">&#34;&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;selinux-enabled&#34;</span>: false,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;shutdown-timeout&#34;</span>: 15,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;storage-driver&#34;</span>: <span class="s2">&#34;&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;storage-opts&#34;</span>: <span class="o">[]</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;swarm-default-advertise-addr&#34;</span>: <span class="s2">&#34;&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;tls&#34;</span>: true,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;tlscacert&#34;</span>: <span class="s2">&#34;&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;tlscert&#34;</span>: <span class="s2">&#34;&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;tlskey&#34;</span>: <span class="s2">&#34;&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;tlsverify&#34;</span>: true,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;userland-proxy&#34;</span>: false,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;userland-proxy-path&#34;</span>: <span class="s2">&#34;/usr/libexec/docker-proxy&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;userns-remap&#34;</span>: <span class="s2">&#34;&#34;</span>
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 无注释版</span>
</span></span><span class="line"><span class="cl"><span class="o">{</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;exec-opts&#34;</span>: <span class="o">[</span><span class="s2">&#34;native.cgroupdriver=systemd&#34;</span><span class="o">]</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;registry-mirrors&#34;</span>: <span class="o">[</span><span class="s2">&#34;https://ynirk4k5.mirror.aliyuncs.com&#34;</span><span class="o">]</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;containerd-namespace&#34;</span>: <span class="s2">&#34;docker&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;data-root&#34;</span>: <span class="s2">&#34;&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;debug&#34;</span>: true,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;default-cgroupns-mode&#34;</span>: <span class="s2">&#34;private&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;default-gateway&#34;</span>: <span class="s2">&#34;&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;default-gateway-v6&#34;</span>: <span class="s2">&#34;&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;default-runtime&#34;</span>: <span class="s2">&#34;runc&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;default-shm-size&#34;</span>: <span class="s2">&#34;64M&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;default-ulimits&#34;</span>: <span class="o">{</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;nofile&#34;</span>: <span class="o">{</span>
</span></span><span class="line"><span class="cl">      <span class="s2">&#34;Hard&#34;</span>: 64000,
</span></span><span class="line"><span class="cl">      <span class="s2">&#34;Name&#34;</span>: <span class="s2">&#34;nofile&#34;</span>,
</span></span><span class="line"><span class="cl">      <span class="s2">&#34;Soft&#34;</span>: <span class="m">64000</span>
</span></span><span class="line"><span class="cl">    <span class="o">}</span>
</span></span><span class="line"><span class="cl">  <span class="o">}</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;init-path&#34;</span>: <span class="s2">&#34;/usr/libexec/docker-init&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;live-restore&#34;</span>: true,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;log-driver&#34;</span>: <span class="s2">&#34;json-file&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;log-level&#34;</span>: <span class="s2">&#34;&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;log-opts&#34;</span>: <span class="o">{</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;cache-disabled&#34;</span>: <span class="s2">&#34;false&#34;</span>,
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;cache-max-file&#34;</span>: <span class="s2">&#34;5&#34;</span>,
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;cache-max-size&#34;</span>: <span class="s2">&#34;20m&#34;</span>,
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;cache-compress&#34;</span>: <span class="s2">&#34;true&#34;</span>,
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;env&#34;</span>: <span class="s2">&#34;os,customer&#34;</span>,
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;labels&#34;</span>: <span class="s2">&#34;somelabel&#34;</span>,
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;max-file&#34;</span>: <span class="s2">&#34;5&#34;</span>,
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;max-size&#34;</span>: <span class="s2">&#34;10m&#34;</span>
</span></span><span class="line"><span class="cl">  <span class="o">}</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;max-concurrent-downloads&#34;</span>: 3,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;max-concurrent-uploads&#34;</span>: 5,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;max-download-attempts&#34;</span>: 5,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;mtu&#34;</span>: 0,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;no-new-privileges&#34;</span>: false,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;oom-score-adjust&#34;</span>: -500,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;pidfile&#34;</span>: <span class="s2">&#34;&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;raw-logs&#34;</span>: false,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;runtimes&#34;</span>: <span class="o">{</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;cc-runtime&#34;</span>: <span class="o">{</span>
</span></span><span class="line"><span class="cl">      <span class="s2">&#34;path&#34;</span>: <span class="s2">&#34;/usr/bin/cc-runtime&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="o">}</span>,
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;custom&#34;</span>: <span class="o">{</span>
</span></span><span class="line"><span class="cl">      <span class="s2">&#34;path&#34;</span>: <span class="s2">&#34;/usr/local/bin/my-runc-replacement&#34;</span>,
</span></span><span class="line"><span class="cl">      <span class="s2">&#34;runtimeArgs&#34;</span>: <span class="o">[</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;--debug&#34;</span>
</span></span><span class="line"><span class="cl">      <span class="o">]</span>
</span></span><span class="line"><span class="cl">    <span class="o">}</span>
</span></span><span class="line"><span class="cl">  <span class="o">}</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;seccomp-profile&#34;</span>: <span class="s2">&#34;&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;selinux-enabled&#34;</span>: false,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;shutdown-timeout&#34;</span>: 15,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;storage-driver&#34;</span>: <span class="s2">&#34;&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;storage-opts&#34;</span>: <span class="o">[]</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;swarm-default-advertise-addr&#34;</span>: <span class="s2">&#34;&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;userland-proxy-path&#34;</span>: <span class="s2">&#34;/usr/libexec/docker-proxy&#34;</span>,
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;userns-remap&#34;</span>: <span class="s2">&#34;&#34;</span>
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 设置证书有效期</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@k8s-master01 ~<span class="o">]</span><span class="c1"># vim /usr/lib/systemd/system/kube-controller-manager.service</span>
</span></span><span class="line"><span class="cl">... <span class="c1"># 加入下面配置</span>
</span></span><span class="line"><span class="cl">--experimental-cluster-signing-duration<span class="o">=</span>876000h0m0s
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@k8s-master01 ~<span class="o">]</span><span class="c1"># systemctl daemon-reload</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@k8s-master01 ~<span class="o">]</span><span class="c1"># systemctl restart kube-controller-manager</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># kubelet优化加密算法，默认的算法容易被漏洞扫描；增长镜像下载周期，避免有些大镜像未下载完成就被动死亡退出</span>
</span></span><span class="line"><span class="cl"><span class="c1"># --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384</span>
</span></span><span class="line"><span class="cl"><span class="c1"># --image-pull-progress-deadline=30m</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@k8s-master01 ~<span class="o">]</span><span class="c1"># vim /etc/systemd/system/kubelet.service.d/10-kubelet.conf</span>
</span></span><span class="line"><span class="cl">... <span class="c1"># 下面这行中KUBELET_EXTRA_ARGS=后加入配置</span>
</span></span><span class="line"><span class="cl"><span class="nv">Environment</span><span class="o">=</span><span class="s2">&#34;KUBELET_EXTRA_ARGS=--tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 --image-pull-progress-deadline=30m&#34;</span>
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 集群配置优化，详见https://kubernetes.io/zh/docs/tasks/administer-cluster/reserve-compute-resources/</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@k8s-master01 ~<span class="o">]</span><span class="c1"># vim /etc/kubernetes/kubelet-conf.yml</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 文件中添加如下配置</span>
</span></span><span class="line"><span class="cl">rotateServerCertificates: <span class="nb">true</span>
</span></span><span class="line"><span class="cl">allowedUnsafeSysctls: <span class="c1"># 允许在修改内核参数，此操作按情况选择，用不到就不用设置</span>
</span></span><span class="line"><span class="cl"> - <span class="s2">&#34;net.core*&#34;</span>
</span></span><span class="line"><span class="cl"> - <span class="s2">&#34;net.ipv4.*&#34;</span>
</span></span><span class="line"><span class="cl">kubeReserved: <span class="c1"># 为Kubernetes集群守护进程组件预留资源，例如：kubelet、Runtime等</span>
</span></span><span class="line"><span class="cl">  cpu: <span class="s2">&#34;100m&#34;</span>
</span></span><span class="line"><span class="cl">  memory: 100Mi
</span></span><span class="line"><span class="cl">  ephemeral-storage: 1Gi
</span></span><span class="line"><span class="cl">systemReserved: <span class="c1"># 为系统守护进程预留资源，例如：sshd、cron等</span>
</span></span><span class="line"><span class="cl">  cpu: <span class="s2">&#34;100m&#34;</span>
</span></span><span class="line"><span class="cl">  memory: 100Mi
</span></span><span class="line"><span class="cl">  ephemeral-storage: 1Gi
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 为集群节点打标签，删除标签把 = 换成 - 即可</span>
</span></span><span class="line"><span class="cl">kubectl label nodes k8s-node01 node-role.kubernetes.io/node<span class="o">=</span>
</span></span><span class="line"><span class="cl">kubectl label nodes k8s-node02 node-role.kubernetes.io/node<span class="o">=</span>
</span></span><span class="line"><span class="cl">kubectl label nodes k8s-master01 node-role.kubernetes.io/master<span class="o">=</span>
</span></span><span class="line"><span class="cl">kubectl label nodes k8s-master02 node-role.kubernetes.io/master<span class="o">=</span>
</span></span><span class="line"><span class="cl">kubectl label nodes k8s-master03 node-role.kubernetes.io/master<span class="o">=</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 添加标签后查看集群状态</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@k8s-master01 ~<span class="o">]</span><span class="c1"># kubectl get node</span>
</span></span><span class="line"><span class="cl">NAME           STATUS   ROLES    AGE   VERSION
</span></span><span class="line"><span class="cl">k8s-master01   Ready    master   35m   v1.26.4
</span></span><span class="line"><span class="cl">k8s-master02   Ready    master   35m   v1.26.4
</span></span><span class="line"><span class="cl">k8s-master03   Ready    master   35m   v1.26.4
</span></span><span class="line"><span class="cl">k8s-node01     Ready    node     35m   v1.26.4
</span></span><span class="line"><span class="cl">k8s-node02     Ready    node     35m   v1.26.4
</span></span></code></pre></div><p>生产环境建议ETCD集群和Kubernetes集群分离，而且使用高性能数据盘存储数据，根据情况决定是否将Master节点也作为Pod调度节点。</p>
<h2 id="测试集群">测试集群<a hidden class="anchor" aria-hidden="true" href="#测试集群">#</a></h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># 测试namespace</span>
</span></span><span class="line"><span class="cl">kubectl get namespace
</span></span><span class="line"><span class="cl">kubectl create namespace <span class="nb">test</span>
</span></span><span class="line"><span class="cl">kubectl get namespace
</span></span><span class="line"><span class="cl">kubectl delete namespace <span class="nb">test</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 创建nginx实例并开放端口</span>
</span></span><span class="line"><span class="cl">kubectl create deployment nginx --image<span class="o">=</span>nginx
</span></span><span class="line"><span class="cl">kubectl expose deployment nginx --port<span class="o">=</span><span class="m">80</span> --type<span class="o">=</span>NodePort
</span></span><span class="line"><span class="cl"><span class="c1"># 查看调度状态和端口号</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@k8s-master01 ~<span class="o">]</span><span class="c1"># kubectl get po,svc -o wide</span>
</span></span><span class="line"><span class="cl">NAME                         READY   STATUS    RESTARTS   AGE     IP               NODE           NOMINATED NODE   READINESS GATES
</span></span><span class="line"><span class="cl">pod/nginx-748c667d99-dmtn6   1/1     Running   <span class="m">0</span>          9m55s   172.25.244.194   k8s-master01   &lt;none&gt;           &lt;none&gt;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>        AGE   SELECTOR
</span></span><span class="line"><span class="cl">service/kubernetes   ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP        69m   &lt;none&gt;
</span></span><span class="line"><span class="cl">service/nginx        NodePort    10.110.18.105   &lt;none&gt;        80:31687/TCP   9s    <span class="nv">app</span><span class="o">=</span>nginx
</span></span></code></pre></div><p>在浏览器输入<a href="http://%E4%BB%BB%E6%84%8F%E8%8A%82%E7%82%B9IP:31687/">http://任意节点IP:31687/</a> 访问nginx，访问结果如图</p>
<p><img loading="lazy" src="https://deemoprobe.oss-cn-shanghai.aliyuncs.com/images/20230421165824.png" alt="20230421165824"  />
</p>
<p>至此，基于二进制方式的Kubernetes高可用集群部署并验证成功。</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share 二进制方式部署kubernetes高可用集群 on twitter"
        href="https://twitter.com/intent/tweet/?text=%e4%ba%8c%e8%bf%9b%e5%88%b6%e6%96%b9%e5%bc%8f%e9%83%a8%e7%bd%b2kubernetes%e9%ab%98%e5%8f%af%e7%94%a8%e9%9b%86%e7%be%a4&amp;url=https%3a%2f%2fdeemoprobe.github.io%2f%25E4%25BA%258C%25E8%25BF%259B%25E5%2588%25B6%25E6%2596%25B9%25E5%25BC%258F%25E9%2583%25A8%25E7%25BD%25B2kubernetes%25E9%25AB%2598%25E5%258F%25AF%25E7%2594%25A8%25E9%259B%2586%25E7%25BE%25A4%2f&amp;hashtags=">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 二进制方式部署kubernetes高可用集群 on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fdeemoprobe.github.io%2f%25E4%25BA%258C%25E8%25BF%259B%25E5%2588%25B6%25E6%2596%25B9%25E5%25BC%258F%25E9%2583%25A8%25E7%25BD%25B2kubernetes%25E9%25AB%2598%25E5%258F%25AF%25E7%2594%25A8%25E9%259B%2586%25E7%25BE%25A4%2f&amp;title=%e4%ba%8c%e8%bf%9b%e5%88%b6%e6%96%b9%e5%bc%8f%e9%83%a8%e7%bd%b2kubernetes%e9%ab%98%e5%8f%af%e7%94%a8%e9%9b%86%e7%be%a4&amp;summary=%e4%ba%8c%e8%bf%9b%e5%88%b6%e6%96%b9%e5%bc%8f%e9%83%a8%e7%bd%b2kubernetes%e9%ab%98%e5%8f%af%e7%94%a8%e9%9b%86%e7%be%a4&amp;source=https%3a%2f%2fdeemoprobe.github.io%2f%25E4%25BA%258C%25E8%25BF%259B%25E5%2588%25B6%25E6%2596%25B9%25E5%25BC%258F%25E9%2583%25A8%25E7%25BD%25B2kubernetes%25E9%25AB%2598%25E5%258F%25AF%25E7%2594%25A8%25E9%259B%2586%25E7%25BE%25A4%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 二进制方式部署kubernetes高可用集群 on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fdeemoprobe.github.io%2f%25E4%25BA%258C%25E8%25BF%259B%25E5%2588%25B6%25E6%2596%25B9%25E5%25BC%258F%25E9%2583%25A8%25E7%25BD%25B2kubernetes%25E9%25AB%2598%25E5%258F%25AF%25E7%2594%25A8%25E9%259B%2586%25E7%25BE%25A4%2f&title=%e4%ba%8c%e8%bf%9b%e5%88%b6%e6%96%b9%e5%bc%8f%e9%83%a8%e7%bd%b2kubernetes%e9%ab%98%e5%8f%af%e7%94%a8%e9%9b%86%e7%be%a4">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 二进制方式部署kubernetes高可用集群 on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fdeemoprobe.github.io%2f%25E4%25BA%258C%25E8%25BF%259B%25E5%2588%25B6%25E6%2596%25B9%25E5%25BC%258F%25E9%2583%25A8%25E7%25BD%25B2kubernetes%25E9%25AB%2598%25E5%258F%25AF%25E7%2594%25A8%25E9%259B%2586%25E7%25BE%25A4%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 二进制方式部署kubernetes高可用集群 on whatsapp"
        href="https://api.whatsapp.com/send?text=%e4%ba%8c%e8%bf%9b%e5%88%b6%e6%96%b9%e5%bc%8f%e9%83%a8%e7%bd%b2kubernetes%e9%ab%98%e5%8f%af%e7%94%a8%e9%9b%86%e7%be%a4%20-%20https%3a%2f%2fdeemoprobe.github.io%2f%25E4%25BA%258C%25E8%25BF%259B%25E5%2588%25B6%25E6%2596%25B9%25E5%25BC%258F%25E9%2583%25A8%25E7%25BD%25B2kubernetes%25E9%25AB%2598%25E5%258F%25AF%25E7%2594%25A8%25E9%259B%2586%25E7%25BE%25A4%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 二进制方式部署kubernetes高可用集群 on telegram"
        href="https://telegram.me/share/url?text=%e4%ba%8c%e8%bf%9b%e5%88%b6%e6%96%b9%e5%bc%8f%e9%83%a8%e7%bd%b2kubernetes%e9%ab%98%e5%8f%af%e7%94%a8%e9%9b%86%e7%be%a4&amp;url=https%3a%2f%2fdeemoprobe.github.io%2f%25E4%25BA%258C%25E8%25BF%259B%25E5%2588%25B6%25E6%2596%25B9%25E5%25BC%258F%25E9%2583%25A8%25E7%25BD%25B2kubernetes%25E9%25AB%2598%25E5%258F%25AF%25E7%2594%25A8%25E9%259B%2586%25E7%25BE%25A4%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://deemoprobe.github.io/">PaperMod</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
