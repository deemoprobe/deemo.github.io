[{"content":"Dockerfile是构建容器镜像的命令合集文本文件，类似脚本。\n字段解析 简介 # 主要字段 FROM image[基础镜像,该文件创建新镜像所依赖的镜像] MAINTAINER user\u0026lt;email\u0026gt;[作者姓名和邮箱] RUN command[镜像构建时运行的命令] ADD [文件拷贝进镜像并解压] COPY [文件拷贝进镜像] CMD [容器启动时要运行的命令或参数] ENTRYPOINT [容器启动时要运行的命令] EXPOSE port[声明端口] WORKDIR work_directory[进入容器默认进入的目录] ENV set_env[创建环境变量] VOLUME [容器数据卷,用于数据保存和持久化] ONBUILD [当前Dockerfile构建时不会调用，当子镜像依赖本镜像（FROM）构建时触发ONBUILD后的命令] USER [指定构建镜像和运行容器的用户用户组] ARG [构建镜像时设定的变量] LABEL [为镜像添加元数据] 用法 # 执行任意路径下的dockerfile docker build -f /path/to/a/Dockerfile # 执行当前目录下dockerfile,注意最后的点 # -t(tag)打上tag docker build -t nginx:v1 . FROM 基础镜像 # 如果不指定版本，默认使用latest FROM image FROM image:tag FROM image@digest # 示例 FROM nginx:1.18.0 MAINTAINER 作者 MAINTAINER user MAINTAINER email MAINTAINER user\u0026lt;email\u0026gt; # 示例 MAINTAINER deemo\u0026lt;deemo@gmail.com\u0026gt; RUN 构建镜像时执行的命令 # Dockerfile里的指令每执行一次会在镜像文件系统中新建一层，为了避免多层文件造成镜像过大，多条命令写在一个RUN后面 # RUN指令创建的中间镜像会被缓存，并会在下次构建中使用。如果不想使用这些缓存镜像，可以在构建时指定--no-cache参数，如：docker build --no-cache RUN command RUN [\u0026#34;\u0026lt;executable\u0026gt;\u0026#34;,\u0026#34;\u0026lt;param1\u0026gt;\u0026#34;,\u0026#34;\u0026lt;param2\u0026gt;\u0026#34;,...] # 示例 RUN yum install -y curl RUN [\u0026#34;./test.php\u0026#34;,\u0026#34;dev\u0026#34;,\u0026#34;offline\u0026#34;] #等价于 RUN ./test.php dev offline ADD 本地文件拷贝进镜像，tar类型的会自动解压 ADD \u0026lt;src\u0026gt; \u0026lt;dest\u0026gt; # 示例 ADD file /dir/ #添加file到/dir/目录 ADD file dir/ #添加file到{WORKDIR}/dir目录 ADD fi* /dir #通配符，添加所有以fi开头的文件到/dir/目录 COPY 本地文件拷贝进镜像，但不会解压 COPY \u0026lt;src\u0026gt; \u0026lt;dest\u0026gt; CMD 容器启动时（docker run时）要运行的命令或参数 # 可以设置多个CMD,但最后一个生效,前面的不生效,也可以被docker run启动容器时后面加的命令替换 CMD [\u0026#34;\u0026lt;executable\u0026gt;\u0026#34;,\u0026#34;\u0026lt;param1\u0026gt;\u0026#34;,\u0026#34;\u0026lt;param2\u0026gt;\u0026#34;,...] #执行可执行文件 CMD [\u0026#34;\u0026lt;param1\u0026gt;\u0026#34;,\u0026#34;\u0026lt;param2\u0026gt;\u0026#34;,...] #已设置ENTRYPOINT，则调用ENTRYPOINT后添加CMD参数 CMD command param1 param2 ... #执行shell内部命令 # 示例 CMD [\u0026#34;/usr/bin/ls\u0026#34;,\u0026#34;-al\u0026#34;] CMD echo \u0026#34;hello\u0026#34; ENTRYPOINT 容器启动时要运行的命令 # 类似于CMD指令，但其不会被docker run的命令行参数指定的指令所覆盖 # 存在多个ENTRYPOINT时，仅最后一个生效 ENTRYPOINT [\u0026#34;\u0026lt;executeable\u0026gt;\u0026#34;,\u0026#34;\u0026lt;param1\u0026gt;\u0026#34;,\u0026#34;\u0026lt;param2\u0026gt;\u0026#34;,...] # 示例 ENTRYPOINT [\u0026#34;nginx\u0026#34;, \u0026#34;-c\u0026#34;] #定参 CMD [\u0026#34;/etc/nginx/nginx.conf\u0026#34;] #变参 EXPOSE 声明容器端口 # EXPOSE仅是声明端口。要使其可访问，需要在docker run运行容器时通过-p来指定端口映射，或通过-P参数来映射EXPOSE端口 EXPOSE \u0026lt;port\u0026gt; [\u0026lt;port\u0026gt;...] # 示例 EXPOSE 80 EXPOSE 80 443 WORKDIR 工作目录 WORKDIR path ENV 设置环境变量 ENV \u0026lt;key\u0026gt; \u0026lt;value\u0026gt; ENV \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt; ... # 示例 ENV dir=/webapp ENV name deemoprobe VOLUME 容器数据卷,用于数据保存和持久化 VOLUME [\u0026#34;/path/to/dir\u0026#34;] # 示例 VOLUME [\u0026#34;/data\u0026#34;] USER 指定构建镜像和运行容器的用户用户组 USER user USER user:group USER uid USER uid:gid USER user:gid USER uid:group # 示例 USER www USER 1080:tomcat ARG 构建镜像时设定的变量 # 与 ENV 作用一致。不过作用域不一样。ARG 设置的环境变量仅对 Dockerfile 内有效，也就是说只有 docker build 的过程中有效，构建好的镜像内不存在此环境变量。 ARG \u0026lt;name\u0026gt;[=\u0026lt;default value\u0026gt;] # 示例 ARG user=www ONBUILD 当构建一个被继承的Dockerfile时运行命令 # 子镜像构建时触发命令并执行。就是Dockerfile里用ONBUILD指定的命令，在本次构建镜像（假设镜像名为test）的过程中不会执行。当有新的Dockerfile使用了该镜像（FROM test），这时执行新镜像的Dockerfile构建时候，会执行test镜像中Dockerfile里的ONBUILD指定的命令。 ONBUILD [INSTRUCTION] # 示例 ONBUILD RUN yum install wget ONBUILD ADD . /data LABEL 为镜像添加元数据 LABEL \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt; \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt; \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt; ... # 示例 LABEL version=\u0026#34;1.0\u0026#34; des=\u0026#34;webapp\u0026#34; 实例1-简单尝试 [root@demo ~]# docker build --help Usage: docker build [OPTIONS] PATH | URL | - Build an image from a Dockerfile Options: --add-host list Add a custom host-to-IP mapping (host:ip) --build-arg list Set build-time variables --cache-from strings Images to consider as cache sources --cgroup-parent string Optional parent cgroup for the container --compress Compress the build context using gzip --cpu-period int Limit the CPU CFS (Completely Fair Scheduler) period --cpu-quota int Limit the CPU CFS (Completely Fair Scheduler) quota -c, --cpu-shares int CPU shares (relative weight) --cpuset-cpus string CPUs in which to allow execution (0-3, 0,1) --cpuset-mems string MEMs in which to allow execution (0-3, 0,1) --disable-content-trust Skip image verification (default true) -f, --file string Name of the Dockerfile (Default is \u0026#39;PATH/Dockerfile\u0026#39;) --force-rm Always remove intermediate containers --iidfile string Write the image ID to the file --isolation string Container isolation technology --label list Set metadata for an image -m, --memory bytes Memory limit --memory-swap bytes Swap limit equal to memory plus swap: \u0026#39;-1\u0026#39; to enable unlimited swap --network string Set the networking mode for the RUN instructions during build (default \u0026#34;default\u0026#34;) --no-cache Do not use cache when building the image --pull Always attempt to pull a newer version of the image -q, --quiet Suppress the build output and print image ID on success --rm Remove intermediate containers after a successful build (default true) --security-opt strings Security options --shm-size bytes Size of /dev/shm -t, --tag list Name and optionally a tag in the \u0026#39;name:tag\u0026#39; format --target string Set the target build stage to build. --ulimit ulimit Ulimit options (default []) # 创建Dockerfile [root@demo ~]# cat Dockerfile FROM centos:centos7.9.2009 # 该镜像已经提前拉取 MAINTAINER deemo\u0026lt;deemo@gmail.com\u0026gt; RUN curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo \u0026amp;\u0026amp; sed -i -e \u0026#39;/mirrors.cloud.aliyuncs.com/d\u0026#39; -e \u0026#39;/mirrors.aliyuncs.com/d\u0026#39; /etc/yum.repos.d/CentOS-Base.repo \u0026amp;\u0026amp; yum install vim -y CMD [\u0026#34;/bin/echo\u0026#34;,\u0026#34;hello\u0026#34;] # 对比父镜像和新镜像，有sed和curl没有vim [root@demo ~]# docker run -it --rm centos:centos7.9.2009 whereis sed sed: /usr/bin/sed [root@demo ~]# docker run -it --rm centos:centos7.9.2009 whereis curl curl: /usr/bin/curl [root@demo ~]# docker run -it --rm centos:centos7.9.2009 whereis vim # 构建镜像 [root@demo ~]# docker build -t centos7:v2 . [+] Building 76.1s (6/6) FINISHED =\u0026gt; [internal] load build definition from Dockerfile 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 408B 0.0s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; [internal] load metadata for docker.io/library/centos:centos7.9.2009 0.0s =\u0026gt; CACHED [1/2] FROM docker.io/library/centos:centos7.9.2009 0.0s =\u0026gt; [2/2] RUN curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo \u0026amp;\u0026amp; sed -i -e /mirrors.clou 72.2s =\u0026gt; exporting to image 3.8s =\u0026gt; =\u0026gt; exporting layers 3.8s =\u0026gt; =\u0026gt; writing image sha256:30f266dbdecb307db7ee31297d333ae19a88f4add15bfd52f17bc6acfeea13f8 0.0s =\u0026gt; =\u0026gt; naming to docker.io/library/centos7:v2 # 查看结果 [root@demo ~]# docker images | grep v2 centos7 v2 30f266dbdecb 3 minutes ago 463MB # 新镜像安装了vim，查看 [root@demo ~]# docker run -it --rm centos7:v2 whereis vim vim: /usr/bin/vim /usr/share/vim # 基于新镜像运行容器输出了hello [root@demo ~]# docker run -it centos7:v2 hello 实例2-构建Tomcat # 准备Dockerfile [root@demo ~]# vim Dockerfile # 基础镜像centos:centos7.9.2009 FROM centos:centos7.9.2009 # 作者签名 MAINTAINER deemoprobe\u0026lt;deemoprobe@gmail.com\u0026gt; # 拷贝宿主机当前目录下文件 COPY tomcat.txt /usr/local/tomcat8.txt # 添加Tomcat安装包并解压至/usr/local ADD apache-tomcat-8.5.53.tar.gz /usr/local # 添加jdk安装包并解压至/usr/local ADD jdk-8u271-linux-x64.tar.gz /usr/local # 安装vim RUN curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo \u0026amp;\u0026amp; sed -i -e \u0026#39;/mirrors.cloud.aliyuncs.com/d\u0026#39; -e \u0026#39;/mirrors.aliyuncs.com/d\u0026#39; /etc/yum.repos.d/CentOS-Base.repo \u0026amp;\u0026amp; yum install vim -y # 设置环境变量 ENV MYPATH /usr/local # 指定工作目录，使用ENV设定的环境变量 WORKDIR $MYPATH # 配置JDK环境 ENV JAVA_HOME /usr/local/jdk1.8.0_271 ENV CLASSPATH $JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar ENV CATALINA_HOME /usr/local/apache-tomcat-8.5.53 ENV CATALINA_BASE /usr/local/apache-tomcat-8.5.53 ENV PATH $PATH:$JAVA_HOME/bin:$CATALINA_HOME/lib:$CATALINA_HOME/bin # 声明端口8080 EXPOSE 8080 # 启动 # ENTRYPOINT [ \u0026#34;/usr/local/apache-tomcat-8.5.53/bin/startup.sh\u0026#34; ] # CMD [ \u0026#34;/usr/local/apache-tomcat-8.5.53/bin/catalina.sh\u0026#34;, \u0026#34;run\u0026#34; ] CMD /usr/local/apache-tomcat-8.5.53/bin/startup.sh \u0026amp;\u0026amp; tail -F /usr/local/apache-tomcat-8.5.53/bin/logs/catalina.out # 准备必要的文件到当前目录 [root@demo ~]# echo \u0026#34;tomcat\u0026#34; \u0026gt;\u0026gt; tomcat.txt # 上传Tomcat和jdk安装包 [root@demo ~]# ls apache-tomcat-8.5.53.tar.gz Dockerfile jdk-8u271-linux-x64.tar.gz tomcat.txt # 构建镜像 [root@demo ~]# docker build -t tomcat8:v1 . [+] Building 50.1s (9/10) =\u0026gt; [internal] load build definition from Dockerfile 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 1.20kB 0.0s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; [internal] load metadata for docker.io/library/centos:centos7.9.2009 0.0s =\u0026gt; CACHED [1/6] FROM docker.io/library/centos:centos7.9.2009 0.0s =\u0026gt; [internal] load build context 4.0s =\u0026gt; =\u0026gt; transferring context: 153.48MB 4.0s =\u0026gt; [2/6] COPY tomcat.txt /usr/local/tomcat8.txt 0.3s =\u0026gt; [3/6] ADD apache-tomcat-8.5.53.tar.gz /usr/local 1.2s =\u0026gt; [4/6] ADD jdk-8u271-linux-x64.tar.gz /usr/local 7.6s =\u0026gt; [5/6] RUN curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo \u0026amp;\u0026amp; sed -i -e /mirrors.clou 48.0s =\u0026gt; [6/6] WORKDIR /usr/local 0.0s =\u0026gt; exporting to image 4.9s =\u0026gt; =\u0026gt; exporting layers 4.9s =\u0026gt; =\u0026gt; writing image sha256:dbccbeed821449bb611ff35b3071547035d925775a8b9d566e888ef6f895c3cf 0.0s =\u0026gt; =\u0026gt; naming to docker.io/library/tomcat8:v1 0.0s # 查看结果 [root@demo ~]# docker images | grep tomcat tomcat8 v1 dbccbeed8214 About a minute ago 833MB # 运行一个容器 # 如果有读写权限问题可以加上--privileged=true [root@demo ~]# docker run -d -p 1080:8080 --name myweb -v /root/web:/usr/local/apache-tomcat-8.5.53/webapps/web -v /root/tomcatlog:/usr/local/apache-tomcat-8.5.53/logs --privileged=true tomcat8:v1 d5f63d3513ba696e54fd7353e52d15a7c2582101d1c71067028c6251f2d82bef [root@demo ~]# docker ps | grep tomcat d5f63d3513ba tomcat8:v1 \u0026#34;/bin/sh -c \u0026#39;/usr/lo…\u0026#34; 17 seconds ago Up 15 seconds 0.0.0.0:1080-\u0026gt;8080/tcp, :::1080-\u0026gt;8080/tcp myweb # 访问Tomcat首页，直接 curl localhost:1080 可以看到返回Tomcat首页的HTML源码 [root@demo ~]# curl -I localhost:1080 HTTP/1.1 200 Content-Type: text/html;charset=UTF-8 Transfer-Encoding: chunked Date: Mon, 10 Jan 2022 10:38:12 GMT # 查看WORKDIR [root@demo ~]# docker exec d5f63d3513ba pwd /usr/local # 查看JDK版本 [root@demo ~]# docker exec d5f63d3513ba java -version java version \u0026#34;1.8.0_271\u0026#34; Java(TM) SE Runtime Environment (build 1.8.0_271-b09) Java HotSpot(TM) 64-Bit Server VM (build 25.271-b09, mixed mode) # 创建web项目，发布服务 [root@demo ~]# ls -l total 149860 -rw-r--r--. 1 root root 10300600 Jan 10 2022 apache-tomcat-8.5.53.tar.gz -rw-r--r--. 1 root root 1073 Jan 10 18:04 Dockerfile -rw-r--r--. 1 root root 143142634 Jan 10 2022 jdk-8u271-linux-x64.tar.gz drwxr-xr-x. 2 root root 197 Jan 10 18:36 tomcatlog -rw-r--r--. 1 root root 7 Jan 10 18:06 tomcat.txt drwxr-xr-x. 2 root root 6 Jan 10 18:36 web [root@demo ~]# cd web [root@demo web]# vim web.jsp \u0026lt;%@ page language=\u0026#34;java\u0026#34; contentType=\u0026#34;text/html; charset=UTF-8\u0026#34; pageEncoding=\u0026#34;UTF-8\u0026#34;%\u0026gt; \u0026lt;!DOCTYPE html PUBLIC \u0026#34;-//W3C//DTD HTML 4.01 Transitional//EN\u0026#34; \u0026#34;http://www.w3.org/TR/html4/loose.dtd\u0026#34;\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta http-equiv=\u0026#34;Content-Type\u0026#34; content=\u0026#34;text/html; charset=UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Insert title here\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; -----------welcome------------ \u0026lt;%=\u0026#34;I am in docker tomcat8\u0026#34;%\u0026gt; \u0026lt;br\u0026gt; \u0026lt;br\u0026gt; \u0026lt;% System.out.println(\u0026#34;=============docker tomcat8\u0026#34;);%\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; [root@demo web]# mkdir WEB-INF [root@demo web]# cd WEB-INF/ [root@demo WEB-INF]# vim web.xml \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;web-app xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns=\u0026#34;http://java.sun.com/xml/ns/javaee\u0026#34; xsi:schemaLocation=\u0026#34;http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd\u0026#34; id=\u0026#34;WebApp_ID\u0026#34; version=\u0026#34;2.5\u0026#34;\u0026gt; \u0026lt;display-name\u0026gt;test-tomcat8\u0026lt;/display-name\u0026gt; \u0026lt;/web-app\u0026gt; # 查看项目结构 [root@demo ~]# yum install tree -y [root@demo ~]# tree . ├── apache-tomcat-8.5.53.tar.gz ├── Dockerfile ├── jdk-8u271-linux-x64.tar.gz ├── tomcatlog │ ├── catalina.2022-01-10.log │ ├── catalina.out │ ├── host-manager.2022-01-10.log │ ├── localhost.2022-01-10.log │ ├── localhost_access_log.2022-01-10.txt │ └── manager.2022-01-10.log ├── tomcat.txt └── web ├── WEB-INF │ └── web.xml └── web.jsp 3 directories, 12 files # 查看容器内数据卷同步结果 [root@centos7 WEB-INF]# docker ps | grep tomcat d5f63d3513ba tomcat8:v1 \u0026#34;/bin/sh -c \u0026#39;/usr/lo…\u0026#34; 5 minutes ago Up 4 minutes 0.0.0.0:1080-\u0026gt;8080/tcp, :::1080-\u0026gt;8080/tcp myweb [root@demo ~]# docker exec d5f63d3513ba ls -l /usr/local/apache-tomcat-8.5.53/webapps/web total 4 drwxr-xr-x. 2 root root 21 Jan 10 10:49 WEB-INF -rw-r--r--. 1 root root 500 Jan 10 10:48 web.jsp [root@demo ~]# docker exec d5f63d3513ba ls -l /usr/local/apache-tomcat-8.5.53/logs total 24 -rw-r-----. 1 root root 7173 Jan 10 10:49 catalina.2022-01-10.log -rw-r-----. 1 root root 7173 Jan 10 10:49 catalina.out -rw-r-----. 1 root root 0 Jan 10 10:36 host-manager.2022-01-10.log -rw-r-----. 1 root root 459 Jan 10 10:36 localhost.2022-01-10.log -rw-r-----. 1 root root 281 Jan 10 10:40 localhost_access_log.2022-01-10.txt -rw-r-----. 1 root root 0 Jan 10 10:36 manager.2022-01-10.log # 重启一下容器 [root@demo ~]# docker restart d5f63d3513ba # 访问结果 [root@demo ~]# curl localhost:1080/web/web.jsp \u0026lt;!DOCTYPE html PUBLIC \u0026#34;-//W3C//DTD HTML 4.01 Transitional//EN\u0026#34; \u0026#34;http://www.w3.org/TR/html4/loose.dtd\u0026#34;\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta http-equiv=\u0026#34;Content-Type\u0026#34; content=\u0026#34;text/html; charset=UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Insert title here\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; -----------welcome------------ I am in docker tomcat8 \u0026lt;br\u0026gt; \u0026lt;br\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; # 查看日志，可以看到访问记录，其他日志文件可以看到Tomcat启动记录等 [root@demo ~]# cd tomcatlog/ [root@demo tomcatlog]# cat localhost_access_log.2022-01-10.txt 172.17.0.1 - - [10/Jan/2022:10:38:12 +0000] \u0026#34;HEAD / HTTP/1.1\u0026#34; 200 - 172.17.0.1 - - [10/Jan/2022:10:38:21 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 11215 172.17.0.1 - - [10/Jan/2022:10:39:49 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 11215 172.17.0.1 - - [10/Jan/2022:10:39:56 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 11215 172.17.0.1 - - [10/Jan/2022:10:58:16 +0000] \u0026#34;GET /web/web.jsp HTTP/1.1\u0026#34; 200 352 参考文档：Docker官方镜像库示例\n","permalink":"https://deemoprobe.github.io/posts/tech/dockerfile/","summary":"Dockerfile是构建容器镜像的命令合集文本文件，类似脚本。 字段解析 简介 # 主要字段 FROM image[基础镜像,该文件创建新镜像所依赖的镜像] MAINTAINER user\u0026lt;email\u0026gt;[作者姓名和邮箱] RUN command[镜像构建时运行的命令] ADD [文件拷贝进镜像并解压] COPY [文件拷贝进镜像]","title":"Dockerfile"},{"content":"本文整理了docker常用的一些命令。包括镜像命令，容器命令，日志查看，容器的高级操作以及从容器传输文件到宿主机。\n常用 # 查看docker版本 docker version # 查看docker系统信息 docker info # 实时监控容器的运行情况 docker stats # 查看容器或镜像的底层信息 docker inspect ID/NAME # 查看容器中进程情况 docker top ID/NAME # 查看容器中进程的日志 docker logs ID/NAME # 进入某个容器系统 docker exec -it ID/NAME bash 详细用法 Usage: docker COMMAND A self-sufficient runtime for containers Commands: attach Attach local standard input, output, and error streams to a running container build Build an image from a Dockerfile commit Create a new image from a container changes cp Copy files/folders between a container and the local filesystem create Create a new container diff Inspect changes to files or directories on a container filesystem events Get real time events from the server exec Run a command in a running container export Export a container filesystem as a tar archive history Show the history of an image images List images import Import the contents from a tarball to create a filesystem image info Display system-wide information inspect Return low-level information on Docker objects kill Kill one or more running containers load Load an image from a tar archive or STDIN login Log in to a Docker registry logout Log out from a Docker registry logs Fetch the logs of a container pause Pause all processes within one or more containers port List port mappings or a specific mapping for the container ps List containers pull Pull an image or a repository from a registry push Push an image or a repository to a registry rename Rename a container restart Restart one or more containers rm Remove one or more containers rmi Remove one or more images run Run a command in a new container save Save one or more images to a tar archive (streamed to STDOUT by default) search Search the Docker Hub for images start Start one or more stopped containers stats Display a live stream of container(s) resource usage statistics stop Stop one or more running containers tag Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE top Display the running processes of a container unpause Unpause all processes within one or more containers update Update configuration of one or more containers version Show the Docker version information wait Block until one or more containers stop, then print their exit codes 镜像命令 REPOSITORY: 镜像的仓库源 TAG: 镜像标签 IMAGE ID: 镜像ID CREATED: 镜像已创建时间 SIZE: 镜像大小 docker images Usage: docker images [OPTIONS] [REPOSITORY[:TAG]] List images Options: -a, --all Show all images (default hides intermediate images) --digests Show digests -f, --filter filter Filter output based on conditions provided --format string Pretty-print images using a Go template --no-trunc Do not truncate output -q, --quiet Only show image IDs # 查看镜像 [root@demo ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE tomcat latest fb5657adc892 12 days ago 680MB hello-world latest feb5d9fea6a5 3 months ago 13.3kB centos latest 5d0da3dc9764 3 months ago 231MB # 查看所有镜像 docker images -a # 查看镜像ID docker images -q # 查看所有镜像ID docker images -qa docker search Usage: docker search [OPTIONS] [IMAGE] Search the Docker Hub for images Options: -f, --filter filter Filter output based on conditions provided --format string Pretty-print search using a Go template --limit int Max number of search results (default 25) --no-trunc Do not truncate output # 从Docker Hub上查询已存在镜像 [root@demo ~]# docker search nginx NAME DESCRIPTION STARS OFFICIAL AUTOMATED nginx Official build of Nginx. 16062 [OK] jwilder/nginx-proxy Automated Nginx reverse proxy for docker con… 2105 [OK] richarvey/nginx-php-fpm Container running Nginx + PHP-FPM capable of… 819 [OK] jc21/nginx-proxy-manager Docker container for managing Nginx proxy ho… 303 linuxserver/nginx An Nginx container, brought to you by LinuxS… 161 tiangolo/nginx-rtmp Docker image with Nginx using the nginx-rtmp… 148 [OK] ... # 根据stars数目来搜索IMAGE # 查看800星以上的nginx镜像 [root@demo ~]# docker search nginx -f stars=800 NAME DESCRIPTION STARS OFFICIAL AUTOMATED nginx Official build of Nginx. 16062 [OK] jwilder/nginx-proxy Automated Nginx reverse proxy for docker con… 2105 [OK] richarvey/nginx-php-fpm Container running Nginx + PHP-FPM capable of… 819 [OK] # 搜索800星以上的nginx镜像，并且不切割摘要信息（摘要全部显示） [root@demo ~]# docker search nginx --no-trunc -f stars=800 NAME DESCRIPTION STARS OFFICIAL AUTOMATED nginx Official build of Nginx. 16062 [OK] jwilder/nginx-proxy Automated Nginx reverse proxy for docker containers 2105 [OK] richarvey/nginx-php-fpm Container running Nginx + PHP-FPM capable of pulling application code from git 819 [OK] docker pull Usage: docker pull [OPTIONS] NAME[:TAG|@DIGEST] Pull an image or a repository from a registry Options: -a, --all-tags Download all tagged images in the repository --disable-content-trust Skip image verification (default true) --platform string Set platform if server is multi-platform capable -q, --quiet Suppress verbose output # 从配置好的仓库拉取镜像, 未配置的话默认从Docker Hub上获取 docker pull IMAGE \u0026lt;==\u0026gt; docker pull IMAGE:latest # 拉取指定版本镜像 docker pull IMAGE:TAG docker rmi Usage: docker rmi [OPTIONS] IMAGE [IMAGE...] Remove one or more images Options: -f, --force Force removal of the image --no-prune Do not delete untagged parents # 删除最新版本镜像 docker rmi IMAGE \u0026lt;==\u0026gt; docker rmi IMAGE:latest # 删除指定版本镜像 docker rmi IMAGE:TAG # -f 强制删除镜像(可删除多层镜像) docker rmi -f IMAGE # 指定镜像ID进行删除 docker rmi -f IMAGE_ID # 删除多个镜像 docker rmi -f IMAGE1 IMAGE2 IMAGE3 # 删除所有镜像 docker rmi -f $(docker images -qa) # 实例 [root@demo ~]# docker rmi hello-world Untagged: hello-world:latest Untagged: hello-world@sha256:2498fce14358aa50ead0cc6c19990fc6ff866ce72aeb5546e1d59caac3d0d60f Deleted: sha256:feb5d9fea6a5e9606aa995e879d862b825965ba48de054caab5ef356dc6b3412 Deleted: sha256:e07ee1baac5fae6a26f30cabfe54a36d3402f96afda318fe0a96cec4ca393359 dangling悬挂镜像 仓库名、标签均为的镜像被称为悬挂镜像，这种镜像已经失去了存在的价值。出现悬挂镜像原因一般是在docker pull *:latest时产生。当新版本发布后重新pull，镜像名会被新镜像所占用，旧镜像的名字会变成。\n# 删除悬挂镜像（dangling），-f不显示提示确认信息，直接强制删除 [root@harbor ~]# docker image prune WARNING! This will remove all dangling images. Are you sure you want to continue? [y/N] # 删除所有未使用的镜像（即未运行容器的镜像），-f不显示提示确认信息，直接强制删除 [root@harbor ~]# docker image prune -a WARNING! This will remove all images without at least one container associated to them. Are you sure you want to continue? [y/N] 容器命令 docker run docker run [OPTIONS] IMAGE [COMMAND] [ARG...] OPTIONS字段说明: - --name 自定义容器名 - -d 后台运行容器 - -it 新建伪终端交互运行容器 - -P 随机分配端口映射 - -p 指定端口映射, 有以下四种方式 - ip:hostPort:containerPort - ip::containerPort - hostPort:containerPort - containerPort # 先获取镜像 [root@demo ~]# docker pull centos [root@demo ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE tomcat latest fb5657adc892 12 days ago 680MB centos latest 5d0da3dc9764 3 months ago 231MB # 启动运行CentOS容器(本地有该镜像就直接启动, 没有就自动拉取) [root@demo ~]# docker run -it centos [root@f75fd428066f /]# cat /etc/redhat-release CentOS Linux release 8.4.2105 [root@f75fd428066f /]# exit [root@demo ~]# docker run -it --namecentos8 centos [root@dd16aaf5cbcd /]# exit [root@demo ~]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES dd16aaf5cbcd centos \u0026#34;/bin/bash\u0026#34; 58 seconds ago Exited (0) 52 seconds ago centos8 f75fd428066f centos \u0026#34;/bin/bash\u0026#34; 3 minutes ago Exited (0) 2 minutes ago silly_engelbart # 为nginx镜像随机分配端口映射 [root@demo ~]# docker run -it -P nginx:1.18.0 Unable to find image \u0026#39;nginx:1.18.0\u0026#39; locally 1.18.0: Pulling from library/nginx f7ec5a41d630: Pull complete 0b20d28b5eb3: Pull complete 1576642c9776: Pull complete c12a848bad84: Pull complete 03f221d9cf00: Pull complete Digest: sha256:e90ac5331fe095cea01b121a3627174b2e33e06e83720e9a934c7b8ccc9c55a0 Status: Downloaded newer image for nginx:1.18.0 /docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration /docker-entrypoint.sh: Looking for bash scripts in /docker-entrypoint.d/ /docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh 10-listen-on-ipv6-by-default.sh: Getting the checksum of /etc/nginx/conf.d/default.conf 10-listen-on-ipv6-by-default.sh: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf /docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh /docker-entrypoint.sh: Configuration complete; ready for start up # 另起一个终端，查看分配的端口 [root@demo ~]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 97c9f5f24db2 nginx:1.18.0 \u0026#34;/docker-entrypoint.…\u0026#34; 42 seconds ago Up 40 seconds 0.0.0.0:49153-\u0026gt;80/tcp, :::49153-\u0026gt;80/tcp jovial_burnell # 指定端口映射 [root@demo ~]# docker run -it --name nginx-web -p 8080:80 nginx:1.18.0 /docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration /docker-entrypoint.sh: Looking for bash scripts in /docker-entrypoint.d/ /docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh 10-listen-on-ipv6-by-default.sh: Getting the checksum of /etc/nginx/conf.d/default.conf 10-listen-on-ipv6-by-default.sh: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf /docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh /docker-entrypoint.sh: Configuration complete; ready for start up # 另起一个终端查看容器信息 [root@demo ~]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7e243e46f6f6 nginx:1.18.0 \u0026#34;/docker-entrypoint.…\u0026#34; 23 seconds ago Up 22 seconds 0.0.0.0:8080-\u0026gt;80/tcp, :::8080-\u0026gt;80/tcp nginx-web # 访问nginx [root@demo ~]# curl localhost:8080 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; # 在第一个终端可看到访问日志 172.17.0.1 - - [04/Jan/2022:01:08:26 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 612 \u0026#34;-\u0026#34; \u0026#34;curl/7.29.0\u0026#34; \u0026#34;-\u0026#34; # 浏览器访问 docker ps Usage: docker ps [OPTIONS] List containers Options: -a, --all Show all containers (default shows just running) -f, --filter filter Filter output based on conditions provided --format string Pretty-print containers using a Go template -n, --last int Show n last created containers (includes all states) (default -1) -l, --latest Show the latest created container (includes all states) --no-trunc Do not truncate output -q, --quiet Only display container IDs -s, --size Display total file sizes # 查看在运行容器 [root@demo ~]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7e243e46f6f6 nginx:1.18.0 \u0026#34;/docker-entrypoint.…\u0026#34; 3 minutes ago Up 3 minutes 0.0.0.0:8080-\u0026gt;80/tcp, :::8080-\u0026gt;80/tcp nginx-web # 查看在运行容器ID [root@demo ~]# docker ps -q 7e243e46f6f6 # 查看所有容器（包含已退出的容器） [root@demo ~]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7e243e46f6f6 nginx:1.18.0 \u0026#34;/docker-entrypoint.…\u0026#34; 3 minutes ago Up 3 minutes 0.0.0.0:8080-\u0026gt;80/tcp, :::8080-\u0026gt;80/tcp nginx-web 97c9f5f24db2 nginx:1.18.0 \u0026#34;/docker-entrypoint.…\u0026#34; 5 minutes ago Exited (0) 4 minutes ago jovial_burnell dd16aaf5cbcd centos \u0026#34;/bin/bash\u0026#34; 10 minutes ago Exited (0) 10 minutes ago centos8 f75fd428066f centos \u0026#34;/bin/bash\u0026#34; 12 minutes ago Exited (0) 11 minutes ago silly_engelbart # 查看所有容器ID（包含已退出的容器） [root@demo ~]# docker ps -qa 7e243e46f6f6 97c9f5f24db2 dd16aaf5cbcd f75fd428066f # 查看最近使用的两个容器 [root@demo ~]# docker ps -n 2 CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7e243e46f6f6 nginx:1.18.0 \u0026#34;/docker-entrypoint.…\u0026#34; 5 minutes ago Up 5 minutes 0.0.0.0:8080-\u0026gt;80/tcp, :::8080-\u0026gt;80/tcp nginx-web 97c9f5f24db2 nginx:1.18.0 \u0026#34;/docker-entrypoint.…\u0026#34; 7 minutes ago Exited (0) 6 minutes ago jovial_burnell # 查看最近一次启动的容器 [root@demo ~]# docker ps -l CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7e243e46f6f6 nginx:1.18.0 \u0026#34;/docker-entrypoint.…\u0026#34; 5 minutes ago Up 5 minutes 0.0.0.0:8080-\u0026gt;80/tcp, :::8080-\u0026gt;80/tcp nginx-web # 查看容器的大小 [root@demo ~]# docker ps -s CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES SIZE 7e243e46f6f6 nginx:1.18.0 \u0026#34;/docker-entrypoint.…\u0026#34; 6 minutes ago Up 6 minutes 0.0.0.0:8080-\u0026gt;80/tcp, :::8080-\u0026gt;80/tcp nginx-web 1.12kB (virtual 133MB) 容器启停 # 启动已停止的容器 docker start 容器名或ID # 重启容器 docker restart 容器名或ID # 停止容器 docker stop 容器名或ID # 强制停止容器 docker kill 容器名或ID # 实例 [root@demo ~]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7e243e46f6f6 nginx:1.18.0 \u0026#34;/docker-entrypoint.…\u0026#34; 7 minutes ago Exited (0) 8 seconds ago nginx-web 97c9f5f24db2 nginx:1.18.0 \u0026#34;/docker-entrypoint.…\u0026#34; 9 minutes ago Exited (0) 8 minutes ago jovial_burnell dd16aaf5cbcd centos \u0026#34;/bin/bash\u0026#34; 14 minutes ago Exited (0) 14 minutes ago centos8 f75fd428066f centos \u0026#34;/bin/bash\u0026#34; 16 minutes ago Exited (0) 15 minutes ago silly_engelbart [root@demo ~]# docker start 7e243e46f6f6 7e243e46f6f6 [root@demo ~]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7e243e46f6f6 nginx:1.18.0 \u0026#34;/docker-entrypoint.…\u0026#34; 7 minutes ago Up 4 seconds 0.0.0.0:8080-\u0026gt;80/tcp, :::8080-\u0026gt;80/tcp nginx-web [root@demo ~]# docker stop nginx-web nginx-web [root@demo ~]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 删除容器 # 删除已停止容器 docker rm 容器名或ID # 强制删除(若在运行,也会强制停止后删除) docker rm -f 容器名或ID # 删除全部容器 docker rm -f $(docker ps -qa) or docker ps -qa | xargs docker rm # 实例 [root@demo ~]# docker rm $(docker ps -qa) 7e243e46f6f6 97c9f5f24db2 dd16aaf5cbcd f75fd428066f [root@demo ~]# docker ps -qa 进入正在运行的容器 Usage: docker exec [OPTIONS] CONTAINER COMMAND [ARG...] Run a command in a running container Options: -d, --detach Detached mode: run command in the background --detach-keys string Override the key sequence for detaching a container -e, --env list Set environment variables --env-file list Read in a file of environment variables -i, --interactive Keep STDIN open even if not attached --privileged Give extended privileges to the command -t, --tty Allocate a pseudo-TTY -u, --user string Username or UID (format: \u0026lt;name|uid\u0026gt;[:\u0026lt;group|gid\u0026gt;]) -w, --workdir string Working directory inside the container # 进入正在运行的容器并启用交互 docker exec -it 容器ID /bin/bash # 不进入正在运行的容器直接交互,比如查看根目录 docker exec -it 容器ID ls -al / exit # 退出 # 实例 # 后台启一个nginx [root@demo ~]# docker run -itd --nametest_exec nginx Unable to find image \u0026#39;nginx:latest\u0026#39; locally latest: Pulling from library/nginx a2abf6c4d29d: Pull complete a9edb18cadd1: Pull complete 589b7251471a: Pull complete 186b1aaa4aa6: Pull complete b4df32aa5a72: Pull complete a0bcbecc962e: Pull complete Digest: sha256:0d17b565c37bcbd895e9d92315a05c1c3c9a29f762b011a10c54a66cd53c9b31 Status: Downloaded newer image for nginx:latest 864a1bdcf178ae110817a3d2f1d9cbf3b4f6d9bba0d7e477b971c403b5281e8a [root@demo ~]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 864a1bdcf178 nginx \u0026#34;/docker-entrypoint.…\u0026#34; 28 seconds ago Up 27 seconds 80/tcp test_exec # 进入容器 [root@demo ~]# docker exec -it test_exec /bin/bash root@864a1bdcf178:/# exit # 不进入容器执行命令，查看/bin目录下文件数量 [root@demo ~]# docker exec -it test_exec ls -al /bin | wc -l 72 退出容器 exit（等价于Ctrl+D） 退出并关闭容器(适用于docker run命令启动的容器, docker exec 进入容器exit退出后不影响容器状态)\n一般需要后台运行的容器可以使用-d先后台启动，需要交互时exec进入容器进行交互。\n容器命令高级操作 docker单独启动容器作为守护进程(后台运行), 启动后docker ps -a会发现已经退出了\n原因是：docker容器运行机制决定,docker容器后台运行就必须要有一个前台进程,否则会自动退出\n所以要解决这个问题就是将要运行的进程以前台进程的形式运行（或者交互模式启动 -itd）\n# 启动容器作为守护进程,这样会直接退出 docker run -d 镜像名 # 后台运行并每两秒在前台输出一次hello docker run -d centos /bin/sh -c \u0026#34;while true;do echo hello;sleep 2;done\u0026#34; # 查看日志, 列出时间, 动态打印日志, 保留之前num行 docker logs -f -t --tail num 容器ID # 实例 # 先删除所有容器 [root@demo ~]# docker rm $(docker ps -qa) -f f0b8817bb234 864a1bdcf178 # 后台启动一个centos，发现启动后会直接退出 [root@demo ~]# docker run -d centos acfd747d7ec4a942374bb526d41d072fe4840e3ba3d1255e67bdee5c2399513a [root@demo ~]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES acfd747d7ec4 centos \u0026#34;/bin/bash\u0026#34; 6 seconds ago Exited (0) 6 seconds ago dreamy_wilbur # 加入前台进程的在运行 [root@demo ~]# docker run -d centos /bin/sh -c \u0026#34;while true;do echo hello;sleep 2;done\u0026#34; 000da5e0a32581d4c65cb6a64292010f86feceac8ebea3f715f2d972fa7c7065 [root@demo ~]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 000da5e0a325 centos \u0026#34;/bin/sh -c \u0026#39;while t…\u0026#34; 3 seconds ago Up 2 seconds xenodochial_shamir acfd747d7ec4 centos \u0026#34;/bin/bash\u0026#34; 52 seconds ago Exited (0) 52 seconds ago dreamy_wilbur # 每两秒打印一次 [root@demo ~]# docker logs 000da5e0a325 hello hello hello hello ... # 查看容器内运行的进程 docker top 容器ID [root@demo ~]# docker top 000da5e0a325 UID PID PPID C STIME TTY TIME CMD root 10408 10388 0 09:33 ? 00:00:00 /bin/sh -c while true;do echo hello;sleep 2;done root 10549 10408 0 09:35 ? 00:00:00 /usr/bin/coreutils --coreutils-prog-shebang=sleep /usr/bin/sleep 2 # 容器内传输数据到宿主机 docker cp 容器ID:/path /宿主机path # 进入一个容器，在根目录下创建文件并拷贝到宿主机根目录 [root@demo ~]# docker exec -it 000da5e0a325 /bin/bash [root@000da5e0a325 /]# echo test \u0026gt;\u0026gt; test.txt [root@000da5e0a325 /]# cat test.txt test [root@000da5e0a325 /]# exit exit [root@demo ~]# docker cp 000da5e0a325:/test.txt / [root@demo ~]# cat /test.txt test 镜像的定制 # 如果该容器内部做了更改，提交打包后更改也包含进去，以此完成镜像的定制 docker commit -a=\u0026#34;作者名\u0026#34; -m=\u0026#34;提交信息\u0026#34; 容器ID 定制后的镜像名 # 启动一个容器，自定义端口映射，基于nginx:1.18.0镜像 [root@demo ~]# docker run -d -p 8080:80 --name nginx1.18.0 nginx:1.18.0 02c275a9254c7714d8187dc35efabf8859b245272b1ef101c4411b68aa85d9c3 [root@demo ~]# docker ps --no-trunc CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 02c275a9254c7714d8187dc35efabf8859b245272b1ef101c4411b68aa85d9c3 nginx:1.18.0 \u0026#34;/docker-entrypoint.sh nginx -g \u0026#39;daemon off;\u0026#39;\u0026#34; 29 seconds ago Up 28 seconds 0.0.0.0:8080-\u0026gt;80/tcp, :::8080-\u0026gt;80/tcp nginx1.18.0 # 定制新镜像并打上tag为1.18.0 [root@demo ~]# docker commit -a=\u0026#34;deemoprobe\u0026#34; -m=\u0026#34;nginx:1.18.0 8080-\u0026gt;80\u0026#34; 02c275a9254c7714d8187dc35efabf8859b245272b1ef101c4411b68aa85d9c3 deemoprobe/nginx:1.18.0 sha256:00b7979f0210c4ebde226fb86d789a4caef7276b2d92304b3942ef34ce733a96 # 查看新的镜像已生成 [root@demo ~]# docker images | grep deemoprobe deemoprobe/nginx 1.18.0 00b7979f0210 28 seconds ago 133MB # 以提交到docker hub为例 # 首先要创建docker hub账户，然后建立一个新仓库 # 登陆docker hub [root@demo ~]# docker login ... Login Succeeded # 推送 [root@demo ~]# docker push deemoprobe/nginx:1.18.0 The push refers to repository [docker.io/deemoprobe/nginx] 4fa6704c8474: Mounted from library/nginx 4fe7d87c8e14: Mounted from library/nginx 6fcbf7acaafd: Mounted from library/nginx f3fdf88f1cb7: Mounted from library/nginx 7e718b9c0c8c: Mounted from library/nginx 1.18.0: digest: sha256:2db445abcd9b126654035448cada7817300d646a27380916a6b6445e8ede699b size: 1362 # docker hub上就能查看到nginx镜像仓库，并且标签为1.18.0 # 拉下来查看 [root@demo ~]# docker pull deemoprobe/nginx:1.18.0 1.18.0: Pulling from deemoprobe/nginx f7ec5a41d630: Already exists 0b20d28b5eb3: Already exists 1576642c9776: Already exists c12a848bad84: Already exists 03f221d9cf00: Already exists Digest: sha256:2db445abcd9b126654035448cada7817300d646a27380916a6b6445e8ede699b Status: Downloaded newer image for deemoprobe/nginx:1.18.0 docker.io/deemoprobe/nginx:1.18.0 [root@demo ~]# docker images deemoprobe/nginx:1.18.0 REPOSITORY TAG IMAGE ID CREATED SIZE deemoprobe/nginx 1.18.0 b5fd6cb4ca9e 20 minutes ago 133MB # 或者说直接将拉取的镜像保存在自己的仓库里，可以直接打标签上传 # 比如拷贝cnych/ingress-nginx-defaultbackend镜像并上传至自己的DockerHub # 前提是已经登陆成功 [root@k8s-node01 ~]# docker images | grep 1.5 cnych/ingress-nginx-defaultbackend 1.5 b5af743e5984 3 years ago 5.13MB [root@k8s-node01 ~]# docker tag cnych/ingress-nginx-defaultbackend:1.5 deemoprobe/defaultbackend:1.5 [root@k8s-node01 ~]# docker push deemoprobe/defaultbackend:1.5 The push refers to repository [docker.io/deemoprobe/defaultbackend] b108d4968233: Mounted from cnych/ingress-nginx-defaultbackend 1.5: digest: sha256:4dc5e07c8ca4e23bddb3153737d7b8c556e5fb2f29c4558b7cd6e6df99c512c7 size: 528 [root@k8s-node01 ~]# docker images | grep 1.5 cnych/ingress-nginx-defaultbackend 1.5 b5af743e5984 3 years ago 5.13MB deemoprobe/defaultbackend 1.5 b5af743e5984 3 years ago 5.13MB 高级命令 docker run进阶 # 临时运行容器，用完即删不产生容器docker ps记录 # 临时开启容器查看镜像的WORKDIR docker run -it --rm IMAGE pwd # 临时开启容器进入查看镜像内容 docker run -it --rm IMAGE /bin/sh 查看docker配置信息 # 查看容器详情信息的某个字段 docker inspect -f \u0026#34;{{ .首字段.子字段 }}\u0026#34; \u0026lt;ContainerNameOrId\u0026gt; # 查看容器IP地址 [root@demo ~]# docker inspect -f \u0026#34;{{ .NetworkSettings.IPAddress }}\u0026#34; 38798985efb9 172.17.0.2 # 查看容器主机名 [root@demo ~]# docker inspect -f \u0026#34;{{ .Config.Hostname }}\u0026#34; 38798985efb9 38798985efb9 # 查看开放的端口 [root@demo ~]# docker inspect -f \u0026#34;{{ .Config.ExposedPorts }}\u0026#34; 38798985efb9 map[80/tcp:{}] 查看网络 # 启动并开放nginx80端口，80端口映射到主机的1234端口 [root@demo ~]# docker run -p 1234:80 -d nginx 03694540d34be5f69d951f15316dbdeae63fdc60a09e1da078273d5e15cb74ff [root@demo ~]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 03694540d34b nginx \u0026#34;/docker-entrypoint.…\u0026#34; 4 seconds ago Up 2 seconds 0.0.0.0:1234-\u0026gt;80/tcp nifty_williams # 查看端口映射关系 [root@demo ~]# docker port 03694540d34b 80 0.0.0.0:1234 # 查看nat规则 [root@demo ~]# iptables -t nat -nL ... Chain DOCKER (2 references) target prot opt source destination RETURN all -- 0.0.0.0/0 0.0.0.0/0 DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:1234 to:172.17.0.3:80 ... # 若容器内部访问不了外网，检查ip_forward和SNAT/MASQUERADE # 开启ip_forward [root@demo ~]# sysctl net.ipv4.ip_forward=1 net.ipv4.ip_forward = 1 # 查看SNAT/MASQUERADE是否是ACCEPT [root@demo ~]# iptables -t nat -nL Chain POSTROUTING (policy ACCEPT) target prot opt source destination # 这条规则指定从容器内出来的包都要进行一次地址转换 MASQUERADE all -- 172.17.0.0/16 0.0.0.0/0 ... ","permalink":"https://deemoprobe.github.io/posts/tech/docker-command/","summary":"本文整理了docker常用的一些命令。包括镜像命令，容器命令，日志查看，容器的高级操作以及从容器传输文件到宿主机。 常用 # 查看docker版本 docker version # 查看docker系统信息 docker info # 实时监控容器的运行情况 docker stats # 查看容器或镜像的底层信息 docker inspect ID/NAME # 查看容器中进程情况 docker top ID/NAME # 查看容器中进程的日","title":"Docker Command"},{"content":"环境说明 操作系统：CentOS Linux release 7.9.2009 (Core) 操作用户：root 内核版本：5.18.12-1 涉及技术：Linux、Docker、Vm 容器技术的基石 Linux内核功能中，Namespaces和Cgroups技术是实现轻量级进程虚拟化的基础，是Linux Container的基石，这两个Linux子系统主要作用是管理和隔离进程资源。\nNamespaces作用：封装抽象成多种namespace，用于限制和隔离进程间通信、进程号、网络以及挂载点等资源。更多Namespaces内容可见博客：Linux Namespaces Cgroups作用：管理资源的分配、限制资源的使用量。更多Cgroups内容可见博客：Linux Cgroups 轻量级进程虚拟化：经过虚拟化的进程看起来和Linux系统中其他进程一样，都是以进程的形式运行在主机中。操作系统可以承载大量的这类轻量级进程，这些轻量级进程共用一个Linux内核。\n容器和虚拟机 容器技术溯源可以追溯到chroot这个Unix/Linux命令，它可以创造出一个与文件系统隔离的环境，这种环境叫做chroot jail，这种环境真实存在，但又不会被外部的进程访问，起到了访问隔离的作用。\n容器技术、虚拟化技术（不论何种抽象层次下的虚拟化技术）都能做到资源层面上的隔离和限制。\nHypervisor是创建和运行虚拟机的管理程序，有两种类型，一种是直接在裸服务器硬件上工作；另一种是在操作系统之上工作。传统虚拟机通常采用这种技术进行虚拟化。\n如今，虚拟机和容器都能带来很好的隔离效果，相对来说虚拟机会带来一些开销，无论是启动时间、大小还是运行操作系统的资源使用。容器实际上是进程，启动速度更快，占用空间更小。如果需要更为彻底的隔离，虚拟机不失为一种选择。综合考虑开销、部署响应速度和资源利用率，容器技术更适合云原生架构。\n主要对比如下表：\n对比项 容器 虚拟机 开机时间 秒级 分钟级 运行机制 container-runtime hypervisor 内存使用 占用很小 占用较大 隔离强度 较弱 很强 部署时长 很短 较长 使用 较为复杂 简易 相比于系统级虚拟化，容器技术是进程级别的，具有启动快、体积小等优势，为软件开发、应用部署带来了诸多便利。如使用开源容器Docker技术，应用程序打包推送到镜像中心后，使用时拉取直接运行，实现了“一次打包，到处运行”，非常方便、快捷；使用开源容器编排技术K8S能够实现应用程序的弹性扩容和自动化部署，满足企业灵活扩展信息系统的需求。但是，随着Docker和K8S应用的日益广泛和深入，安全问题也越来越凸显。\nDocker容器技术应用中可能存在的技术性安全风险分为镜像安全风险、容器虚拟化安全风险、网络安全风险等类型。\nDocker Hub中的镜像可由个人开发者上传，其数量丰富、版本多样，但质量参差不齐，甚至存在包含恶意漏洞的恶意镜像，因而可能存在较大的安全风险。具体而言，Docker镜像的安全风险分布在创建过程、获取来源、获取途径等方方面面。\n与传统虚拟机相比，Docker容器不拥有独立的资源配置，且没有做到操作系统内核层面的隔离，因此可能存在资源隔离不彻底与资源限制不到位所导致的容器虚拟化安全风险。\n网络安全风险是互联网中所有信息系统所面临的重要风险，不论是物理设备还是虚拟机，都存在难以完全规避的网络安全风险问题。而在轻量级虚拟化的容器网络和容器编排环境中，其网络安全风险较传统网络而言更为复杂严峻。\n上面主要是Docker容器技术面临的安全风险，当然如今容器运行时已经不止Docker一种（诸如：containerd、CRI-O等），但面临的安全风险是同样的，都需要引起同样的关注和安全风险的评估。\n镜像 镜像是一种轻量级、独立可执行的软件包，用来打包软件运行环境和基于该环境开发的软件, 包括代码、运行时、库、环境变量和配置文件等。\n镜像的特点 Docker镜像都是只读的，当容器启动时，一个新的可写层被加载到镜像的顶部。这一层通常被称作为\u0026quot;容器层\u0026quot;，“容器层”之下的都叫\u0026quot;镜像层\u0026quot;。\nUnionFS(联合文件系统) UnionFS(联合文件系统): Union文件系统(UnionFS)是一种分层、轻量级并且高性能的文件系统，它支持对文件系统的修改作为一次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下(unite several directories into a single virtual filesystem)Union文件系统是Docker镜像的基础。镜像可以通过分层来进行继承, 基于基础镜像(没有父镜像)， 可以制作各种具体的应用镜像。\n特性: 一次同时加载多个文件系统，但从外面看起来，只能看到一个文件系统，联合加载会把各层文件系统叠加起来，这样最终的文件系统会包含所有底层的文件和目录。\nDocker镜像加载原理 docker的镜像实际上由一层一层的文件系统组成，这种层级的文件系统UnionFS。\nbootfs(boot file system),主要包含bootloader和kernel，bootloader主要是引导加载kernel，Linux刚启动时会加载bootfs文件系统，在Docker镜像的最底层是bootfs。这一层与我们典型的Linux/Unix系统是一样的, 包含boot加载器和内核。当boot加载完成之后整个内核就都在内存中了，此时内存的使用权已由bootfs转交给内核，此时系统也会卸载bootfs。\nrootfs(root file system),在bootfs之上。包含的就是典型Linux系统中的/dev, /proc, /bin, /etc等标准目录和文件。rootfs就是各种不同的操作系统发行版，比如Ubuntu，Centos等等。\n虚拟机的CentOS一般是几个G，docker这里231M\n对于一个精简的OS，rootfs可以很小，只需要包括最基本的命令、工具和程序库就可以了，因为底层直接用Host的kernel，自己只需要提供rootfs就行了。由此可见对于不同的linux发行版，bootfs基本是一致的，rootfs会有差别，因此不同的发行版可以共用bootfs。\n分层的镜像 以docker pull为例，在下载的过程中可以看到docker的镜像是在一层一层的在下载\nDocker镜像采用分层结构最大的一个好处就是共享资源。比如：有多个镜像都从相同的base镜像构建而来，那么宿主机只需在磁盘上保存一份base镜像,同时内存中也只需加载一份base镜像，就可以为所有容器服务了。而且镜像的每一层都可以被共享。\n镜像大小优化 RUN、COPY 和 ADD 指令会在已有镜像层的基础上创建一个新的镜像层，执行指令产生的所有文件系统变更会在指令结束后作为一个镜像层整体提交。 镜像层具有 copy-on-write 的特性，如果去更新其他镜像层中已存在的文件，会先将其复制到新的镜像层中再修改，造成双倍的文件空间占用。 如果去删除其他镜像层的一个文件，只会在当前镜像层生成一个该文件的删除标记，并不会减少整个镜像的实际体积。\nDocker部署 官方参考文档 个人博客文档\n数据卷 Docker 镜像是由多个文件系统（只读层）叠加而成。当我们启动一个容器的时候，Docker 会加载只读镜像层并在其上（镜像栈顶部）添加一个读写层。如果运行中的容器修改了现有的一个已经存在的文件，那该文件将会从读写层下面的只读层复制到读写层，该文件的只读版本仍然存在，只是已经被读写层中该文件的副本所隐藏。当删除Docker容器，并通过该镜像重新启动时，之前的更改将会丢失。\n为了能够保存（持久化）数据以及共享容器间的数据，Docker提出了Volume的概念。简单来说，数据卷是存在于一个或多个容器中的特定文件或文件夹，它可以绕过默认的联合文件系统，以正常的文件或者目录的形式存在于宿主机上。其生存周期独立于容器的生存周期。\nDocker提供了三种方式将数据从宿主机挂载到容器中：\nvolumes: Docker管理宿主机文件系统的一部分，默认位于 var/lib/docker/volumes 目录中，这是最常用的方式。 bind mounts: 可以存储在宿主机系统的任意位置，但在目录结构不同的操作系统之间不可移植。 tmpfs: 挂载存储在宿主机系统的内存中，而不会写入宿主机的文件系统。 docker volume [root@demo ~]# docker volume --help Usage: docker volume COMMAND Manage volumes Commands: create Create a volume inspect Display detailed information on one or more volumes ls List volumes prune Remove all unused local volumes rm Remove one or more volumes # 创建volume [root@demo ~]# docker volume create new_volume new_volume [root@demo ~]# docker volume ls DRIVER VOLUME NAME local new_volume [root@demo ~]# docker volume inspect new_volume [ { \u0026#34;CreatedAt\u0026#34;: \u0026#34;2022-01-10T07:43:43+08:00\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Labels\u0026#34;: {}, \u0026#34;Mountpoint\u0026#34;: \u0026#34;/var/lib/docker/volumes/new_volume/_data\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;new_volume\u0026#34;, \u0026#34;Options\u0026#34;: {}, \u0026#34;Scope\u0026#34;: \u0026#34;local\u0026#34; } ] # 在宿主机可以找到对应目录 [root@demo ~]# ls -al /var/lib/docker/volumes/ .. drwx-----x. 3 root root 19 Jan 10 07:43 new_volume 命令行中可以用-v使用数据卷 -v/--volume，由（:）分隔的三个字段组成，卷名:容器路径:选项。选项可以ro/rw。 --mount，由多个键值对组成，由,分隔，每个由一个key=value元组组成。 type，值可以为 bind，volume，tmpfs。 source，对于命名卷，是卷名。对于匿名卷，这个字段被省略。可能被指定为 source 或 src。 destination，文件或目录将被挂载到容器中的路径。可以指定为 destination，dst 或 target。 volume-opt 可以多次指定。 # 挂载数据卷new_volume到容器的/volume目录，创建文件并查看同步效果 # 下面命令等效于 docker run -itd --name mountvol --mount source=new_volume,target=/volume nginx [root@demo ~]# docker run -itd --name mountvol -v new_volume:/volume nginx c3450a454f209f30987f60863547c7a5a60d58fffa19faf89c08d0840cb6e1ac [root@demo ~]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c3450a454f20 nginx \u0026#34;/docker-entrypoint.…\u0026#34; 22 seconds ago Up 20 seconds 80/tcp mountvol [root@demo ~]# docker exec -it c3450a454f20 /bin/bash root@c3450a454f20:/# cd /volume/ root@c3450a454f20:/volume# echo volume \u0026gt; testfile root@c3450a454f20:/volume# ls testfile root@c3450a454f20:/volume# exit exit [root@demo ~]# cat /var/lib/docker/volumes/new_volume/_data/testfile volume # 默认数据卷在容器内挂载内容具备读写（rw）权限，指定只读 [root@demo ~]# docker run -itd --name mountvol2 -v new_volume:/volume:ro nginx 440ca257ed6de9b4387dc83d85b67e071ebafdb27ce5b5b2e4faa970c21cbd97 [root@demo _data]# docker exec -it 440ca257ed6d /bin/bash root@440ca257ed6d:/# cd /volume/ root@440ca257ed6d:/volume# touch file touch: cannot touch \u0026#39;file\u0026#39;: Read-only file system # 清理容器和数据卷 [root@demo _data]# docker volume rm new_volume Error response from daemon: remove new_volume: volume is in use - [c3450a454f209f30987f60863547c7a5a60d58fffa19faf89c08d0840cb6e1ac] [root@demo _data]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c3450a454f20 nginx \u0026#34;/docker-entrypoint.…\u0026#34; 16 minutes ago Up 16 minutes 80/tcp mountvol [root@demo _data]# docker stop c3450a454f20 c3450a454f20 [root@demo _data]# docker rm c3450a454f20 c3450a454f20 [root@demo _data]# docker volume rm new_volume new_volume [root@demo _data]# docker volume ls DRIVER VOLUME NAME # 清除未使用的数据卷 docker volume prune 使用主机目录 # 将主机任意目录挂载到容器作为数据卷，-v参数，如果宿主机没有相关目录，会自动创建 [root@demo ~]# docker run -itd --name web -v /webapp:/usr/share/nginx/html nginx 46b47e81ee8e53687b78aa109389f75135cdb70005934f1e4d49a992e47eb3ff [root@demo ~]# docker inspect web | grep -e Mounts -A 9 \u0026#34;Mounts\u0026#34;: [ { \u0026#34;Type\u0026#34;: \u0026#34;bind\u0026#34;, \u0026#34;Source\u0026#34;: \u0026#34;/webapp\u0026#34;, \u0026#34;Destination\u0026#34;: \u0026#34;/usr/share/nginx/html\u0026#34;, \u0026#34;Mode\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;RW\u0026#34;: true, \u0026#34;Propagation\u0026#34;: \u0026#34;rprivate\u0026#34; } ], # --mount，宿主机目录不存在会报错 [root@demo ~]# docker run -itd --name web2 --mount type=bind,source=/app/webapp,target=/usr/share/nginx/html,readonly nginx docker: Error response from daemon: invalid mount config for type \u0026#34;bind\u0026#34;: bind source path does not exist: /app/webapp. See \u0026#39;docker run --help\u0026#39;. [root@demo ~]# mkdir -p /app/webapp [root@demo ~]# docker run -itd --name web2 --mount type=bind,source=/app/webapp,target=/usr/share/nginx/html,readonly nginx 2f0f5e1a42bf63995fbe3918842bd541dd173d1b25dfe458e1043591b9d42ef8 网络参考文件：国家保密局-开源容器技术安全分析\n","permalink":"https://deemoprobe.github.io/posts/tech/docker/","summary":"环境说明 操作系统：CentOS Linux release 7.9.2009 (Core) 操作用户：root 内核版本：5.18.12-1 涉及技术：Linux、Docker、Vm 容器技术的基石 Linux内核功能中，Namespaces和Cgroups技术是实现轻量级进程虚拟化的基础，是Linux Container的基石，这两个Lin","title":"Docker"},{"content":" 备用下载链接是本人存在阿里云OSS的安装包，不能保证版本是最新的，仅供交流使用，请支持正版，若有侵权请联系删除。\nWindows10平台 Super F4: 强制杀前台进程，可有效解决进程页面卡死问题，快捷键Ctrl+Alt+F4\n备用下载链接\nQTTabbar: 文件资源管理器中使用Tab标签功能，安装后此电脑(或文件资源管理器)-\u0026gt;查看(最上面)-\u0026gt;点击选项两个字-\u0026gt;QTTabbar-\u0026gt;启用成功\n备用下载链接\nCaptura: 录屏软件\n备用下载链接\nBitwarden: 全平台密码管理工具，在Google浏览器可下载对应插件，不过不建议存放敏感的密码（如银行卡密码等），本人相信只要是联网的密码管理工具安全性都不可能到100%，但管理网页登陆口令是比较方便的，可自动填充。\n备用下载链接\nKeePass: 开源密码管理器，密码数据库存放在本地，不联网即可管理，数据库文件使用当前已知的最佳和最安全的加密算法（AES-256、ChaCha20 和 Twofish）进行加密，安全性比较高。缺点是不如Bitwarden方便且不支持多平台，没有浏览器插件。\n备用下载链接\nrufus: 启动盘制作工具\n备用下载链接\nEverything: 全局资源搜索软件\n备用下载链接\nHWiNFO: 优秀的硬件信息搜集软件\n备用下载链接\nBrave浏览器: 保护隐私的浏览器\n备用下载链接\nFastStonecapture: 非常好用的滚动截图工具，当然常规截图功能都有。\n备用下载链接\nGPU-Z: 显卡检测工具\n备用下载链接\nScreenToGif: 截图生成GIF格式图片\n备用下载链接\nShadowsocks-4.1.9.2: 程序员必备的知识工具\nSecureCRT+FX: 好用的SSH工具\n备用下载链接\nTranslucentTB: Win10任务栏透明化小工具\n备用下载链接\nWox: 超级好用的快速启动器，搭配Everything使用更是如虎添翼，还可以添加很多插件。\n备用下载链接\nPscp: Windows和linux之间传输文件的小工具，使用方法见我的blogpscp使用说明\n备用下载链接\nRenamer: 强大的文件批量重命名工具\n备用下载链接\nVLC: 媒体播放器，多种解码。\n备用下载链接\n小孩桌面便签: 简约实用轻量的桌面便签,有很多便签样式可以选择,目前支持Windows/Android/IOS三端云同步.\n备用下载链接\nSnipaste: 开源轻量级截图贴图软件,可以看作QQ截图的升级版\n备用下载链接\nWGestures: 全局鼠标手势软件,可以设定多种鼠标快捷操作(如复制粘贴等)\n备用下载链接\nDism++: Windows系统优化神器\n备用下载链接\nLinux平台 Vim8.2\nPython3.7.7\nApache-tomcat-8.5.53\nHelm-v3.7.2\nJdk-8u271-linux-x64\nansible-collection-ansible-posix-1.2.0-1.el7.noarch.rpm\n移动终端 暂无\n其他 暂无\n","permalink":"https://deemoprobe.github.io/tools/","summary":"备用下载链接是本人存在阿里云OSS的安装包，不能保证版本是最新的，仅供交流使用，请支持正版，若有侵权请联系删除。 Windows10平台 Super F4: 强制杀前台进程，可有效解决进程页面卡死问题，快捷键Ctrl+Alt+F4 备用下载链接 QTTabbar: 文件资源管理器中使用Tab标签功能，安装后此电脑(或文件","title":"Tools"},{"content":" ","permalink":"https://deemoprobe.github.io/posts/life/4k-wallpaper/","summary":"","title":"4k Wallpaper"},{"content":"页面测试 由于访问速度问题，没有域名也无法使用CDN加速，所以，去掉了很多花里胡哨的功能\n","permalink":"https://deemoprobe.github.io/posts/life/life/","summary":"页面测试 由于访问速度问题，没有域名也无法使用CDN加速，所以，去掉了很多花里胡哨的功能","title":"Life"},{"content":"页面测试 由于访问速度问题，没有域名也无法使用CDN加速，所以，去掉了很多花里胡哨的功能\n","permalink":"https://deemoprobe.github.io/posts/read/read/","summary":"页面测试 由于访问速度问题，没有域名也无法使用CDN加速，所以，去掉了很多花里胡哨的功能","title":"Read"},{"content":"环境说明 宿主机系统：Windows 10 虚拟机版本：VMware® Workstation 16 Pro IOS镜像版本：CentOS Linux release 7.9.2009 Kubernetes版本：1.26.4 Runtime：Containerd v1.6.20 Etcd版本：3.5.6 集群操作用户：root 更新时间：2023-04-20 CentOS7安装请参考博客文章：LINUX之VMWARE WORKSTATION安装CENTOS-7\n资源分配 网段划分\nKubernetes集群需要规划三个网段：\n宿主机网段：Kubernetes集群节点的网段 Pod网段：集群内Pod的网段，相当于容器的IP Service网段：集群内服务发现使用的网段，service用于集群容器通信 生产环境根据申请到的IP资源进行分配即可，原则是三个网段不允许有重合IP。IP网段计算可以参考：在线IP地址计算。本文虚拟机练习环境IP地址网段分配如下：\n宿主机网段：192.168.43.1/24 Pod网段：172.16.0.0/12 Service：10.96.0.0/12 节点分配\n采用3管理节点2工作节点的高可用Kubernetes集群模式：\nk8s-master01/k8s-master02/k8s-master03 集群的Master节点 三个master节点同时做etcd集群 k8s-node01/k8s-node02 集群的Node节点 k8s-master-vip做高可用k8s-master01~03的VIP，不占用物理资源 主机节点名称 IP CPU核心数 内存大小 磁盘大小 k8s-master-vip 192.168.43.200 / / / k8s-master01 192.168.43.201 2 2G 40G k8s-master02 192.168.43.202 2 2G 40G k8s-master03 192.168.43.203 2 2G 40G k8s-node01 192.168.43.204 2 2G 40G k8s-node02 192.168.43.205 2 2G 40G 操作步骤 标题后小括号注释表明操作范围：\nALL 所有节点（k8s-master01/k8s-master02/k8s-master03/k8s-node01/k9s-node02）执行 Master 只需要在master节点（k8s-master01/k8s-master02/k8s-master03）执行 Node 只需要在node节点（k8s-node01/k8s-node02）执行 已标注的个别命令只需要在某一台机器执行，会在操作前说明 未标注的会在操作时说明 使用cat \u0026lt;\u0026lt; \u0026quot;EOF\u0026quot; \u0026gt;\u0026gt; file或cat \u0026gt;\u0026gt; file \u0026lt;\u0026lt; \u0026quot;EOF\u0026quot;添加文件内容注意cat后面的EOF一定要加上双引号（标准输入的），否则不会保留输入时的缩进格式而且会直接解析输入时的变量，进而造成文件可读性差甚至不可用；同时注意文件的\u0026gt;重写与\u0026gt;\u0026gt;追加。虽然单独转义输入时的变量也能避免变量被解析，但是不推荐，漏转义会造成不必要的麻烦。\n准备工作(ALL) 添加主机信息、关闭防火墙、关闭swap、关闭SELinux、dnsmasq、NetworkManager\n# 添加主机信息 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt;\u0026gt; /etc/hosts 192.168.43.200 k8s-master-vip 192.168.43.201 k8s-master01 192.168.43.202 k8s-master02 192.168.43.203 k8s-master03 192.168.43.204 k8s-node01 192.168.43.205 k8s-node02 EOF # 关闭防火墙、dnsmasq、NetworkManager，--now参数表示关闭服务并移除开机自启 # 这些服务是否可以关闭视情况而定，本文是虚拟机实践，没有用到这些服务 systemctl disable --now firewalld systemctl disable --now dnsmasq systemctl disable --now NetworkManager # 关闭swap，并注释fstab文件swap所在行 swapoff -a sed -i \u0026#39;/swap/s/^\\(.*\\)$/#\\1/g\u0026#39; /etc/fstab # 关闭SELinux，并更改selinux配置文件 setenforce 0 sed -i \u0026#34;s/=enforcing/=disabled/g\u0026#34; /etc/selinux/config 值得注意的是/etc/sysconfig/selinux文件是/etc/selinux/config文件的软连接，用sed -i命令修改软连接文件会破坏软连接属性，将/etc/sysconfig/selinux变为一个独立的文件，即使该文件被修改了，但源文件/etc/selinux/config配置是没变的。此外，使用vim等编辑器编辑源文件或链接文件（编辑模式不会修改文件属性）修改也可以。软链接原理可参考博客：LINUX之INODE详解\n# 默认的yum源太慢，更新为阿里源，同时用sed命令删除文件中不需要的两个URL的行 curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo sed -i -e \u0026#39;/mirrors.cloud.aliyuncs.com/d\u0026#39; -e \u0026#39;/mirrors.aliyuncs.com/d\u0026#39; /etc/yum.repos.d/CentOS-Base.repo # 安装常用工具包 yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git -y # 配置ntpdate，同步服务器时间 rpm -ivh http://mirrors.wlnmp.com/centos/wlnmp-release-centos.noarch.rpm yum install ntpdate -y # 同步时区和时间 ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime echo \u0026#39;Asia/Shanghai\u0026#39; \u0026gt;/etc/timezone ntpdate time2.aliyun.com # 可以加入计划任务，保证集群时钟是一致的 # /var/spool/cron/root文件也是crontab -e写入的文件 # crontab执行日志查看可用：tail -f /var/log/cron cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt;\u0026gt; /var/spool/cron/root */5 * * * * /usr/sbin/ntpdate time2.aliyun.com EOF # 须知：如果设置了定时任务，会经常收到提示“You have new mail in /var/spool/mail/root” # （可选）禁用提示：echo \u0026#34;unset MAILCHECK\u0026#34; \u0026gt;\u0026gt; ~/.bashrc;source ~/.bashrc # 禁用提示后/var/spool/mail/root文件依旧会记录root操作日志，可随时查看 # 保证文件句柄不会限制集群的可持续发展，配置limits ulimit -SHn 65500 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt;\u0026gt; /etc/security/limits.conf * soft nofile 65500 * hard nofile 65500 * soft nproc 65500 * hard nproc 65500 * soft memlock unlimited * hard memlock unlimited EOF # 配置免密登录，k8s-master01到其他节点 # 生成密钥对（在k8s-master01节点配置即可） ssh-keygen -t rsa # 拷贝公钥到其他节点，首次需要认证一下各个节点的root密码，以后就可以免密ssh到其他节点 for i in k8s-master02 k8s-master03 k8s-node01 k8s-node02;do ssh-copy-id -i .ssh/id_rsa.pub $i;done # 克隆二进制仓库1.26分支的文件（k8s-master01上操作即可） cd /root;git clone https://gitee.com/deemoprobe/k8s-ha-install.git -b manual-installation-v1.26.x # 所有节点系统升级 yum update --exclude=kernel* -y 升级内核，4.17以下的内核cgroup存在内存泄漏的BUG，具体分析过程浏览器搜Kubernetes集群为什么要升级内核会有很多文章讲解\n内核备用下载（下载到本地后上传到服务器，尽量不要用wget）：\nkernel-ml-devel-4.19.12-1.el7.elrepo.x86_64.rpm kernel-ml-4.19.12-1.el7.elrepo.x86_64.rpm # 下载4.19版本内核，如果无法下载，可以用上面提供的备用下载 cd /root wget http://193.49.22.109/elrepo/kernel/el7/x86_64/RPMS/kernel-ml-devel-4.19.12-1.el7.elrepo.x86_64.rpm wget http://193.49.22.109/elrepo/kernel/el7/x86_64/RPMS/kernel-ml-4.19.12-1.el7.elrepo.x86_64.rpm # 可以在k8s-master01节点下载后，免密传到其他节点 for i in k8s-master02 k8s-master03 k8s-node01 k8s-node02;do scp kernel-ml-* $i:/root;done # 所有节点安装内核 cd /root \u0026amp;\u0026amp; yum localinstall -y kernel-ml* # 所有节点更改内核启动顺序 grub2-set-default 0 \u0026amp;\u0026amp; grub2-mkconfig -o /etc/grub2.cfg grubby --args=\u0026#34;user_namespace.enable=1\u0026#34; --update-kernel=\u0026#34;$(grubby --default-kernel)\u0026#34; # 查看默认内核，并重启节点 grubby --default-kernel reboot # 确认内核版本 uname -a # （可选）删除老版本的内核，避免以后被升级取代默认的开机4.19内核 rpm -qa | grep kernel yum remove -y kernel-3* # 升级系统软件包（如果跳过内核升级加参数 --exclude=kernel*） yum update -y # 安装IPVS相关工具，由于IPVS在资源消耗和性能上均已明显优于iptables，所以推荐开启 # 具体原因可参考官网介绍 https://kubernetes.io/zh/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/ yum install ipvsadm ipset sysstat conntrack libseccomp -y # 加载模块，最后一条4.18及以下内核使用nf_conntrack_ipv4，4.19已改为nf_conntrack modprobe -- ip_vs modprobe -- ip_vs_rr modprobe -- ip_vs_wrr modprobe -- ip_vs_sh modprobe -- nf_conntrack # 编写参数文件 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/modules-load.d/ipvs.conf ip_vs ip_vs_lc ip_vs_wlc ip_vs_rr ip_vs_wrr ip_vs_lblc ip_vs_lblcr ip_vs_dh ip_vs_sh ip_vs_fo ip_vs_nq ip_vs_sed ip_vs_ftp ip_vs_sh nf_conntrack ip_tables ip_set xt_set ipt_set ipt_rpfilter ipt_REJECT ipip EOF # systemd-modules-load加入开机自启 systemctl enable --now systemd-modules-load # 自定义内核参数优化配置文件 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/sysctl.d/kubernetes.conf net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 fs.may_detach_mounts = 1 net.ipv4.conf.all.route_localnet = 1 vm.overcommit_memory=1 vm.panic_on_oom=0 fs.inotify.max_user_watches=89100 fs.file-max=52706963 fs.nr_open=52706963 net.netfilter.nf_conntrack_max=2310720 net.ipv4.tcp_keepalive_time = 600 net.ipv4.tcp_keepalive_probes = 3 net.ipv4.tcp_keepalive_intvl =15 net.ipv4.tcp_max_tw_buckets = 36000 net.ipv4.tcp_tw_reuse = 1 net.ipv4.tcp_max_orphans = 327680 net.ipv4.tcp_orphan_retries = 3 net.ipv4.tcp_syncookies = 1 net.ipv4.tcp_max_syn_backlog = 16384 net.ipv4.ip_conntrack_max = 65536 net.ipv4.tcp_max_syn_backlog = 16384 net.ipv4.tcp_timestamps = 0 net.core.somaxconn = 16384 EOF # 加载 sysctl --system # 重启查看IPVS模块是否依旧加载 reboot lsmod | grep -e ip_vs -e nf_conntrack 保证每台服务器中IPVS加载成功，以k8s-master01为例，如图： 部署Containerd(ALL) Kubernetes1.24版本以后将不再支持Docker作为Runtime，本文安装使用Containerd作为Runtime。\n# 配置阿里docker源 yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo # 安装最新版本docker和containerd.io，安装docker是为了使用docker CLI yum install docker-ce docker-ce-cli containerd.io -y # （可选）也可以按需安装指定版本 yum install docker-ce-20.10.* docker-ce-cli-20.10.* containerd -y # 配置Containerd模块 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/modules-load.d/containerd.conf overlay br_netfilter EOF # 加载模块 modprobe -- overlay modprobe -- br_netfilter # 配置内核参数 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/sysctl.d/containerd.conf net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 EOF # 加载内核参数 sysctl --system # 生成默认配置文件 mkdir -p /etc/containerd containerd config default | tee /etc/containerd/config.toml # 更改Cgroup为Systemd，在containerd.runtimes.runc.options行后的SystemdCgroup = false修改为true，如果配置项不存在就自行添加，缩进俩空格添加SystemdCgroup = true一行 vim /etc/containerd/config.toml ... [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc.options] SystemdCgroup = true ... # 将sandbox_image的Pause镜像地址改成国内：registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.7 vim /etc/containerd/config.toml sandbox_image = \u0026#34;registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.7\u0026#34; # （可选）也可以在k8s-master01编辑/etc/containerd/config.toml文件后，将编辑后同名文件同步到其他服务器，自动覆盖 for i in k8s-master02 k8s-master03 k8s-node01 k8s-node02;do scp /etc/containerd/config.toml $i:/etc/containerd/config.toml;done # 查看确认是否配置成功 cat /etc/containerd/config.toml | grep -e Systemd -e sandbox_image # 启动并加入开机自启 systemctl daemon-reload systemctl enable --now containerd # containerd运行时的CLI是ctr [root@k8s-master01 ~]# ctr images ls REF TYPE DIGEST SIZE PLATFORMS LABELS [root@k8s-master01 ~]# ctr version Client: Version: 1.6.20 Revision: 2806fc1057397dbaeefbea0e4e17bddfbd388f38 Go version: go1.19.7 Server: Version: 1.6.20 Revision: 2806fc1057397dbaeefbea0e4e17bddfbd388f38 UUID: 9958928e-300c-4778-b83c-6c0073414f3e # 配置crictl连接的运行时socket接口（指向containerd\u0026#39;s GRPC server：/run/containerd/containerd.sock） # crictl 默认连接到unix:///var/run/dockershim.sock cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/crictl.yaml runtime-endpoint: unix:///run/containerd/containerd.sock image-endpoint: unix:///run/containerd/containerd.sock timeout: 10 debug: false EOF # crictl按需选择，https://github.com/kubernetes-sigs/cri-tools/releases # containerd容器管理CLI是crictl，该命令使用和docker命令类似，下载包上传到k8s-master01 [root@k8s-master01 ~]# tar -zxvf crictl-v1.27.0-linux-amd64.tar.gz [root@k8s-master01 ~]# mv crictl /usr/local/bin/ [root@k8s-master01 ~]# crictl version Version: 0.1.0 RuntimeName: containerd RuntimeVersion: 1.6.20 RuntimeApiVersion: v1 # 二进制可执行文件发送到其他节点 [root@k8s-master01 ~]# for i in k8s-master02 k8s-master03 k8s-node01 k8s-node02;do scp /usr/local/bin/crictl $i:/usr/local/bin/;done crictl 是CRI（Container Runtime Interface）兼容的容器运行时的CLI（Command-Line Interface）。可以这个命令来检查和调试 Kubernetes 节点上的容器运行时和应用程序。介绍和安装方式可见：critools或Kubernetes官方介绍\n二进制包和证书 k8s-master01节点上下载并安装Kubernetes二进制安装包（server-binaries，选择对应的架构即可）和ETCD二进制安装包，可在GitHub上查看Kubernetes1.23.x版本的信息，官方GitHub-Kubernetes1.23版本链接，ETCD官方链接\n# 以下在k8s-master01执行 # 下载太慢的话可以在相应链接网页上下载好传到服务器 wget https://dl.k8s.io/v1.26.4/kubernetes-server-linux-amd64.tar.gz wget https://github.com/etcd-io/etcd/releases/download/v3.5.6/etcd-v3.5.6-linux-amd64.tar.gz # 解压安装，--strip-components=N表示解压时忽略解压后的N层目录，直接获取N层目录后的目标文件。kubernetes/server/bin/是三层，etcd-v3.5.6-linux-amd64/是一层 tar -zxvf kubernetes-server-linux-amd64.tar.gz --strip-components=3 -C /usr/local/bin kubernetes/server/bin/kube{let,ctl,-apiserver,-controller-manager,-scheduler,-proxy} tar -zxvf etcd-v3.5.6-linux-amd64.tar.gz --strip-components=1 -C /usr/local/bin etcd-v3.5.6-linux-amd64/etcd{,ctl} # 确认版本 kubectl version kubelet --version etcdctl version # 拷贝组件到其他节点，Node节点只需要kubelet和kube-proxy for i in k8s-master02 k8s-master03; do scp /usr/local/bin/kube{let,ctl,-apiserver,-controller-manager,-scheduler,-proxy} $i:/usr/local/bin/; scp /usr/local/bin/etcd* $i:/usr/local/bin/; done for i in k8s-node01 k8s-node02; do scp /usr/local/bin/kube{let,-proxy} $i:/usr/local/bin/; done 配置证书 # master节点创建etcd证书目录 mkdir -p /etc/etcd/ssl # 所有节点创建pki证书目录和CNI目录（后面calico使用） mkdir -p /etc/kubernetes/pki mkdir -p /opt/cni/bin # 以下在k8s-master01操作 # 安装证书生成工具，如果速度慢可以浏览器下载后上传至服务器改名即可 wget \u0026#34;https://pkg.cfssl.org/R1.2/cfssl_linux-amd64\u0026#34; -O /usr/local/bin/cfssl wget \u0026#34;https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64\u0026#34; -O /usr/local/bin/cfssljson chmod +x /usr/local/bin/cfssl /usr/local/bin/cfssljson # 在master01节点生成etcd证书 cd /root/k8s-ha-install/pki cfssl gencert -initca etcd-ca-csr.json | cfssljson -bare /etc/etcd/ssl/etcd-ca cfssl gencert \\ -ca=/etc/etcd/ssl/etcd-ca.pem \\ -ca-key=/etc/etcd/ssl/etcd-ca-key.pem \\ -config=ca-config.json \\ -hostname=127.0.0.1,k8s-master01,k8s-master02,k8s-master03,192.168.43.201,192.168.43.202,192.168.43.203 \\ -profile=kubernetes \\ etcd-csr.json | cfssljson -bare /etc/etcd/ssl/etcd # 复制etcd证书到其他master节点 for i in k8s-master02 k8s-master03; do for FILE in etcd-ca-key.pem etcd-ca.pem etcd-key.pem etcd.pem; do scp /etc/etcd/ssl/${FILE} $i:/etc/etcd/ssl/${FILE} done done # 生成Kubernetes集群ca证书 cd /root/k8s-ha-install/pki cfssl gencert -initca ca-csr.json | cfssljson -bare /etc/kubernetes/pki/ca # k8s service网段10.96.0.0/12，填入网段第一个IP；集群VIP地址192.168.43.200 cfssl gencert -ca=/etc/kubernetes/pki/ca.pem -ca-key=/etc/kubernetes/pki/ca-key.pem -config=ca-config.json -hostname=10.96.0.1,192.168.43.200,127.0.0.1,kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.default.svc.cluster.local,192.168.43.201,192.168.43.202,192.168.43.203 -profile=kubernetes apiserver-csr.json | cfssljson -bare /etc/kubernetes/pki/apiserver # 生成apiserver的第三方组件使用的聚合证书，告警可以忽略 cfssl gencert -initca front-proxy-ca-csr.json | cfssljson -bare /etc/kubernetes/pki/front-proxy-ca cfssl gencert -ca=/etc/kubernetes/pki/front-proxy-ca.pem -ca-key=/etc/kubernetes/pki/front-proxy-ca-key.pem -config=ca-config.json -profile=kubernetes front-proxy-client-csr.json | cfssljson -bare /etc/kubernetes/pki/front-proxy-client # 生成controller-manager证书 cfssl gencert \\ -ca=/etc/kubernetes/pki/ca.pem \\ -ca-key=/etc/kubernetes/pki/ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ manager-csr.json | cfssljson -bare /etc/kubernetes/pki/controller-manager # 设置集群信息，集群名：kubernetes server：https://192.168.43.200:8443 # 如果不是高可用集群，192.168.43.200:8443改为master01的地址，8443改为apiserver的端口，默认是6443 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/pki/ca.pem \\ --embed-certs=true \\ --server=https://192.168.43.200:8443 \\ --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig # 设置上下文信息，context为system:kube-controller-manager@kubernetes kubectl config set-context system:kube-controller-manager@kubernetes \\ --cluster=kubernetes \\ --user=system:kube-controller-manager \\ --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig # 设置用户认证信息，用户为system:kube-controller-manager kubectl config set-credentials system:kube-controller-manager \\ --client-certificate=/etc/kubernetes/pki/controller-manager.pem \\ --client-key=/etc/kubernetes/pki/controller-manager-key.pem \\ --embed-certs=true \\ --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig # 设置默认集群环境 kubectl config use-context system:kube-controller-manager@kubernetes \\ --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig # 生成scheduler证书 cfssl gencert \\ -ca=/etc/kubernetes/pki/ca.pem \\ -ca-key=/etc/kubernetes/pki/ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ scheduler-csr.json | cfssljson -bare /etc/kubernetes/pki/scheduler # 同样的，scheduler需要和controller-manager设置相同的集群上下文等信息 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/pki/ca.pem \\ --embed-certs=true \\ --server=https://192.168.43.200:8443 \\ --kubeconfig=/etc/kubernetes/scheduler.kubeconfig kubectl config set-credentials system:kube-scheduler \\ --client-certificate=/etc/kubernetes/pki/scheduler.pem \\ --client-key=/etc/kubernetes/pki/scheduler-key.pem \\ --embed-certs=true \\ --kubeconfig=/etc/kubernetes/scheduler.kubeconfig kubectl config set-context system:kube-scheduler@kubernetes \\ --cluster=kubernetes \\ --user=system:kube-scheduler \\ --kubeconfig=/etc/kubernetes/scheduler.kubeconfig kubectl config use-context system:kube-scheduler@kubernetes \\ --kubeconfig=/etc/kubernetes/scheduler.kubeconfig # 配置admin证书，以及集群上下文等信息 cfssl gencert \\ -ca=/etc/kubernetes/pki/ca.pem \\ -ca-key=/etc/kubernetes/pki/ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ admin-csr.json | cfssljson -bare /etc/kubernetes/pki/admin kubectl config set-cluster kubernetes --certificate-authority=/etc/kubernetes/pki/ca.pem --embed-certs=true --server=https://192.168.43.200:8443 --kubeconfig=/etc/kubernetes/admin.kubeconfig kubectl config set-credentials kubernetes-admin --client-certificate=/etc/kubernetes/pki/admin.pem --client-key=/etc/kubernetes/pki/admin-key.pem --embed-certs=true --kubeconfig=/etc/kubernetes/admin.kubeconfig kubectl config set-context kubernetes-admin@kubernetes --cluster=kubernetes --user=kubernetes-admin --kubeconfig=/etc/kubernetes/admin.kubeconfig kubectl config use-context kubernetes-admin@kubernetes --kubeconfig=/etc/kubernetes/admin.kubeconfig # 创建ServiceAccount密钥对 openssl genrsa -out /etc/kubernetes/pki/sa.key 2048 openssl rsa -in /etc/kubernetes/pki/sa.key -pubout -out /etc/kubernetes/pki/sa.pub # 拷贝证书到其他master节点 for i in k8s-master02 k8s-master03; do for FILE in $(ls /etc/kubernetes/pki | grep -v etcd); do scp /etc/kubernetes/pki/${FILE} $i:/etc/kubernetes/pki/${FILE}; done; for FILE in admin.kubeconfig controller-manager.kubeconfig scheduler.kubeconfig; do scp /etc/kubernetes/${FILE} $i:/etc/kubernetes/${FILE}; done; done # 查看证书数量是否为23 ls /etc/kubernetes/pki/ | wc -l 23 ETCD集群(Master) 如果Etcd集群服务器和Kubernetes集群服务器不重合（即独立的Etcd集群），需要根据实际情况配置集群IP。\n# master01 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/etcd/etcd.config.yml name: \u0026#39;k8s-master01\u0026#39; data-dir: /var/lib/etcd wal-dir: /var/lib/etcd/wal snapshot-count: 5000 heartbeat-interval: 100 election-timeout: 1000 quota-backend-bytes: 0 listen-peer-urls: \u0026#39;https://192.168.43.201:2380\u0026#39; listen-client-urls: \u0026#39;https://192.168.43.201:2379,http://127.0.0.1:2379\u0026#39; max-snapshots: 3 max-wals: 5 cors: initial-advertise-peer-urls: \u0026#39;https://192.168.43.201:2380\u0026#39; advertise-client-urls: \u0026#39;https://192.168.43.201:2379\u0026#39; discovery: discovery-fallback: \u0026#39;proxy\u0026#39; discovery-proxy: discovery-srv: initial-cluster: \u0026#39;k8s-master01=https://192.168.43.201:2380,k8s-master02=https://192.168.43.202:2380,k8s-master03=https://192.168.43.203:2380\u0026#39; initial-cluster-token: \u0026#39;etcd-k8s-cluster\u0026#39; initial-cluster-state: \u0026#39;new\u0026#39; strict-reconfig-check: false enable-v2: true enable-pprof: true proxy: \u0026#39;off\u0026#39; proxy-failure-wait: 5000 proxy-refresh-interval: 30000 proxy-dial-timeout: 1000 proxy-write-timeout: 5000 proxy-read-timeout: 0 client-transport-security: cert-file: \u0026#39;/etc/kubernetes/pki/etcd/etcd.pem\u0026#39; key-file: \u0026#39;/etc/kubernetes/pki/etcd/etcd-key.pem\u0026#39; client-cert-auth: true trusted-ca-file: \u0026#39;/etc/kubernetes/pki/etcd/etcd-ca.pem\u0026#39; auto-tls: true peer-transport-security: cert-file: \u0026#39;/etc/kubernetes/pki/etcd/etcd.pem\u0026#39; key-file: \u0026#39;/etc/kubernetes/pki/etcd/etcd-key.pem\u0026#39; peer-client-cert-auth: true trusted-ca-file: \u0026#39;/etc/kubernetes/pki/etcd/etcd-ca.pem\u0026#39; auto-tls: true debug: false log-package-levels: log-outputs: [default] force-new-cluster: false EOF # master02 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/etcd/etcd.config.yml name: \u0026#39;k8s-master02\u0026#39; data-dir: /var/lib/etcd wal-dir: /var/lib/etcd/wal snapshot-count: 5000 heartbeat-interval: 100 election-timeout: 1000 quota-backend-bytes: 0 listen-peer-urls: \u0026#39;https://192.168.43.202:2380\u0026#39; listen-client-urls: \u0026#39;https://192.168.43.202:2379,http://127.0.0.1:2379\u0026#39; max-snapshots: 3 max-wals: 5 cors: initial-advertise-peer-urls: \u0026#39;https://192.168.43.202:2380\u0026#39; advertise-client-urls: \u0026#39;https://192.168.43.202:2379\u0026#39; discovery: discovery-fallback: \u0026#39;proxy\u0026#39; discovery-proxy: discovery-srv: initial-cluster: \u0026#39;k8s-master01=https://192.168.43.201:2380,k8s-master02=https://192.168.43.202:2380,k8s-master03=https://192.168.43.203:2380\u0026#39; initial-cluster-token: \u0026#39;etcd-k8s-cluster\u0026#39; initial-cluster-state: \u0026#39;new\u0026#39; strict-reconfig-check: false enable-v2: true enable-pprof: true proxy: \u0026#39;off\u0026#39; proxy-failure-wait: 5000 proxy-refresh-interval: 30000 proxy-dial-timeout: 1000 proxy-write-timeout: 5000 proxy-read-timeout: 0 client-transport-security: cert-file: \u0026#39;/etc/kubernetes/pki/etcd/etcd.pem\u0026#39; key-file: \u0026#39;/etc/kubernetes/pki/etcd/etcd-key.pem\u0026#39; client-cert-auth: true trusted-ca-file: \u0026#39;/etc/kubernetes/pki/etcd/etcd-ca.pem\u0026#39; auto-tls: true peer-transport-security: cert-file: \u0026#39;/etc/kubernetes/pki/etcd/etcd.pem\u0026#39; key-file: \u0026#39;/etc/kubernetes/pki/etcd/etcd-key.pem\u0026#39; peer-client-cert-auth: true trusted-ca-file: \u0026#39;/etc/kubernetes/pki/etcd/etcd-ca.pem\u0026#39; auto-tls: true debug: false log-package-levels: log-outputs: [default] force-new-cluster: false EOF # master03 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/etcd/etcd.config.yml name: \u0026#39;k8s-master03\u0026#39; data-dir: /var/lib/etcd wal-dir: /var/lib/etcd/wal snapshot-count: 5000 heartbeat-interval: 100 election-timeout: 1000 quota-backend-bytes: 0 listen-peer-urls: \u0026#39;https://192.168.43.203:2380\u0026#39; listen-client-urls: \u0026#39;https://192.168.43.203:2379,http://127.0.0.1:2379\u0026#39; max-snapshots: 3 max-wals: 5 cors: initial-advertise-peer-urls: \u0026#39;https://192.168.43.203:2380\u0026#39; advertise-client-urls: \u0026#39;https://192.168.43.203:2379\u0026#39; discovery: discovery-fallback: \u0026#39;proxy\u0026#39; discovery-proxy: discovery-srv: initial-cluster: \u0026#39;k8s-master01=https://192.168.43.201:2380,k8s-master02=https://192.168.43.202:2380,k8s-master03=https://192.168.43.203:2380\u0026#39; initial-cluster-token: \u0026#39;etcd-k8s-cluster\u0026#39; initial-cluster-state: \u0026#39;new\u0026#39; strict-reconfig-check: false enable-v2: true enable-pprof: true proxy: \u0026#39;off\u0026#39; proxy-failure-wait: 5000 proxy-refresh-interval: 30000 proxy-dial-timeout: 1000 proxy-write-timeout: 5000 proxy-read-timeout: 0 client-transport-security: cert-file: \u0026#39;/etc/kubernetes/pki/etcd/etcd.pem\u0026#39; key-file: \u0026#39;/etc/kubernetes/pki/etcd/etcd-key.pem\u0026#39; client-cert-auth: true trusted-ca-file: \u0026#39;/etc/kubernetes/pki/etcd/etcd-ca.pem\u0026#39; auto-tls: true peer-transport-security: cert-file: \u0026#39;/etc/kubernetes/pki/etcd/etcd.pem\u0026#39; key-file: \u0026#39;/etc/kubernetes/pki/etcd/etcd-key.pem\u0026#39; peer-client-cert-auth: true trusted-ca-file: \u0026#39;/etc/kubernetes/pki/etcd/etcd-ca.pem\u0026#39; auto-tls: true debug: false log-package-levels: log-outputs: [default] force-new-cluster: false EOF # 在所有master节点创建etcd服务并启动 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /usr/lib/systemd/system/etcd.service [Unit] Description=Etcd Service Documentation=https://coreos.com/etcd/docs/latest/ After=network.target [Service] Type=notify ExecStart=/usr/local/bin/etcd --config-file=/etc/etcd/etcd.config.yml Restart=on-failure RestartSec=10 LimitNOFILE=65536 [Install] WantedBy=multi-user.target Alias=etcd3.service EOF # 所有Master节点创建etcd证书目录并链接证书，否则启动会失败 mkdir /etc/kubernetes/pki/etcd ln -s /etc/etcd/ssl/* /etc/kubernetes/pki/etcd/ # 启动 systemctl daemon-reload \u0026amp;\u0026amp; systemctl enable --now etcd # 查看etcd集群状态如下即可 ETCDCTL_API=3 etcdctl --endpoints=\u0026#34;192.168.43.203:2379,192.168.43.202:2379,192.168.43.201:2379\u0026#34; --cacert=/etc/kubernetes/pki/etcd/etcd-ca.pem --cert=/etc/kubernetes/pki/etcd/etcd.pem --key=/etc/kubernetes/pki/etcd/etcd-key.pem endpoint status --write-out=table +---------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS | +---------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ | 192.168.43.203:2379 | fd1372a073304e | 3.5.1 | 20 kB | false | false | 2 | 9 | 9 | | | 192.168.43.202:2379 | 837ce9c47e0719eb | 3.5.1 | 20 kB | false | false | 2 | 9 | 9 | | | 192.168.43.201:2379 | e9bf8d99824c9061 | 3.5.1 | 20 kB | true | false | 2 | 9 | 9 | | +---------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ 高可用组件(Master) # 所有master节点安装Keepalived和haproxy，并创建配置文件目录 yum install keepalived haproxy -y mkdir /etc/haproxy mkdir /etc/keepalived # 为所有master节点添加haproxy配置，配置都一样，检查最后三行主机名和IP地址对应上就行 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/haproxy/haproxy.cfg global maxconn 2000 ulimit-n 16384 log 127.0.0.1 local0 err stats timeout 30s defaults log global mode http option httplog timeout connect 5000 timeout client 50000 timeout server 50000 timeout http-request 15s timeout http-keep-alive 15s frontend monitor-in bind *:33305 mode http option httplog monitor-uri /monitor frontend k8s-master bind 0.0.0.0:8443 bind 127.0.0.1:8443 mode tcp option tcplog tcp-request inspect-delay 5s default_backend k8s-master backend k8s-master mode tcp option tcplog option tcp-check balance roundrobin default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100 server k8s-master01 192.168.43.201:6443 check server k8s-master02 192.168.43.202:6443 check server k8s-master03 192.168.43.203:6443 check EOF # keepalived配置不一样，注意区分网卡名、IP地址和虚拟IP地址 # 检查服务器网卡名 ip a 或 ifconfig # k8s-master01 Keepalived配置 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/keepalived/keepalived.conf ! Configuration File for keepalived global_defs { router_id LVS_DEVEL script_user root enable_script_security } vrrp_script chk_apiserver { script \u0026#34;/etc/keepalived/check_apiserver.sh\u0026#34; interval 5 weight -5 fall 2 rise 1 } vrrp_instance VI_1 { state MASTER interface ens33 mcast_src_ip 192.168.43.201 virtual_router_id 51 priority 101 advert_int 2 authentication { auth_type PASS auth_pass K8SHA_KA_AUTH } virtual_ipaddress { 192.168.43.200 } track_script { chk_apiserver } } EOF # k8s-master02 Keepalived配置 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/keepalived/keepalived.conf ! Configuration File for keepalived global_defs { router_id LVS_DEVEL script_user root enable_script_security } vrrp_script chk_apiserver { script \u0026#34;/etc/keepalived/check_apiserver.sh\u0026#34; interval 5 weight -5 fall 2 rise 1 } vrrp_instance VI_1 { state BACKUP interface ens33 mcast_src_ip 192.168.43.202 virtual_router_id 51 priority 100 advert_int 2 authentication { auth_type PASS auth_pass K8SHA_KA_AUTH } virtual_ipaddress { 192.168.43.200 } track_script { chk_apiserver } } EOF # k8s-master03 Keepalived配置 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/keepalived/keepalived.conf ! Configuration File for keepalived global_defs { router_id LVS_DEVEL script_user root enable_script_security } vrrp_script chk_apiserver { script \u0026#34;/etc/keepalived/check_apiserver.sh\u0026#34; interval 5 weight -5 fall 2 rise 1 } vrrp_instance VI_1 { state BACKUP interface ens33 mcast_src_ip 192.168.43.203 virtual_router_id 51 priority 100 advert_int 2 authentication { auth_type PASS auth_pass K8SHA_KA_AUTH } virtual_ipaddress { 192.168.43.200 } track_script { chk_apiserver } } EOF # 所有master节点配置Keepalived健康检查脚本 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/keepalived/check_apiserver.sh #!/bin/bash err=0 for k in $(seq 1 3) do check_code=$(pgrep haproxy) if [[ $check_code == \u0026#34;\u0026#34; ]]; then err=$(expr $err + 1) sleep 1 continue else err=0 break fi done if [[ $err != \u0026#34;0\u0026#34; ]]; then echo \u0026#34;systemctl stop keepalived\u0026#34; /usr/bin/systemctl stop keepalived exit 1 else exit 0 fi EOF # 赋予可执行权限 chmod +x /etc/keepalived/check_apiserver.sh # 启动haproxy和Keepalived并加入开机启动 systemctl daemon-reload \u0026amp;\u0026amp; systemctl enable --now haproxy \u0026amp;\u0026amp; systemctl enable --now keepalived # 检查服务是否正常 # 这种告警可以忽略：Mar 06 14:03:35 k8s-master01 haproxy-systemd-wrapper[1981]: [WARNING] 064/140335 (1982) : config : frontend \u0026#39;GLOBAL\u0026#39; has no \u0026#39;bind\u0026#39; systemctl status haproxy systemctl status keepalived # 测试一波 telnet k8s-master-vip 8443 ping k8s-master-vip 配置集群组件 # 所有节点创建以下集群资源目录 mkdir -p /etc/kubernetes/manifests/ /etc/systemd/system/kubelet.service.d /var/lib/kubelet /var/log/kubernetes Apiserver(Master)\n所有master节点配置kube-apiserver服务 # service网段为10.96.0.0/12，可自行设置，其他参数按需配置即可 # master01配置 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /usr/lib/systemd/system/kube-apiserver.service [Unit] Description=Kubernetes API Server Documentation=https://github.com/kubernetes/kubernetes After=network.target [Service] ExecStart=/usr/local/bin/kube-apiserver \\ --v=2 \\ --allow-privileged=true \\ --bind-address=0.0.0.0 \\ --secure-port=6443 \\ --advertise-address=192.168.43.201 \\ --service-cluster-ip-range=10.96.0.0/12 \\ --service-node-port-range=30000-32767 \\ --etcd-servers=https://192.168.43.201:2379,https://192.168.43.202:2379,https://192.168.43.203:2379 \\ --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem \\ --etcd-certfile=/etc/etcd/ssl/etcd.pem \\ --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \\ --client-ca-file=/etc/kubernetes/pki/ca.pem \\ --tls-cert-file=/etc/kubernetes/pki/apiserver.pem \\ --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem \\ --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem \\ --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem \\ --service-account-key-file=/etc/kubernetes/pki/sa.pub \\ --service-account-signing-key-file=/etc/kubernetes/pki/sa.key \\ --service-account-issuer=https://kubernetes.default.svc.cluster.local \\ --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname \\ --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota \\ --authorization-mode=Node,RBAC \\ --enable-bootstrap-token-auth=true \\ --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem \\ --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem \\ --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem \\ --requestheader-allowed-names=aggregator \\ --requestheader-group-headers=X-Remote-Group \\ --requestheader-extra-headers-prefix=X-Remote-Extra- \\ --requestheader-username-headers=X-Remote-User # --token-auth-file=/etc/kubernetes/token.csv Restart=on-failure RestartSec=10s LimitNOFILE=65535 [Install] WantedBy=multi-user.target EOF # master02配置 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /usr/lib/systemd/system/kube-apiserver.service [Unit] Description=Kubernetes API Server Documentation=https://github.com/kubernetes/kubernetes After=network.target [Service] ExecStart=/usr/local/bin/kube-apiserver \\ --v=2 \\ --allow-privileged=true \\ --bind-address=0.0.0.0 \\ --secure-port=6443 \\ --advertise-address=192.168.43.202 \\ --service-cluster-ip-range=10.96.0.0/12 \\ --service-node-port-range=30000-32767 \\ --etcd-servers=https://192.168.43.201:2379,https://192.168.43.202:2379,https://192.168.43.203:2379 \\ --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem \\ --etcd-certfile=/etc/etcd/ssl/etcd.pem \\ --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \\ --client-ca-file=/etc/kubernetes/pki/ca.pem \\ --tls-cert-file=/etc/kubernetes/pki/apiserver.pem \\ --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem \\ --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem \\ --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem \\ --service-account-key-file=/etc/kubernetes/pki/sa.pub \\ --service-account-signing-key-file=/etc/kubernetes/pki/sa.key \\ --service-account-issuer=https://kubernetes.default.svc.cluster.local \\ --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname \\ --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota \\ --authorization-mode=Node,RBAC \\ --enable-bootstrap-token-auth=true \\ --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem \\ --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem \\ --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem \\ --requestheader-allowed-names=aggregator \\ --requestheader-group-headers=X-Remote-Group \\ --requestheader-extra-headers-prefix=X-Remote-Extra- \\ --requestheader-username-headers=X-Remote-User # --token-auth-file=/etc/kubernetes/token.csv Restart=on-failure RestartSec=10s LimitNOFILE=65535 [Install] WantedBy=multi-user.target EOF # master03配置 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /usr/lib/systemd/system/kube-apiserver.service [Unit] Description=Kubernetes API Server Documentation=https://github.com/kubernetes/kubernetes After=network.target [Service] ExecStart=/usr/local/bin/kube-apiserver \\ --v=2 \\ --allow-privileged=true \\ --bind-address=0.0.0.0 \\ --secure-port=6443 \\ --advertise-address=192.168.43.203 \\ --service-cluster-ip-range=10.96.0.0/12 \\ --service-node-port-range=30000-32767 \\ --etcd-servers=https://192.168.43.201:2379,https://192.168.43.202:2379,https://192.168.43.203:2379 \\ --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem \\ --etcd-certfile=/etc/etcd/ssl/etcd.pem \\ --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \\ --client-ca-file=/etc/kubernetes/pki/ca.pem \\ --tls-cert-file=/etc/kubernetes/pki/apiserver.pem \\ --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem \\ --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem \\ --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem \\ --service-account-key-file=/etc/kubernetes/pki/sa.pub \\ --service-account-signing-key-file=/etc/kubernetes/pki/sa.key \\ --service-account-issuer=https://kubernetes.default.svc.cluster.local \\ --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname \\ --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota \\ --authorization-mode=Node,RBAC \\ --enable-bootstrap-token-auth=true \\ --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem \\ --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem \\ --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem \\ --requestheader-allowed-names=aggregator \\ --requestheader-group-headers=X-Remote-Group \\ --requestheader-extra-headers-prefix=X-Remote-Extra- \\ --requestheader-username-headers=X-Remote-User # --token-auth-file=/etc/kubernetes/token.csv Restart=on-failure RestartSec=10s LimitNOFILE=65535 [Install] WantedBy=multi-user.target EOF # 启动kube-apiserver systemctl daemon-reload \u0026amp;\u0026amp; systemctl enable --now kube-apiserver ControllerManager(Master)\n所有Master节点配置kube-controller-manager服务，配置都一样\n# Pod网段是172.16.0.0/12，可按需更改，不要和其他在用网段冲突即可 # 给所有master节点配置服务 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /usr/lib/systemd/system/kube-controller-manager.service [Unit] Description=Kubernetes Controller Manager Documentation=https://github.com/kubernetes/kubernetes After=network.target [Service] ExecStart=/usr/local/bin/kube-controller-manager \\ --v=2 \\ --root-ca-file=/etc/kubernetes/pki/ca.pem \\ --cluster-signing-cert-file=/etc/kubernetes/pki/ca.pem \\ --cluster-signing-key-file=/etc/kubernetes/pki/ca-key.pem \\ --service-account-private-key-file=/etc/kubernetes/pki/sa.key \\ --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig \\ --feature-gates=LegacyServiceAccountTokenNoAutoGeneration=false \\ --leader-elect=true \\ --use-service-account-credentials=true \\ --node-monitor-grace-period=40s \\ --node-monitor-period=5s \\ --pod-eviction-timeout=2m0s \\ --controllers=*,bootstrapsigner,tokencleaner \\ --allocate-node-cidrs=true \\ --cluster-cidr=172.16.0.0/12 \\ --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem \\ --node-cidr-mask-size=24 Restart=always RestartSec=10s [Install] WantedBy=multi-user.target EOF # 启动 systemctl daemon-reload \u0026amp;\u0026amp; systemctl enable --now kube-controller-manager Scheduler(Master)\n所有Master节点配置kube-scheduler服务，配置都一样\ncat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /usr/lib/systemd/system/kube-scheduler.service [Unit] Description=Kubernetes Scheduler Documentation=https://github.com/kubernetes/kubernetes After=network.target [Service] ExecStart=/usr/local/bin/kube-scheduler \\ --v=2 \\ --leader-elect=true \\ --authentication-kubeconfig=/etc/kubernetes/scheduler.kubeconfig \\ --authorization-kubeconfig=/etc/kubernetes/scheduler.kubeconfig \\ --kubeconfig=/etc/kubernetes/scheduler.kubeconfig Restart=always RestartSec=10s [Install] WantedBy=multi-user.target EOF # 启动 systemctl daemon-reload \u0026amp;\u0026amp; systemctl enable --now kube-scheduler 确认集群组件状态\n# 所有Master节点均检查，-l参数输出完整信息 # 如果有E开头的报错，需要排查解决一下，常见问题是IP冲突、证书错误 # W告警可暂时忽略 systemctl status kube-apiserver -l systemctl status kube-controller-manager -l systemctl status kube-scheduler -l 配置TLS Bootstrapping 只需在master01节点配置，TLS Bootstrapping的官方说明请见：TLS Bootstrapping\ncd /root/k8s-ha-install/bootstrap # 查看一下secret，name后要和token-id一致，token-id.token-secret和集群设置--token=对应上 [root@k8s-master01 bootstrap]# cat bootstrap.secret.yaml apiVersion: v1 kind: Secret metadata: name: bootstrap-token-c8ad9c namespace: kube-system type: bootstrap.kubernetes.io/token stringData: description: \u0026#34;The default bootstrap token generated by \u0026#39;kubelet \u0026#39;.\u0026#34; token-id: c8ad9c token-secret: 2e4d610cf3e7426e ... kubectl config set-cluster kubernetes --certificate-authority=/etc/kubernetes/pki/ca.pem --embed-certs=true --server=https://192.168.43.200:8443 --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig kubectl config set-credentials tls-bootstrap-token-user --token=c8ad9c.2e4d610cf3e7426e --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig kubectl config set-context tls-bootstrap-token-user@kubernetes --cluster=kubernetes --user=tls-bootstrap-token-user --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig kubectl config use-context tls-bootstrap-token-user@kubernetes --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig # 创建配置目录，拷贝admin.kubeconfig文件为config，授权kubectl，使得当前用户可以使用kubectl创建资源 # 其他master节点如果需要使用kubectl命令创建资源，也可以拷贝文件过去 # 下面是没有授权的情况 # [root@k8s-master01 bootstrap]# kubectl get cs # The connection to the server localhost:8080 was refused - did you specify the right host or port? mkdir -p /root/.kube;cp /etc/kubernetes/admin.kubeconfig /root/.kube/config # 授权后查看集群状态 [root@k8s-master01 bootstrap]# kubectl get cs Warning: v1 ComponentStatus is deprecated in v1.19+ NAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy {\u0026#34;health\u0026#34;:\u0026#34;true\u0026#34;,\u0026#34;reason\u0026#34;:\u0026#34;\u0026#34;} etcd-1 Healthy {\u0026#34;health\u0026#34;:\u0026#34;true\u0026#34;,\u0026#34;reason\u0026#34;:\u0026#34;\u0026#34;} etcd-2 Healthy {\u0026#34;health\u0026#34;:\u0026#34;true\u0026#34;,\u0026#34;reason\u0026#34;:\u0026#34;\u0026#34;} # 创建bootstrap-secret [root@k8s-master01 bootstrap]# kubectl apply -f bootstrap.secret.yaml secret/bootstrap-token-c8ad9c created clusterrolebinding.rbac.authorization.k8s.io/kubelet-bootstrap created clusterrolebinding.rbac.authorization.k8s.io/node-autoapprove-bootstrap created clusterrolebinding.rbac.authorization.k8s.io/node-autoapprove-certificate-rotation created clusterrole.rbac.authorization.k8s.io/system:kube-apiserver-to-kubelet created clusterrolebinding.rbac.authorization.k8s.io/system:kube-apiserver created 配置Kubelet # 从master01拷贝证书文件到其他节点 cd /etc/kubernetes/ for i in k8s-master02 k8s-master03 k8s-node01 k8s-node02; do ssh $i mkdir -p /etc/kubernetes/pki for FILE in pki/ca.pem pki/ca-key.pem pki/front-proxy-ca.pem bootstrap-kubelet.kubeconfig; do scp /etc/kubernetes/$FILE $i:/etc/kubernetes/${FILE} done done # 所有节点配置kubelet服务 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /usr/lib/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kubelet Restart=always StartLimitInterval=0 RestartSec=10 [Install] WantedBy=multi-user.target EOF # Runtime为Containerd，kubelet服务的配置文件 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/systemd/system/kubelet.service.d/10-kubelet.conf [Service] Environment=\u0026#34;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig --kubeconfig=/etc/kubernetes/kubelet.kubeconfig\u0026#34; Environment=\u0026#34;KUBELET_SYSTEM_ARGS=--container-runtime=remote --runtime-request-timeout=15m --container-runtime-endpoint=unix:///run/containerd/containerd.sock\u0026#34; Environment=\u0026#34;KUBELET_CONFIG_ARGS=--config=/etc/kubernetes/kubelet-conf.yml\u0026#34; Environment=\u0026#34;KUBELET_EXTRA_ARGS=--node-labels=node.kubernetes.io/node=\u0026#39;\u0026#39; \u0026#34; ExecStart= ExecStart=/usr/local/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_SYSTEM_ARGS $KUBELET_EXTRA_ARGS EOF # 创建kubelet配置文件，对应上面Environment配置KUBELET_CONFIG_ARGS # clusterDNS配置为service网段的第十个地址 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/kubernetes/kubelet-conf.yml apiVersion: kubelet.config.k8s.io/v1beta1 kind: KubeletConfiguration address: 0.0.0.0 port: 10250 readOnlyPort: 10255 authentication: anonymous: enabled: false webhook: cacheTTL: 2m0s enabled: true x509: clientCAFile: /etc/kubernetes/pki/ca.pem authorization: mode: Webhook webhook: cacheAuthorizedTTL: 5m0s cacheUnauthorizedTTL: 30s cgroupDriver: systemd cgroupsPerQOS: true clusterDNS: - 10.96.0.10 clusterDomain: cluster.local containerLogMaxFiles: 5 containerLogMaxSize: 10Mi contentType: application/vnd.kubernetes.protobuf cpuCFSQuota: true cpuManagerPolicy: none cpuManagerReconcilePeriod: 10s enableControllerAttachDetach: true enableDebuggingHandlers: true enforceNodeAllocatable: - pods eventBurst: 10 eventRecordQPS: 5 evictionHard: imagefs.available: 15% memory.available: 100Mi nodefs.available: 10% nodefs.inodesFree: 5% evictionPressureTransitionPeriod: 5m0s failSwapOn: true fileCheckFrequency: 20s hairpinMode: promiscuous-bridge healthzBindAddress: 127.0.0.1 healthzPort: 10248 httpCheckFrequency: 20s imageGCHighThresholdPercent: 85 imageGCLowThresholdPercent: 80 imageMinimumGCAge: 2m0s iptablesDropBit: 15 iptablesMasqueradeBit: 14 kubeAPIBurst: 10 kubeAPIQPS: 5 makeIPTablesUtilChains: true maxOpenFiles: 1000000 maxPods: 110 nodeStatusUpdateFrequency: 10s oomScoreAdj: -999 podPidsLimit: -1 registryBurst: 10 registryPullQPS: 5 resolvConf: /etc/resolv.conf rotateCertificates: true runtimeRequestTimeout: 2m0s serializeImagePulls: true staticPodPath: /etc/kubernetes/manifests streamingConnectionIdleTimeout: 4h0m0s syncFrequency: 1m0s volumeStatsAggPeriod: 1m0s EOF # 启动服务 systemctl daemon-reload \u0026amp;\u0026amp; systemctl enable --now kubelet # 以k8s-master01为例查看运行日志和状态，只有CNI一处报错表示配置正确，后面CNI配置好后就会正常 tail -f /var/log/messages ... Apr 21 16:15:56 k8s-master01 kubelet: E0421 16:15:56.608747 7169 kubelet.go:2475] \u0026#34;Container runtime network not ready\u0026#34; networkReady=\u0026#34;NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\u0026#34; ... [root@k8s-master01 ~]# systemctl status kubelet ● kubelet.service - Kubernetes Kubelet Loaded: loaded (/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled) Drop-In: /etc/systemd/system/kubelet.service.d └─10-kubelet.conf Active: active (running) since 五 2023-04-21 16:09:00 CST; 6min ago Docs: https://github.com/kubernetes/kubernetes Main PID: 7169 (kubelet) Tasks: 11 Memory: 40.6M CGroup: /system.slice/kubelet.service └─7169 /usr/local/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig --kubeconfig=/etc/kuberne... 4月 21 16:14:26 k8s-master01 kubelet[7169]: E0421 16:14:26.578931 7169 kubelet.go:2475] \u0026#34;Container runtime network not read...alized\u0026#34; 4月 21 16:14:31 k8s-master01 kubelet[7169]: E0421 16:14:31.580077 7169 kubelet.go:2475] \u0026#34;Container runtime network not read...alized\u0026#34; .... # 并且此时节点状态应该是可查询且处于NotReady，CNI配置后就会ready [root@k8s-master01 ~]# kubectl get node NAME STATUS ROLES AGE VERSION k8s-master01 NotReady \u0026lt;none\u0026gt; 7m12s v1.26.4 k8s-master02 NotReady \u0026lt;none\u0026gt; 7m11s v1.26.4 k8s-master03 NotReady \u0026lt;none\u0026gt; 7m10s v1.26.4 k8s-node01 NotReady \u0026lt;none\u0026gt; 7m24s v1.26.4 k8s-node02 NotReady \u0026lt;none\u0026gt; 7m12s v1.26.4 配置Kube-proxy # 只需在k8s-master01上执行 cd /root/k8s-ha-install kubectl -n kube-system create serviceaccount kube-proxy kubectl create clusterrolebinding system:kube-proxy --clusterrole system:node-proxier --serviceaccount kube-system:kube-proxy SECRET=$(kubectl -n kube-system get sa/kube-proxy \\ --output=jsonpath=\u0026#39;{.secrets[0].name}\u0026#39;) JWT_TOKEN=$(kubectl -n kube-system get secret/$SECRET \\ --output=jsonpath=\u0026#39;{.data.token}\u0026#39; | base64 -d) PKI_DIR=/etc/kubernetes/pki K8S_DIR=/etc/kubernetes kubectl config set-cluster kubernetes --certificate-authority=/etc/kubernetes/pki/ca.pem --embed-certs=true --server=https://192.168.43.200:8443 --kubeconfig=${K8S_DIR}/kube-proxy.kubeconfig kubectl config set-credentials kubernetes --token=${JWT_TOKEN} --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig kubectl config set-context kubernetes --cluster=kubernetes --user=kubernetes --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig kubectl config use-context kubernetes --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig # 从k8s-master01将kubeconfig拷贝到其他节点 for i in k8s-master02 k8s-master03 k8s-node01 k8s-node02; do scp /etc/kubernetes/kube-proxy.kubeconfig $i:/etc/kubernetes/kube-proxy.kubeconfig done # 所有节点配置kube-proxy服务 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /usr/lib/systemd/system/kube-proxy.service [Unit] Description=Kubernetes Kube Proxy Documentation=https://github.com/kubernetes/kubernetes After=network.target [Service] ExecStart=/usr/local/bin/kube-proxy \\ --config=/etc/kubernetes/kube-proxy.yaml \\ --v=2 Restart=always RestartSec=10s [Install] WantedBy=multi-user.target EOF # 所有节点配置kube-proxy.yaml,clusterCIDR为pod网段 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/kubernetes/kube-proxy.yaml apiVersion: kubeproxy.config.k8s.io/v1alpha1 bindAddress: 0.0.0.0 clientConnection: acceptContentTypes: \u0026#34;\u0026#34; burst: 10 contentType: application/vnd.kubernetes.protobuf kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig qps: 5 clusterCIDR: 172.16.0.0/12 configSyncPeriod: 15m0s conntrack: max: null maxPerCore: 32768 min: 131072 tcpCloseWaitTimeout: 1h0m0s tcpEstablishedTimeout: 24h0m0s enableProfiling: false healthzBindAddress: 0.0.0.0:10256 hostnameOverride: \u0026#34;\u0026#34; iptables: masqueradeAll: false masqueradeBit: 14 minSyncPeriod: 0s syncPeriod: 30s ipvs: masqueradeAll: true minSyncPeriod: 5s scheduler: \u0026#34;rr\u0026#34; syncPeriod: 30s kind: KubeProxyConfiguration metricsBindAddress: 127.0.0.1:10249 mode: \u0026#34;ipvs\u0026#34; nodePortAddresses: null oomScoreAdj: -999 portRange: \u0026#34;\u0026#34; udpIdleTimeout: 250ms EOF # 启动 systemctl daemon-reload \u0026amp;\u0026amp; systemctl enable --now kube-proxy 配置Calico # k8s-master01上执行，更改calico中Pod网段为自己的 cd /root/k8s-ha-install/calico/ sed -i \u0026#34;s#POD_CIDR#172.16.0.0/12#g\u0026#34; calico.yaml # 检查是否更改成功 [root@k8s-master01 calico]# grep \u0026#34;IPV4POOL_CIDR\u0026#34; calico.yaml -A 1 - name: CALICO_IPV4POOL_CIDR value: \u0026#34;172.16.0.0/12\u0026#34; # 应用calico [root@k8s-master01 calico]# kubectl apply -f calico.yaml poddisruptionbudget.policy/calico-kube-controllers created poddisruptionbudget.policy/calico-typha created serviceaccount/calico-kube-controllers created serviceaccount/calico-node created configmap/calico-config created customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created clusterrole.rbac.authorization.k8s.io/calico-node created clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created clusterrolebinding.rbac.authorization.k8s.io/calico-node created service/calico-typha created daemonset.apps/calico-node created deployment.apps/calico-kube-controllers created deployment.apps/calico-typha created # （中间状态）在calico生效过程中查看各节点污点状态，发现均有不可调度污点，不必管他，等待calico生效即可，这种污点是kubernetes集群保护机制，在节点处于not ready状态时，节点不可调度 [root@k8s-master01 calico]# kubectl describe node| grep Taints: Taints: node.kubernetes.io/not-ready:NoSchedule Taints: node.kubernetes.io/not-ready:NoSchedule Taints: node.kubernetes.io/not-ready:NoSchedule Taints: node.kubernetes.io/not-ready:NoSchedule Taints: node.kubernetes.io/not-ready:NoSchedule # calico Pod处于Running后集群网络将建立，node将处于Ready状态，calico建立网络取决于电脑性能（硬件和网络环境），一般几分钟即可完成，性能差的可能会花费更长时间 [root@k8s-master01 calico]# kubectl get po -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-6bd6b69df9-5nwkl 1/1 Running 0 10m kube-system calico-node-8cvcd 1/1 Running 0 10m kube-system calico-node-96mx8 1/1 Running 1 (36s ago) 10m kube-system calico-node-f49rb 1/1 Running 0 10m kube-system calico-node-rgs7f 1/1 Running 0 10m kube-system calico-node-vzrls 1/1 Running 0 10m kube-system calico-typha-77fc8866f5-h9bsj 1/1 Running 0 10m # 查看集群状态和资源 [root@k8s-master01 calico]# kubectl get node NAME STATUS ROLES AGE VERSION k8s-master01 Ready \u0026lt;none\u0026gt; 23m v1.26.4 k8s-master02 Ready \u0026lt;none\u0026gt; 23m v1.26.4 k8s-master03 Ready \u0026lt;none\u0026gt; 23m v1.26.4 k8s-node01 Ready \u0026lt;none\u0026gt; 23m v1.26.4 k8s-node02 Ready \u0026lt;none\u0026gt; 23m v1.26.4 配置CoreDNS 在k8s-master01操作 # 如果更改了k8s service的网段需要将coredns的serviceIP改成k8s service网段的第十个IP cd /root/k8s-ha-install/ COREDNS_SERVICE_IP=`kubectl get svc | grep kubernetes | awk \u0026#39;{print $3}\u0026#39;`0 sed -i \u0026#34;s#KUBEDNS_SERVICE_IP#${COREDNS_SERVICE_IP}#g\u0026#34; CoreDNS/coredns.yaml kubectl apply -f CoreDNS/coredns.yaml [root@k8s-master01 k8s-ha-install]# kubectl get pod -A | grep coredns kube-system coredns-5db5696c7-tsqts 1/1 Running 0 80s 配置Metrics 在k8s-master01操作 在新版的Kubernetes中系统资源的采集均使用Metrics-server，可以通过Metrics采集节点和Pod的内存、磁盘、CPU和网络的使用率。\ncd /root/k8s-ha-install/metrics-server kubectl create -f . # 等待metrics-server部署好后，便可使用 [root@k8s-master01 metrics-server]# kubectl top nodes NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% k8s-master01 186m 9% 1094Mi 58% k8s-master02 192m 9% 1176Mi 62% k8s-master03 176m 8% 1123Mi 60% k8s-node01 72m 3% 463Mi 24% k8s-node02 66m 3% 472Mi 25% 配置Dashboard Dashboard是一个展示Kubernetes集群资源和Pod日志，甚至可以执行容器命令的web控制台。\n# 直接部署即可 cd /root/k8s-ha-install/dashboard/ kubectl apply -f . # 查看dashboard端口，默认是NodePort模式，访问集群内任意节点的32486端口即可 [root@k8s-master01 ~]# kubectl get svc -A | grep dash kubernetes-dashboard dashboard-metrics-scraper ClusterIP 10.111.39.8 \u0026lt;none\u0026gt; 8000/TCP 12m kubernetes-dashboard kubernetes-dashboard NodePort 10.98.143.126 \u0026lt;none\u0026gt; 443:32486/TCP 12m 访问dashboard：https://集群内任意节点IP:32486\n发现提示隐私设置错误的问题，解决方法是在Chrome浏览器启动参数加入--test-type --ignore-certificate-errors，再访问就没有这个提示\n# 获取登陆令牌（token） [root@k8s-master01 dashboard]# kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk \u0026#39;{print $1}\u0026#39;) Name: admin-user-token-cj2kt Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: admin-user kubernetes.io/service-account.uid: c86fbde2-36ea-4dd3-94fd-8ce8012fdf22 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1411 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6ImFPeklobHBkNVRzZzZYVF9nbG5BMTgwOHdvMUNkV2FGbW1wdmUzZzdJRXcifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLWNqMmt0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJjODZmYmRlMi0zNmVhLTRkZDMtOTRmZC04Y2U4MDEyZmRmMjIiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.YgxIsaxR-hfyT9YLGdszggQ0Rvoc4SvyqswgvHz2ySc27q8lAQ7EJxhze3bhrdTL79z3J30T6vmuA5Be3kq_c2r42r2Iy-pC92t8xTISlPWEl7JfSg8GSbX2-UxUM_wqCmbMO3RWGYW5FpzrJ2pSVaeIGlu2JmYTugtS50LCFi87DmP2tDAKLQfh1NRylpEPI1AJPbl41E2wyDBUlS86YF_glUnQxyDCyrf2wJ2Akjqe7If2b9tAXHbSZBcQJFGHENymYhdBW6QObmTRxUsaOX9wdTToFcoHr-FaE4LcP9KuXhxP-gNNyVN7HN0k0WbhAp6CBIoypFCVLIN96EvNIg 选择ALL namespace，可以查看如下图 集群优化(可选) Docker可在/etc/docker/daemon.json自定义优化配置，所有配置可见：官方docker configuration，docker常用优化配置见下方注释说明。\n# （！！！如果使用docker作为Runtime的话）优化docker配置 # /etc/docker/daemon.json文件，按需配置，不需要全部都照抄，使用时删除注释，因为JSON文件不支持注释 { \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;], # cgroups驱动 \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://ynirk4k5.mirror.aliyuncs.com\u0026#34;], # 镜像加速器地址 \u0026#34;allow-nondistributable-artifacts\u0026#34;: [], \u0026#34;api-cors-header\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;authorization-plugins\u0026#34;: [], \u0026#34;bip\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;bridge\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;cgroup-parent\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;cluster-advertise\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;cluster-store\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;cluster-store-opts\u0026#34;: {}, \u0026#34;containerd\u0026#34;: \u0026#34;/run/containerd/containerd.sock\u0026#34;, \u0026#34;containerd-namespace\u0026#34;: \u0026#34;docker\u0026#34;, \u0026#34;data-root\u0026#34;: \u0026#34;\u0026#34;, # 数据根目录，大量docker镜像可能会占用较大存储，可以设置系统盘外的挂载盘 \u0026#34;debug\u0026#34;: true, \u0026#34;default-address-pools\u0026#34;: [ { \u0026#34;base\u0026#34;: \u0026#34;172.30.0.0/16\u0026#34;, \u0026#34;size\u0026#34;: 24 }, { \u0026#34;base\u0026#34;: \u0026#34;172.31.0.0/16\u0026#34;, \u0026#34;size\u0026#34;: 24 } ], \u0026#34;default-cgroupns-mode\u0026#34;: \u0026#34;private\u0026#34;, \u0026#34;default-gateway\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;default-gateway-v6\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;default-runtime\u0026#34;: \u0026#34;runc\u0026#34;, \u0026#34;default-shm-size\u0026#34;: \u0026#34;64M\u0026#34;, \u0026#34;default-ulimits\u0026#34;: { \u0026#34;nofile\u0026#34;: { \u0026#34;Hard\u0026#34;: 64000, \u0026#34;Name\u0026#34;: \u0026#34;nofile\u0026#34;, \u0026#34;Soft\u0026#34;: 64000 } }, \u0026#34;dns\u0026#34;: [], \u0026#34;dns-opts\u0026#34;: [], \u0026#34;dns-search\u0026#34;: [], \u0026#34;exec-root\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;experimental\u0026#34;: false, \u0026#34;features\u0026#34;: {}, \u0026#34;fixed-cidr\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;fixed-cidr-v6\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;group\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;hosts\u0026#34;: [], \u0026#34;icc\u0026#34;: false, \u0026#34;init\u0026#34;: false, \u0026#34;init-path\u0026#34;: \u0026#34;/usr/libexec/docker-init\u0026#34;, \u0026#34;insecure-registries\u0026#34;: [], \u0026#34;ip\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;ip-forward\u0026#34;: false, \u0026#34;ip-masq\u0026#34;: false, \u0026#34;iptables\u0026#34;: false, \u0026#34;ip6tables\u0026#34;: false, \u0026#34;ipv6\u0026#34;: false, \u0026#34;labels\u0026#34;: [], \u0026#34;live-restore\u0026#34;: true, # docker进程宕机时容器依然保持存活 \u0026#34;log-driver\u0026#34;: \u0026#34;json-file\u0026#34;, # 日志格式 \u0026#34;log-level\u0026#34;: \u0026#34;\u0026#34;, # 日志级别 \u0026#34;log-opts\u0026#34;: { # 日志优化 \u0026#34;cache-disabled\u0026#34;: \u0026#34;false\u0026#34;, \u0026#34;cache-max-file\u0026#34;: \u0026#34;5\u0026#34;, \u0026#34;cache-max-size\u0026#34;: \u0026#34;20m\u0026#34;, \u0026#34;cache-compress\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;env\u0026#34;: \u0026#34;os,customer\u0026#34;, \u0026#34;labels\u0026#34;: \u0026#34;somelabel\u0026#34;, \u0026#34;max-file\u0026#34;: \u0026#34;5\u0026#34;, # 最大日志数量 \u0026#34;max-size\u0026#34;: \u0026#34;10m\u0026#34; # 保存的最大日志大小 }, \u0026#34;max-concurrent-downloads\u0026#34;: 3, # pull下载并发数 \u0026#34;max-concurrent-uploads\u0026#34;: 5, # push上传并发数 \u0026#34;max-download-attempts\u0026#34;: 5, \u0026#34;mtu\u0026#34;: 0, \u0026#34;no-new-privileges\u0026#34;: false, \u0026#34;node-generic-resources\u0026#34;: [ \u0026#34;NVIDIA-GPU=UUID1\u0026#34;, \u0026#34;NVIDIA-GPU=UUID2\u0026#34; ], \u0026#34;oom-score-adjust\u0026#34;: -500, \u0026#34;pidfile\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;raw-logs\u0026#34;: false, \u0026#34;runtimes\u0026#34;: { \u0026#34;cc-runtime\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/usr/bin/cc-runtime\u0026#34; }, \u0026#34;custom\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/usr/local/bin/my-runc-replacement\u0026#34;, \u0026#34;runtimeArgs\u0026#34;: [ \u0026#34;--debug\u0026#34; ] } }, \u0026#34;seccomp-profile\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;selinux-enabled\u0026#34;: false, \u0026#34;shutdown-timeout\u0026#34;: 15, \u0026#34;storage-driver\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;storage-opts\u0026#34;: [], \u0026#34;swarm-default-advertise-addr\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;tls\u0026#34;: true, \u0026#34;tlscacert\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;tlscert\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;tlskey\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;tlsverify\u0026#34;: true, \u0026#34;userland-proxy\u0026#34;: false, \u0026#34;userland-proxy-path\u0026#34;: \u0026#34;/usr/libexec/docker-proxy\u0026#34;, \u0026#34;userns-remap\u0026#34;: \u0026#34;\u0026#34; } # 无注释版 { \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;], \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://ynirk4k5.mirror.aliyuncs.com\u0026#34;], \u0026#34;containerd-namespace\u0026#34;: \u0026#34;docker\u0026#34;, \u0026#34;data-root\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;debug\u0026#34;: true, \u0026#34;default-cgroupns-mode\u0026#34;: \u0026#34;private\u0026#34;, \u0026#34;default-gateway\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;default-gateway-v6\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;default-runtime\u0026#34;: \u0026#34;runc\u0026#34;, \u0026#34;default-shm-size\u0026#34;: \u0026#34;64M\u0026#34;, \u0026#34;default-ulimits\u0026#34;: { \u0026#34;nofile\u0026#34;: { \u0026#34;Hard\u0026#34;: 64000, \u0026#34;Name\u0026#34;: \u0026#34;nofile\u0026#34;, \u0026#34;Soft\u0026#34;: 64000 } }, \u0026#34;init-path\u0026#34;: \u0026#34;/usr/libexec/docker-init\u0026#34;, \u0026#34;live-restore\u0026#34;: true, \u0026#34;log-driver\u0026#34;: \u0026#34;json-file\u0026#34;, \u0026#34;log-level\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;log-opts\u0026#34;: { \u0026#34;cache-disabled\u0026#34;: \u0026#34;false\u0026#34;, \u0026#34;cache-max-file\u0026#34;: \u0026#34;5\u0026#34;, \u0026#34;cache-max-size\u0026#34;: \u0026#34;20m\u0026#34;, \u0026#34;cache-compress\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;env\u0026#34;: \u0026#34;os,customer\u0026#34;, \u0026#34;labels\u0026#34;: \u0026#34;somelabel\u0026#34;, \u0026#34;max-file\u0026#34;: \u0026#34;5\u0026#34;, \u0026#34;max-size\u0026#34;: \u0026#34;10m\u0026#34; }, \u0026#34;max-concurrent-downloads\u0026#34;: 3, \u0026#34;max-concurrent-uploads\u0026#34;: 5, \u0026#34;max-download-attempts\u0026#34;: 5, \u0026#34;mtu\u0026#34;: 0, \u0026#34;no-new-privileges\u0026#34;: false, \u0026#34;oom-score-adjust\u0026#34;: -500, \u0026#34;pidfile\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;raw-logs\u0026#34;: false, \u0026#34;runtimes\u0026#34;: { \u0026#34;cc-runtime\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/usr/bin/cc-runtime\u0026#34; }, \u0026#34;custom\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/usr/local/bin/my-runc-replacement\u0026#34;, \u0026#34;runtimeArgs\u0026#34;: [ \u0026#34;--debug\u0026#34; ] } }, \u0026#34;seccomp-profile\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;selinux-enabled\u0026#34;: false, \u0026#34;shutdown-timeout\u0026#34;: 15, \u0026#34;storage-driver\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;storage-opts\u0026#34;: [], \u0026#34;swarm-default-advertise-addr\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;userland-proxy-path\u0026#34;: \u0026#34;/usr/libexec/docker-proxy\u0026#34;, \u0026#34;userns-remap\u0026#34;: \u0026#34;\u0026#34; } # 设置证书有效期 [root@k8s-master01 ~]# vim /usr/lib/systemd/system/kube-controller-manager.service ... # 加入下面配置 --experimental-cluster-signing-duration=876000h0m0s ... [root@k8s-master01 ~]# systemctl daemon-reload [root@k8s-master01 ~]# systemctl restart kube-controller-manager # kubelet优化加密算法，默认的算法容易被漏洞扫描；增长镜像下载周期，避免有些大镜像未下载完成就被动死亡退出 # --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 # --image-pull-progress-deadline=30m [root@k8s-master01 ~]# vim /etc/systemd/system/kubelet.service.d/10-kubelet.conf ... # 下面这行中KUBELET_EXTRA_ARGS=后加入配置 Environment=\u0026#34;KUBELET_EXTRA_ARGS=--tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 --image-pull-progress-deadline=30m\u0026#34; ... # 集群配置优化，详见https://kubernetes.io/zh/docs/tasks/administer-cluster/reserve-compute-resources/ [root@k8s-master01 ~]# vim /etc/kubernetes/kubelet-conf.yml # 文件中添加如下配置 rotateServerCertificates: true allowedUnsafeSysctls: # 允许在修改内核参数，此操作按情况选择，用不到就不用设置 - \u0026#34;net.core*\u0026#34; - \u0026#34;net.ipv4.*\u0026#34; kubeReserved: # 为Kubernetes集群守护进程组件预留资源，例如：kubelet、Runtime等 cpu: \u0026#34;100m\u0026#34; memory: 100Mi ephemeral-storage: 1Gi systemReserved: # 为系统守护进程预留资源，例如：sshd、cron等 cpu: \u0026#34;100m\u0026#34; memory: 100Mi ephemeral-storage: 1Gi # 为集群节点打标签，删除标签把 = 换成 - 即可 kubectl label nodes k8s-node01 node-role.kubernetes.io/node= kubectl label nodes k8s-node02 node-role.kubernetes.io/node= kubectl label nodes k8s-master01 node-role.kubernetes.io/master= kubectl label nodes k8s-master02 node-role.kubernetes.io/master= kubectl label nodes k8s-master03 node-role.kubernetes.io/master= # 添加标签后查看集群状态 [root@k8s-master01 ~]# kubectl get node NAME STATUS ROLES AGE VERSION k8s-master01 Ready master 35m v1.26.4 k8s-master02 Ready master 35m v1.26.4 k8s-master03 Ready master 35m v1.26.4 k8s-node01 Ready node 35m v1.26.4 k8s-node02 Ready node 35m v1.26.4 生产环境建议ETCD集群和Kubernetes集群分离，而且使用高性能数据盘存储数据，根据情况决定是否将Master节点也作为Pod调度节点。\n测试集群 # 测试namespace kubectl get namespace kubectl create namespace test kubectl get namespace kubectl delete namespace test # 创建nginx实例并开放端口 kubectl create deployment nginx --image=nginx kubectl expose deployment nginx --port=80 --type=NodePort # 查看调度状态和端口号 [root@k8s-master01 ~]# kubectl get po,svc -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/nginx-748c667d99-dmtn6 1/1 Running 0 9m55s 172.25.244.194 k8s-master01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 69m \u0026lt;none\u0026gt; service/nginx NodePort 10.110.18.105 \u0026lt;none\u0026gt; 80:31687/TCP 9s app=nginx 在浏览器输入http://任意节点IP:31687/ 访问nginx，访问结果如图\n至此，基于二进制方式的Kubernetes高可用集群部署并验证成功。\n","permalink":"https://deemoprobe.github.io/posts/tech/kubernetesinstallation-binary/","summary":"环境说明 宿主机系统：Windows 10 虚拟机版本：VMware® Workstation 16 Pro IOS镜像版本：CentOS Linux release 7.9.2009 Kubernetes版本：1.26.4 Runtime：Containerd v1.6.20 Etcd版本：3.5.6 集群操作用户：root 更新时间：2023-04-20 CentOS7安装请参考博","title":"KubernetesInstallation Binary"},{"content":"擅长Linux、Docker、Kubernetes等运维工作，熟悉常见的网络协议和网络数据包分析，熟悉常见的中间件，熟悉CI/CD、Kubesphere、Harbor、Helm、Prometheus、Microservices等云原生相关技术。了解业务架构设计的基本理念，对云原生和业务架构设计很感兴趣，目标是成为一名优秀的云原生全栈架构师。\n自知水平尚浅，但也始终相信“一分耕耘，一分收获”。\n本博客将永久更新和维护，记录技术生涯的点滴以及生活的感悟。\n由于个人建立博客的目的仅是为了记录笔记以及生活感悟。所以采用了Hugo+GitHubPages这种静态博客方式发布，没有留言系统，没有特殊字体，没有过多的JavaScript外链，即使如此网页访问速度也一般，但足够简洁实用了。\nNetName: deemoprobe Email: deemoprobe@gmail.com Blog: https://deemoprobe.github.com GitHub: deemoprobe BlogTheme: hugo-papermod2 Skills: Linux（RHCSA/RHCE/Shell） Docker/Kubernetes（CKA/CKS） DevOps（CICD：Jenkins/Ansible/Gitlab） Middleware（Weblogic/Nginx/Apache/Tomcat/JVM） Network(HTTP/TCPIP/netwox/tcpdump/Wireshark/PT/eNSP) Hobby: Cooking/Journey/Marathon/Cloud Native/Architecture/Kali/eBPF/Keyboard/ Address: Shanghai China 截至2023年1月1日，已取得相关技能证书清单如下：\nRED HAT CERTIFIED SYSTEM ADMINISTRATOR 证书验证：https://rhtapps.redhat.com/verify\nRED HAT CERTIFIED ENGINEER 证书验证：https://rhtapps.redhat.com/verify\nCERTIFIED KUBERNETES ADMINISTRATOR 证书验证：https://training.linuxfoundation.org/certification/verify\nCERTIFIED KUBERNETES SECURITY SPECIALIST 证书验证：https://training.linuxfoundation.org/certification/verify\nKUBERNETES AND CLOUD NATIVE ASSOCIATE 证书验证：https://training.linuxfoundation.org/certification/verify\n诸君共勉: 业精于勤荒于嬉,行成于思毁于随\n","permalink":"https://deemoprobe.github.io/about/","summary":"擅长Linux、Docker、Kubernetes等运维工作，熟悉常见的网络协议和网络数据包分析，熟悉常见的中间件，熟悉CI/CD、Kubesphere、Harbor、Helm、Prometheus、Microservices等云原生相关技术。了解业务架构设计的基本理念，对云原生","title":"🙋🏻‍♂️ 关于作者"},{"content":"页面测试 由于访问速度问题，没有域名也无法使用CDN加速，所以，去掉了很多花里胡哨的功能\n","permalink":"https://deemoprobe.github.io/posts/blog/blog/","summary":"页面测试 由于访问速度问题，没有域名也无法使用CDN加速，所以，去掉了很多花里胡哨的功能","title":"Blog"},{"content":" William\u0026#39;s Blog 一个记录技术、阅读、生活的博客 👉友链格式 名称： William\u0026rsquo;s Blog 网址： https://YOUR_DOMAIN 图标： https://YOUR_DOMAIN/img/bear.gif 描述： 一个记录技术、阅读、生活的博客 👉友链申请要求 秉承互换友链原则、文章定期更新、不能有太多广告、个人描述字数控制在15字内\n👉Hugo博客交流群 YOUR_QQ\n","permalink":"https://deemoprobe.github.io/links/","summary":"William\u0026#39;s Blog 一个记录技术、阅读、生活的博客 👉友链格式 名称： William\u0026rsquo;s Blog 网址： https://YOUR_DOMAIN 图标： https://YOUR_DOMAIN/img/bear.gif 描述： 一个记录技术、阅读、生活的博客 👉友链申请要求 秉承互换友链原则、文章定期更新、不能有太多广告、个人描述字数控制在15字内 👉Hugo博客交流群 YOUR_QQ","title":"🤝友链"}]