[{"content":"概述 平台：VMware® Workstation 16 Pro nfs客户端IP：192.168.43.110 主机名为nfs-clinet nfs服务端IP：192.168.43.120 主机名为nfs-server CentOS-7版本启动NFS server之前，首先要启动RPC服务，完成NFS向RPC服务的注册。如果RPC服务重新启动，原来已经注册好的NFS端口数据就会丢失。因此，此时RPC服务管理的NFS程序也需要重新启动以重新向RPC注册。\nNFS 日期: 2023-04-30 \u0026nbsp; 标签: #linux\u0026nbsp; 概述 平台：VMware® Workstation 16 Pro nfs客户端IP：192.168.43.110 主机名为nfs-clinet nfs服务端IP：192.168.43.120 主机名为nfs-server CentOS-7版本启动NFS server之前，首先要启动RPC服务，完成NFS向RPC服务的注册。如 ...... 说明：一般修改NFS配置文件后，是不需要重启NFS的，直接在命令行执行systemctl reload nfs或exportfs -rv即可使修改的/etc/exports生效。\n注意：一台机器不要同时做NFS的服务端和NFS的客户端。如果同时作了服务端和客户端，那么在关机的时候，会一直卡住，可能十分钟之后甚至更久才能关闭成功。\nNFS 日期: 2023-04-30 \u0026nbsp; 标签: #linux\u0026nbsp; 概述 平台：VMware® Workstation 16 Pro nfs客户端IP：192.168.43.110 主机名为nfs-clinet nfs服务端IP：192.168.43.120 主机名为nfs-server CentOS-7版本启动NFS server之前，首先要启动RPC服务，完成NFS向RPC服务的注册。如 ...... ","permalink":"https://deemoprobe.github.io/posts/tech/kubernetes/kubernetesvolume/","summary":"概述 平台：VMware® Workstation 16 Pro nfs客户端IP：192.168.43.110 主机名为nfs-clinet nfs服务端IP：192.168.43.120 主机名为nfs-server CentOS-7版本启动NFS server之前，首先要启动RPC服务，完成NFS向RPC服务的注册。如","title":"KubernetesVolume"},{"content":"概述 平台：VMware® Workstation 16 Pro nfs客户端IP：192.168.43.110 主机名为nfs-clinet nfs服务端IP：192.168.43.120 主机名为nfs-server CentOS-7版本启动NFS server之前，首先要启动RPC服务，完成NFS向RPC服务的注册。如果RPC服务重新启动，原来已经注册好的NFS端口数据就会丢失。因此，此时RPC服务管理的NFS程序也需要重新启动以重新向RPC注册。\n说明：一般修改NFS配置文件后，是不需要重启NFS的，直接在命令行执行systemctl reload nfs或exportfs -rv即可使修改的/etc/exports生效。\n注意：一台机器不要同时做NFS的服务端和NFS的客户端。如果同时作了服务端和客户端，那么在关机的时候，会一直卡住，可能十分钟之后甚至更久才能关闭成功。\n安装NFS和RPC服务 服务端和客户端均安装\n# nfs-server和nfs-client均需要安装，此处以nfs-server为例 # 安装NFS和RPC [root@nfs-server ~]# yum install nfs-utils rpcbind -y # 查看 [root@nfs-server ~]# rpm -qa nfs-utils rpcbind nfs-utils-1.3.0-0.68.el7.2.x86_64 rpcbind-0.2.0-49.el7.x86_64 # 安装成功后自动创建了三个相关用户 [root@nfs-server ~]# tail -n 3 /etc/passwd rpc❌32:32:Rpcbind Daemon:/var/lib/rpcbind:/sbin/nologin rpcuser❌29:29:RPC Service User:/var/lib/nfs:/sbin/nologin nfsnobody❌65534:65534:Anonymous NFS User:/var/lib/nfs:/sbin/nologin 配置NFS服务端 [root@nfs-server ~]# mkdir /data [root@nfs-server ~]# chown -R nfsnobody.nfsnobody /data/ [root@nfs-server ~]# ll -d /data/ drwxr-xr-x 2 nfsnobody nfsnobody 6 Mar 16 14:29 /data/ # 配置nfs工作网段权限 [root@nfs-server ~]# vim /etc/exports /data 192.168.43.0/24(rw,sync) # 启动查看rpcbind [root@nfs-server ~]# systemctl start rpcbind [root@nfs-server ~]# rpcinfo -p localhost program vers proto port service 100000 4 tcp 111 portmapper 100000 3 tcp 111 portmapper 100000 2 tcp 111 portmapper 100000 4 udp 111 portmapper 100000 3 udp 111 portmapper 100000 2 udp 111 portmapper [root@nfs-server ~]# netstat -lntp | grep rpc tcp 0 0 0.0.0.0:111 0.0.0.0:* LISTEN 1597/rpcbind tcp6 0 0 :::111 :::* LISTEN 1597/rpcbind # 启动NFS [root@nfs-server ~]# systemctl start nfs [root@nfs-server ~]# ps -ef | grep nfs root 1659 2 0 14:34 ? 00:00:00 [nfsd] root 1660 2 0 14:34 ? 00:00:00 [nfsd] root 1661 2 0 14:34 ? 00:00:00 [nfsd] root 1662 2 0 14:34 ? 00:00:00 [nfsd] root 1663 2 0 14:34 ? 00:00:00 [nfsd] root 1664 2 0 14:34 ? 00:00:00 [nfsd] root 1665 2 0 14:34 ? 00:00:00 [nfsd] root 1666 2 0 14:34 ? 00:00:00 [nfsd] [root@nfs-server ~]# netstat -lntp | grep rpc tcp 0 0 0.0.0.0:111 0.0.0.0:* LISTEN 1597/rpcbind tcp 0 0 0.0.0.0:20048 0.0.0.0:* LISTEN 1651/rpc.mountd tcp 0 0 0.0.0.0:49275 0.0.0.0:* LISTEN 1627/rpc.statd tcp6 0 0 :::52143 :::* LISTEN 1627/rpc.statd tcp6 0 0 :::111 :::* LISTEN 1597/rpcbind tcp6 0 0 :::20048 :::* LISTEN 1651/rpc.mountd [root@nfs-server ~]# rpcinfo -p localhost program vers proto port service 100000 4 tcp 111 portmapper 100000 3 tcp 111 portmapper 100000 2 tcp 111 portmapper 100000 4 udp 111 portmapper 100000 3 udp 111 portmapper 100000 2 udp 111 portmapper 100024 1 udp 47016 status 100024 1 tcp 49275 status 100005 1 udp 20048 mountd 100005 1 tcp 20048 mountd 100005 2 udp 20048 mountd 100005 2 tcp 20048 mountd 100005 3 udp 20048 mountd 100005 3 tcp 20048 mountd 100003 3 tcp 2049 nfs 100003 4 tcp 2049 nfs 100227 3 tcp 2049 nfs_acl 100003 3 udp 2049 nfs 100227 3 udp 2049 nfs_acl 100021 1 udp 52915 nlockmgr 100021 3 udp 52915 nlockmgr 100021 4 udp 52915 nlockmgr 100021 1 tcp 43557 nlockmgr 100021 3 tcp 43557 nlockmgr 100021 4 tcp 43557 nlockmgr # 加入开机启动项 [root@nfs-server ~]# systemctl enable nfs [root@nfs-server ~]# systemctl enable rpcbind # 确认两个服务的状态 [root@nfs-server ~]# systemctl status rpcbind ● rpcbind.service - RPC bind service Loaded: loaded (/usr/lib/systemd/system/rpcbind.service; enabled; vendor preset: enabled) Active: active (running) since Wed 2022-03-16 14:31:15 CST; 6min ago Main PID: 1597 (rpcbind) CGroup: /system.slice/rpcbind.service └─1597 /sbin/rpcbind -w Mar 16 14:31:15 cicd-dest systemd[1]: Starting RPC bind service... Mar 16 14:31:15 cicd-dest systemd[1]: Started RPC bind service. [root@nfs-server ~]# systemctl status nfs ● nfs-server.service - NFS server and services Loaded: loaded (/usr/lib/systemd/system/nfs-server.service; enabled; vendor preset: disabled) Drop-In: /run/systemd/generator/nfs-server.service.d └─order-with-mounts.conf Active: active (exited) since Wed 2022-03-16 14:34:42 CST; 2min 58s ago Main PID: 1654 (code=exited, status=0/SUCCESS) CGroup: /system.slice/nfs-server.service Mar 16 14:34:41 cicd-dest systemd[1]: Starting NFS server and services... Mar 16 14:34:42 cicd-dest systemd[1]: Started NFS server and services. # 查看nfs相关配置参数 [root@nfs-server ~]# cat /var/lib/nfs/etab /data 192.168.43.0/24(rw,sync,wdelay,hide,nocrossmnt,secure,root_squash,no_all_squash,no_subtree_check,secure_locks,acl,no_pnfs,anonuid=65534,anongid=65534,sec=sys,rw,secure,root_squash,no_all_squash) 参数说明:\nro：只读设置,这样 NFS 客户端只能读、不能写(默认设置) rw：读写设置,NFS 客户端可读写 sync：将数据同步写入磁盘中,效率低,但可以保证数据的一致性(默认设置) async：将数据先保存在内存缓冲区中,必要时才写入磁盘;如果服务器重新启动,这种行为可能会导致数据损坏,但效率高 root_squash：当客户端用 root 用户访问该共享文件夹时,将 root 用户映射成匿名用户(默认设置) no_root_squash：客户端的 root 用户不映射.这样客户端的 root 用户与服务端的 root 用户具有相同的访问权限,这可能会带来严重的安全影响.没有充分的理由,不应该指定此选项 all_squash：客户端所有普通用户及所属组都映射为匿名用户及匿名用户组;(推荐设置) no_all_squash：客户端所有普通用户及所属组不映射(默认设置) subtree_check：如果共享,如:/usr/bin之类的子目录时,强制NFS检查父目录的权限 no_subtree_check：即使共享 NFS 服务端的子目录时,nfs服务端也不检查其父目录的权限,这样可以提高效率(默认设置) secure：限制客户端只能从小于1024的tcp/ip端口连接nfs服务器(默认设置) insecure：允许客户端从大于1024的tcp/ip端口连接服务器 wdelay：检查是否有相关的写操作,如果有则将这些写操作一起执行,这样可以提高效率(默认设置) no_wdelay：若有写操作则立即执行,当使用async时,无需此设置 anonuid=xxx：将远程访问的所有用户主都映射为匿名用户主账户,并指定该匿名用户主为本地用户主(UID=xxx) anongid=xxx：将远程访问的所有用户组都映射为匿名用户组账户,并指定该匿名用户组为本地用户组(GID=xxx) # 查看服务端NFS已经开放/data 192.168.43.120为服务端IP [root@nfs-server ~]# showmount -e 192.168.43.120 Export list for 192.168.43.120: /data 192.168.43.0/24 # 防火墙永久开放 nfs rpc-bind mountd 三个服务, 临时开放把--permanent去掉即可(重启失效) # 若不开放, 客户端访问时会报错, 类似clnt_create: RPC: Port mapper failure - Unable to receive: errno 113 (No route to host) # 重启后, showmount若报错\u0026#34;clnt_create: RPC: Program not registered\u0026#34;, 可尝试先重启rpcbind再重启nfs [root@nfs-server ~]# firewall-cmd --add-service=nfs --permanent success [root@nfs-server ~]# firewall-cmd --add-service=rpc-bind --permanent success [root@nfs-server ~]# firewall-cmd --add-service=mountd --permanent success # 重新加载防火墙规则使之生效 [root@nfs-server ~]# firewall-cmd --reload success 配置NFS客户端 # 启动rpcbind [root@k8s-node02 ~]# systemctl start rpcbind # 查看服务端NFS服务共享信息, 192.168.43.120为服务端IP # 如果报错no route, 检查是否开放服务或是否reload防火墙 # 若配置更新还不能访问, 尝试重启服务端nfs和rpcbind即可 [root@k8s-node02 ~]# showmount -e 192.168.43.120 Export list for 192.168.43.120: /data 192.168.43.0/24 # 挂载NFS并查看 [root@k8s-node02 ~]# mount -t nfs 192.168.43.120:/data /mnt [root@k8s-node02 ~]# df -h | grep mnt 192.168.43.120:/data 17G 1.8G 16G 11% /mnt # 推荐用下面命令查看挂载, 比较详细 [root@k8s-node02 ~]# cat /proc/mounts | grep mnt 192.168.43.120:/data /mnt nfs4 rw,relatime,vers=4.1,rsize=131072,wsize=131072,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=192.168.43.110,local_lock=none,addr=192.168.43.120 0 0 测试 # 1.客户端创建文件夹或创建文件并且输入数据，在服务端查看同步结果 # 客户端 [root@k8s-node02 ~]# cd /mnt/ [root@k8s-node02 mnt]# echo \u0026#34;Message from nfs-client\u0026#34; \u0026gt; client.log [root@k8s-node02 mnt]# mkdir client # 服务端 [root@nfs-server ~]# cd /data/ [root@nfs-server data]# ls client client.log [root@nfs-server data]# cat client.log Message from nfs-client # 2.服务端创建文件夹或创建文件并且输入数据,在客户端查看 # 服务端 [root@nfs-server data]# mkdir server [root@nfs-server data]# echo \u0026#34;Message from nfs-server\u0026#34; \u0026gt; server.log # 客户端 [root@k8s-node02 mnt]# ls client client.log server server.log [root@k8s-node02 mnt]# cat server.log Message from nfs-server 重启自动挂载 配置客户端重启自动挂载NFS，若是下面方式1，必须保证/etc/rc.d/rc.local文件具有可执行权限，否则该脚本不会执行也不会生效。推荐方式2。\n# 开机自启动方式1, 挂载信息写入系统启动加载文件 [root@k8s-node02 mnt]# ll /etc/rc.local lrwxrwxrwx. 1 root root 13 Mar 5 15:12 /etc/rc.local -\u0026gt; rc.d/rc.local [root@k8s-node02 mnt]# ll /etc/rc.d/rc.local -rw-r--r--. 1 root root 473 Jan 14 00:54 /etc/rc.d/rc.local [root@k8s-node02 mnt]# chmod +x /etc/rc.d/rc.local [root@k8s-node02 mnt]# vim /etc/rc.local # 最后一行加入 mount -t nfs 192.168.43.120:/data /mnt # 开机自启动方式2, 写入挂载文件（推荐） [root@k8s-node02 mnt]# vim /etc/fstab # 最后一行加入 192.168.43.120:/data /mnt nfs defaults 0 0 问题分析 nfs相关服务加入了开机自启动，当重启NFS客户端机器后，如果此时NFS服务端机器已关机，或者网络存在问题(网络断连或IP变更)等等。使NFS客户端连接NFS服务端失败，那么此时会造成NFS客户端机器启动很慢（因为检查/etc/rc.local或/etc/fstab挂载无法找到目标服务端机器）的情况。为了避免该情况发生，不建议机器开机自启动就挂载NFS。\n如果一台机器必须挂载 NFS，可以做监控。当该机器未挂载 NFS 时就告警，然后使用命令mount -t nfs 192.168.43.120:/data /mnt去手动挂载。\n当然如果实际环境中NFS服务极其稳定，且几乎不再改变NFS服务端IP地址，那么此时也可以加入开机自启动。具体问题具体分析。\n","permalink":"https://deemoprobe.github.io/posts/tech/linux/nfs/","summary":"概述 平台：VMware® Workstation 16 Pro nfs客户端IP：192.168.43.110 主机名为nfs-clinet nfs服务端IP：192.168.43.120 主机名为nfs-server CentOS-7版本启动NFS server之前，首先要启动RPC服务，完成NFS向RPC服务的注册。如","title":"NFS"},{"content":"Service是一种Pod的服务暴露机制，其他Pod可以通过这个Service访问到这个Service代理的一个或一组Pod。\n带有selector的Service在创建后会同时创建一个同名的Endpoints实例，Endpoints实例记录了selector匹配的Pod的IP地址和端口。当Pod被重建后，Endpoints会被更新，而我们使用Service名称连接后端Pod，IP变动不会对服务造成影响。\n# 集群初始化后的Service和Endpoint [root@k8s-master01 yamls]# kubectl get svc | grep kubernetes kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 8d [root@k8s-master01 yamls]# kubectl get ep | grep kubernetes kubernetes 192.168.43.201:6443,192.168.43.202:6443,192.168.43.203:6443 8d 定义 假设存在一个或一组打有\u0026quot;app=myapp\u0026quot;标签的pod，Pod对外暴露的端口是8001，那么可以创建Service去代理这些Pod\n# 单个端口代理示例 apiVersion: v1 kind: Service metadata: name: my-service spec: selector: app: myapp # 选择带有该标签的Pod ports: - protocol: TCP # UDP TCP SCTP default: TCP port: 8080 # Service自己的端口 targetPort: 8001 # 后端应用Pod暴露的端口 # name: http # Service端口的名称 # 多个端口代理示例 apiVersion: v1 kind: Service metadata: name: my-service spec: selector: app: myapp ports: - name: http protocol: TCP port: 80 targetPort: 8001 - name: https protocol: TCP port: 443 targetPort: 8002 # 无selector的Service apiVersion: v1 kind: Service metadata: name: my-service spec: ports: - protocol: TCP port: 80 targetPort: 8001 # 由于没有selector，该Service创建后不会自动创建Endpoints实例，需要手动创建 apiVersion: v1 kind: Endpoints metadata: name: my-service subnets: - addresses: - ip: 1.2.3.4 ports: - port: 8001 # ExternalName Service，没有selector没有Endpoints，仅使用别名解析Service apiVersion: v1 kind: Service metadata: name: my-service namespace: dev spec: type: ExternalName externalName: my.nginx.example.com 说明：Service 能够将一个接收 port 映射到任意的 targetPort。 默认情况下，targetPort 将被设置为与 port 字段相同的值。\nHeadless Service 有时不需要或不想要负载均衡，以及单独的 Service IP。可以通过指定spec下面的clusterIP: None来创建 Headless Service。\n无头服务并不会分配 Cluster IP，kube-proxy 不会处理它们，而且平台也不会为它们进行负载均衡和路由。DNS如何实现自动配置，依赖于 Service 是否定义了selector。\n对于定义了selector的无头服务，Endpoint 控制器在 API 中创建了 Endpoints 记录，并且修改 DNS 配置返回 A 记录（IP 地址），通过这个地址直接到达 Service 的后端 Pod 上。\n对于没有定义selector的无头服务，Endpoint 控制器不会创建 Endpoints 记录。DNS系统会查找和配置：\n对于 ExternalName 类型的服务，查找其 CNAME 记录 对所有其他类型的服务，查找与 Service 名称相同的任何 Endpoints 的记录 Service发布类型 Kubernetes 允许指定你所需要的 Service 类型，默认是 ClusterIP，可以自定义spec字段下的type字段。type的取值以及行为如下：\nClusterIP：通过集群的内部 IP 暴露服务，选择该值时服务只能够在集群内部访问。 NodePort：通过每个节点上的 IP 和静态端口（NodePort）暴露服务。NodePort服务会路由到自动创建的ClusterIP服务。通过请求\u0026lt;节点 IP\u0026gt;:\u0026lt;节点端口\u0026gt;，NodePort端口范围默认是30000-32767。可以从集群的外部访问。 LoadBalancer：使用云提供商的负载均衡器向外部暴露服务。外部负载均衡器可以将流量路由到自动创建的NodePort服务和ClusterIP服务上。 ExternalName：通过返回CNAME（别名）和对应值，可以将服务映射到 externalName 字段的内容（例如，foo.bar.example.com）。无需创建任何类型代理。 说明：kube-dns 1.7 及以上版本或者 CoreDNS 0.0.8 及以上版本才能使用 ExternalName 类型。\n# ClusterIP类型，type可以不指定 apiVersion: v1 kind: Service metadata: name: my-service spec: selector: app: myapp ports: # 默认情况下，为了方便起见，`targetPort` 被设置为与 `port` 字段相同的值。 - port: 80 targetPort: 80 # NodePort类型 apiVersion: v1 kind: Service metadata: name: my-service spec: type: NodePort selector: app: myapp ports: # 默认情况下，为了方便起见，`targetPort` 被设置为与 `port` 字段相同的值。 - port: 80 targetPort: 80 # 可选字段，不指定的话会自动设置为30000-32767中的一个 nodePort: 30007 # LoadBalancer类型，status.loadBalancer字段指定云厂商提供的负载均衡地址 apiVersion: v1 kind: Service metadata: name: my-service spec: selector: app: MyApp ports: - protocol: TCP port: 80 targetPort: 8001 clusterIP: 10.96.0.239 type: LoadBalancer status: loadBalancer: ingress: - ip: 192.0.2.127 # ExternalName类型 # 访问nginx-externalname.test.svc.cluster.local被重定向到www.baidu.com apiVersion: v1 kind: Service metadata: labels: app: nginx-externalname name: nginx-externalname namespace: test spec: type: ExternalName externalName: www.baidu.com 外部IP 如果外部IP路由到集群中一个或多个Node上，Kubernetes Service 会被暴露给这些externalIP。通过外部 IP（作为目的 IP 地址）进入到集群，流量将会被路由到 Service 的 Endpoint 上。根据 Service 的规定，externalIPs 可以同任意的 ServiceType 来一起指定。 在下面的例子中，my-service 可以通过 \u0026ldquo;80.11.12.10:80\u0026rdquo;(externalIP:port)被客户端访问。\napiVersion: v1 kind: Service metadata: name: my-service spec: selector: app: myapp ports: - name: http protocol: TCP port: 80 targetPort: 8001 externalIPs: - 80.11.12.10 Service代理模式 在 Kubernetes 集群中，kube-proxy负责为Service实现了一种VIP形式的代理，而不是ExternalName的形式。这种VIP形式的代理主要有下面两种实现方式，现在kubernetes推荐使用ipvs\niptables代理模式 在iptables模式中，kube-proxy 监视 Kubernetes的Service和Endpoints的状态变化（创建或删除），对每个 Service它都会创建iptables规则，从而捕获到达该 Service 的ClusterIP和端口的请求，进而将请求重定向到 Service 后端中的某个 Pod 上面。 对于每个 Endpoints 对象，它也会创建iptables规则，这个规则会选择一个后端pod。\n默认的策略是，kube-proxy 在 iptables 模式下随机选择一个后端。如果要确保每次都将来自特定客户端的连接传递到同一Pod（即会话保持），则可以通过设置亲和力调度策略service.spec.sessionAffinity: ClientIP（默认值是 None），来基于客户端的 IP 地址选择会话关联。还可以通过设置service.spec.sessionAffinityConfig.clientIP.timeoutSeconds来配置最大会话停留时间。（默认值为 10800 秒，即 3 小时）\nIPVS代理模式 在ipvs模式中，kube-proxy 监视 Kubernetes的Service和Endpoints的状态变化（创建或删除），调用netlink接口创建 IPVS 规则，并定期将规则与Kubernetes的Service和Endpoints同步。确保IPVS状态与所需状态匹配。访问服务时，IPVS根据规则将流量定向到后端Pod之一\n类似iptables，IPVS基于netfilter钩子函数，但是使用哈希表作为基础数据结构，在内核空间中工作，内核级的转发，所以速度很快。\nIPVS 提供以下选项来实现负载均衡：\nrr：轮询（Round-Robin） lc：最少链接（Least Connection），即打开链接数量最少者优先 dh：目标地址哈希（Destination Hashing） sh：源地址哈希（Source Hashing） sed：最短预期延迟（Shortest Expected Delay） nq：从不排队（Never Queue） 说明：要在 IPVS 模式下运行 kube-proxy，必须在启动 kube-proxy 之前使 IPVS 在节点上可用（启用IPVS可以参考高可用集群安装文档 当kube-proxy 以 IPVS 代理模式启动时，它将验证 IPVS 内核模块是否可用。 如果未检测到 IPVS 内核模块，则 kube-proxy 将退回到以 iptables 代理模式运行。\n选择合适的模式 iptables由于线性查找匹配、全量更新等特点，当规则很多时，性能会比ipvs差，所以一般推荐选择ipvs\niptables工具基于Linux内核的Netfilter模块，默认定义了多张表。每张表里包含若干内置链（chain），也可能包含用户自定义的链。每条链是一套规则（rule）列表，用于匹配一组数据包。每条规则都指定如何处理匹配的数据包。\ntable：包含一组chain的表\nchain：包含一组rule的链\nrule：匹配数据包的规则，例如：协议，端口号\ntarget：规则中的具体行为，例如：ACCEPT，DROP，INPUT，FORWARD，OUTPUT\n表\nfilter：默认表，通用数据包过滤表 mangle：为特定的数据包设计 nat：针对创建新连接的数据包 raw：主要用于结合 NOTRACK target 配置连接跟踪的豁免 security：用于MAC（Mandatory Access Control，强制访问控制）规则 规则链\nINPUT：以本机为目标的入口数据包规则链 OUTPUT：本机产生，向外转发的数据包规则链 FORWARD：路由经过本机的数据包规则链 PREROUTING：数据包进入路由之前的规则链 POSTROUTING：数据包发送到目标前（出路由）的规则链 IPtables处理链接的算法复杂度为O(n)，IPVS为O(1)，Service规模在1000内二者差距并不是很大。在Service规模超过1000后，IPtables规则链达到2000以上，性能开始下降，响应时间成倍数增加，IPVS则几乎不受Service规模的影响。Calico虽然也采用IPtables技术，但对规则链进行了优化，算法复杂度也达到了了O(1)的水平。除了大规模服务性能的差距，IPVS还具备复杂均衡算法全（轮询、最小连接数、哈希值、最小延迟等）、支持健康检查等优点。\n流量转发策略 外部流量 可以通过设置spec.externalTrafficPolicy字段来控制来自于外部的流量是如何路由的。可选值有Cluster和Local。字段设为Cluster会将外部流量路由到所有就绪的Endpoint，设为Local只会路由到当前节点上就绪的Endpoint。如果流量策略设置为Local，而当前节点上没有就绪的Endpoint，kube-proxy不会转发请求相关服务的任何流量。\n内部流量 可以通过设置spec.internalTrafficPolicy字段来控制内部来源的流量是如何转发的。可设置的值有Cluster和Local。将字段设置为Cluster会将内部流量路由到所有就绪Endpoint，设置为Local只会路由到当前节点上就绪的Endpoint。如果流量策略是Local，而当前节点上没有就绪的Endpoint，那么 kube-proxy 会丢弃流量。\n服务发现 Kubernetes 支持两种基本的服务发现模式 —— 环境变量和 DNS。\n环境变量: Pod创建的时候，服务的ip和port等信息会以环境变量的形式注入到pod里 DNS: Service创建成功后，会在dns服务器里导入一些记录，想要访问某个服务，通过dns服务器解析出对应的ip和port，从而实现服务访问 基于环境变量服务发现 当 Pod 运行在 Node 上，kubelet 会为每个活跃的 Service 添加一组环境变量。 支持 {SVCNAME}_SERVICE_HOST 和 {SVCNAME}_SERVICE_PORT 等变量。这里 Service 的名称需大写，横线被转换成下划线。\n例如有一个名称为 nginx-service 的 Service 暴露了 TCP 端口8180，同时给它分配了Cluster IP地址 10.96.0.11，这个 Service 生成了如下环境变量：\nNGINX_SERVICE_SERVICE_HOST=10.96.0.11 NGINX_SERVICE_PORT_8180_TCP_ADDR=10.96.0.11 KUBERNETES_PORT_443_TCP_PROTO=tcp NGINX_SERVICE_PORT_8180_TCP_PORT=8180 NGINX_SERVICE_PORT_8180_TCP_PROTO=tcp NGINX_SERVICE_SERVICE_PORT=8180 NGINX_SERVICE_PORT=tcp://10.96.0.11:8180 NGINX_SERVICE_PORT_8180_TCP=tcp://10.96.0.11:8180 # 实例 [root@k8s-master01 ~]# mkdir network [root@k8s-master01 ~]# cd network/ [root@k8s-master01 network]# vim nginx-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deploy labels: app: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.18.0 ports: - containerPort: 80 # 配置Service [root@k8s-master01 network]# vim nginx-service.yaml apiVersion: v1 kind: Service metadata: name: nginx-service labels: name: nginx-service spec: ports: - port: 8180 targetPort: 80 selector: app: nginx [root@k8s-master01 network]# kubectl apply -f nginx-deploy.yaml deployment.apps/nginx-deploy created [root@k8s-master01 network]# kubectl apply -f nginx-service.yaml service/nginx-service created [root@k8s-master01 network]# kubectl get deployments.apps,svc NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-deploy 2/2 2 2 40s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/nginx-service ClusterIP 10.105.254.62 \u0026lt;none\u0026gt; 8180/TCP 33s [root@k8s-master01 network]# kubectl get po NAME READY STATUS RESTARTS AGE busybox 1/1 Running 3 (38m ago) 4h46m look-svc-env 0/1 Completed 0 18s [root@k8s-master01 network]# vim look-svc-env.yaml apiVersion: v1 kind: Pod metadata: name: look-svc-env spec: containers: - name: look-svc-env image: busybox command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;env\u0026#34;] [root@k8s-master01 network]# kubectl apply -f look-svc-env.yaml pod/look-svc-env created # 可以查看到nginx-service环境变量已经写入 [root@k8s-master01 network]# kubectl logs look-svc-env KUBERNETES_PORT=tcp://10.96.0.1:443 KUBERNETES_SERVICE_PORT=443 HOSTNAME=look-svc-env SHLVL=1 HOME=/root KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1 PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin NGINX_SERVICE_SERVICE_HOST=10.105.254.62 KUBERNETES_PORT_443_TCP_PORT=443 NGINX_SERVICE_PORT_8180_TCP_ADDR=10.105.254.62 KUBERNETES_PORT_443_TCP_PROTO=tcp NGINX_SERVICE_PORT_8180_TCP_PORT=8180 NGINX_SERVICE_PORT_8180_TCP_PROTO=tcp NGINX_SERVICE_PORT=tcp://10.105.254.62:8180 NGINX_SERVICE_SERVICE_PORT=8180 KUBERNETES_SERVICE_PORT_HTTPS=443 KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443 KUBERNETES_SERVICE_HOST=10.96.0.1 PWD=/ NGINX_SERVICE_PORT_8180_TCP=tcp://10.105.254.62:8180 # nginx-service停掉后，nginx-service相关的环境变量消失 基于DNS服务发现 支持集群的 DNS 服务器（例如 CoreDNS）监视 Kubernetes API 中的Service，并为每个Service创建一组 DNS 记录。如果在整个集群中都启用了DNS，则所有 Pod 都应该能够通过其 DNS 名称自动解析服务。\n例如，如果你在 Kubernetes 命名空间my-ns中有一个名为my-service的Service，则控制节点和DNS服务共同为my-service.my-ns创建DNS记录。my-ns命名空间中的Pod可以直接用名称my-service来找到服务（当然使用my-service.my-ns也可以）。\nKubernetes 从 v1.11 开始可以使用 CoreDNS 来提供命名服务，并从 v1.13 开始成为默认 DNS 服务。CoreDNS 的特点是效率更高，资源占用率更小，推荐使用 CoreDNS 替代 kube-dns 为集群提供 DNS 服务。CoreDNS基本架构如下:\n如果需要定制DNS服务,可参考下面官方给的方案:https://kubernetes.io/zh/docs/tasks/administer-cluster/dns-custom-nameservers/\nPod DNS DNS策略可以逐个Pod来设定。目前 Kubernetes 支持以下特定 Pod 的 DNS 策略。这些策略可以在Pod的spec.dnsPolicy字段设置：\n\u0026ldquo;Default\u0026rdquo;: Pod 从运行所在的节点继承名称解析配置 \u0026ldquo;ClusterFirst\u0026rdquo;: 与配置的集群域后缀不匹配的任何 DNS 查询（例如 \u0026ldquo;www.kubernetes.io\u0026rdquo;） 都将转发到从节点继承的上游名称服务器。集群管理员可能配置了额外的存根域和上游 DNS 服务器 \u0026ldquo;ClusterFirstWithHostNet\u0026rdquo;：对于以 hostNetwork 方式运行的 Pod，应显式设置其 DNS 策略 \u0026ldquo;ClusterFirstWithHostNet\u0026rdquo; \u0026ldquo;None\u0026rdquo;: 此设置允许 Pod 忽略 Kubernetes 环境中的 DNS 设置。Pod 会使用其dnsConfig字段所提供的 DNS 设置 说明： \u0026ldquo;Default\u0026rdquo; 不是默认的 DNS 策略。如果未明确指定 dnsPolicy，则使用 \u0026ldquo;ClusterFirst\u0026rdquo;。\nPod 的 DNS 配置可让用户对 Pod 的 DNS 设置进行更多控制。\ndnsConfig 字段是可选的，它可以与任何 dnsPolicy 设置一起使用。 但是，当 Pod 的 dnsPolicy 设置为 \u0026ldquo;None\u0026rdquo; 时，必须指定 dnsConfig 字段。\n用户可以在 dnsConfig 字段中指定以下属性：\nnameservers：将用作于 Pod 的 DNS 服务器的 IP 地址列表。 最多可以指定 3 个 IP 地址。当 Pod 的 dnsPolicy 设置为 \u0026ldquo;None\u0026rdquo; 时， 列表必须至少包含一个 IP 地址，否则此属性是可选的 searches：用于在 Pod 中查找主机名的 DNS 搜索域的列表。此属性是可选的, Kubernetes 最多允许 6 个搜索域 options：可选的对象列表，其中每个对象可能具有 name 属性（必需）和 value 属性（可选） # 创建自定义DNS的Pod [root@k8s-master01 network]# vim pod_dns.yaml apiVersion: v1 kind: Pod metadata: name: nginx labels: app: nginx spec: containers: - name: nginx image: nginx:1.18.0 ports: - containerPort: 80 dnsPolicy: \u0026#34;None\u0026#34; dnsConfig: nameservers: - 8.8.8.8 searches: - default.svc.cluster-domain.example - cluster-domain.example options: - name: pod_num value: \u0026#34;1\u0026#34; [root@k8s-master01 network]# kubectl apply -f pod_dns.yaml pod/nginx created # 查看DNS信息 [root@k8s-master01 network]# kubectl exec nginx -- cat /etc/resolv.conf nameserver 8.8.8.8 search default.svc.cluster-domain.example cluster-domain.example options pod_num:1 # Pod访问验证 # 创建Service来暴露Pod [root@k8s-master01 network]# cat nginx-service.yaml apiVersion: v1 kind: Service metadata: name: nginx-service labels: name: nginx-service spec: ports: - port: 8180 targetPort: 80 selector: app: nginx [root@k8s-master01 network]# kubectl apply -f nginx-service.yaml service/nginx-service created [root@k8s-master01 network]# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-service ClusterIP 10.97.188.44 \u0026lt;none\u0026gt; 8180/TCP 75s [root@k8s-master01 network]# kubectl exec busybox -- nslookup 10.97.188.44 Server: 10.96.0.10 Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local Name: 10.97.188.44 Address 1: 10.97.188.44 nginx-service.default.svc.cluster.local [root@k8s-master01 network]# kubectl exec busybox -- wget -q -O- nginx-service.default.svc.cluster.local:8180 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; ","permalink":"https://deemoprobe.github.io/posts/tech/kubernetes/kubernetesservicediscovery/","summary":"Service是一种Pod的服务暴露机制，其他Pod可以通过这个Service访问到这个Service代理的一个或一组Pod。 带有selector的Service在创建后会同时创建一个同名的Endpoints实例，Endpoints实例记录了selector匹配的Pod的IP地址","title":"KubernetesServiceDiscovery"},{"content":" 使用Helm部署K8S资源指标获取工具: metrics-server\n项目地址: https://github.com/kubernetes-sigs/metrics-server\n# 默认情况下下面资源指标是无法获取的 [root@k8s-master ingress]# kubectl top node error: Metrics API not available [root@k8s-master ingress]# kubectl top pod error: Metrics API not available 获取metrics-server-amd64镜像\n# 在集群所有节点都需要执行 docker pull registry.cn-beijing.aliyuncs.com/google_registry/metrics-server-amd64:v0.3.6 docker tag registry.cn-beijing.aliyuncs.com/google_registry/metrics-server-amd64:v0.3.6 k8s.gcr.io/metrics-server-amd64:v0.3.6 docker rmi registry.cn-beijing.aliyuncs.com/google_registry/metrics-server-amd64:v0.3.6 配置metrics-server.yaml文件\n[root@k8s-master kubernetes]# mkdir monitor;cd monitor [root@k8s-master monitor]# vi metrics-server.yaml args: - --logtostderr - --kubelet-insecure-tls - --kubelet-preferred-address-types=InternalIP 通过helm部署metrics-server\n# 添加官方最新的Helm仓库 [root@k8s-master monitor]# helm repo add stable https://charts.helm.sh/stable \u0026#34;stable\u0026#34; has been added to your repositories [root@k8s-master monitor]# helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026#34;apphub\u0026#34; chart repository ...Successfully got an update from the \u0026#34;stable\u0026#34; chart repository Update Complete. ⎈Happy Helming!⎈ [root@k8s-master monitor]# helm repo list NAME URL apphub https://apphub.aliyuncs.com stable https://charts.helm.sh/stable # 先搜一波 [root@k8s-master monitor]# helm search repo stable | grep metrics-server stable/metrics-server 2.11.4 0.3.6 DEPRECATED - Metrics Server is a cluster-wide a... # 安装 [root@k8s-master monitor]# helm install metrics-server stable/metrics-server --namespace kube-system -f metrics-server.yaml NAME: metrics-server LAST DEPLOYED: Tue Jan 12 15:14:57 2021 NAMESPACE: kube-system STATUS: deployed REVISION: 1 NOTES: The metric server has been deployed. In a few minutes you should be able to list metrics using the following command: kubectl get --raw \u0026#34;/apis/metrics.k8s.io/v1beta1/nodes\u0026#34; # 查看Metrics-Server相关资源 [root@k8s-master monitor]# helm list --namespace=kube-system NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION metrics-server kube-system 1 2021-01-12 15:14:57.092114352 +0800 CST deployed metrics-server-2.11.4 0.3.6 [root@k8s-master monitor]# kubectl get deploy -A | grep \u0026#39;metrics-server\u0026#39; kube-system metrics-server 1/1 1 1 99s [root@k8s-master monitor]# kubectl get pod -A | grep \u0026#39;metrics-server\u0026#39; kube-system metrics-server-68cc74d4d-7cj44 1/1 Running 0 77s # 获取node节点资源占用情况 [root@k8s-master monitor]# kubectl get --raw \u0026#34;/apis/metrics.k8s.io/v1beta1/nodes\u0026#34; {\u0026#34;kind\u0026#34;:\u0026#34;NodeMetricsList\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;metrics.k8s.io/v1beta1\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;selfLink\u0026#34;:\u0026#34;/apis/metrics.k8s.io/v1beta1/nodes\u0026#34;},\u0026#34;items\u0026#34;:[{\u0026#34;metadata\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;k8s-master\u0026#34;,\u0026#34;selfLink\u0026#34;:\u0026#34;/apis/metrics.k8s.io/v1beta1/nodes/k8s-master\u0026#34;,\u0026#34;creationTimestamp\u0026#34;:\u0026#34;2021-01-12T07:18:02Z\u0026#34;},\u0026#34;timestamp\u0026#34;:\u0026#34;2021-01-12T07:17:40Z\u0026#34;,\u0026#34;window\u0026#34;:\u0026#34;30s\u0026#34;,\u0026#34;usage\u0026#34;:{\u0026#34;cpu\u0026#34;:\u0026#34;1395283488n\u0026#34;,\u0026#34;memory\u0026#34;:\u0026#34;1246356Ki\u0026#34;}},{\u0026#34;metadata\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;k8s-node1\u0026#34;,\u0026#34;selfLink\u0026#34;:\u0026#34;/apis/metrics.k8s.io/v1beta1/nodes/k8s-node1\u0026#34;,\u0026#34;creationTimestamp\u0026#34;:\u0026#34;2021-01-12T07:18:02Z\u0026#34;},\u0026#34;timestamp\u0026#34;:\u0026#34;2021-01-12T07:17:37Z\u0026#34;,\u0026#34;window\u0026#34;:\u0026#34;30s\u0026#34;,\u0026#34;usage\u0026#34;:{\u0026#34;cpu\u0026#34;:\u0026#34;224689267n\u0026#34;,\u0026#34;memory\u0026#34;:\u0026#34;997016Ki\u0026#34;}}]} # 查看node节点资源使用情况 [root@k8s-master monitor]# kubectl top node NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% k8s-master 1396m 69% 1217Mi 70% k8s-node1 225m 11% 973Mi 36% # 查看所有Pod的资源使用情况 [root@k8s-master monitor]# kubectl top po -A NAMESPACE NAME CPU(cores) MEMORY(bytes) default busybox-k8s-master 0m 0Mi default high-priority 1m 1Mi default look-svc-env-k8s-master 0m 0Mi default myapp-deploy1-6c468d6b6c-nr28d 0m 1Mi default myapp-deploy1-6c468d6b6c-w69sz 0m 1Mi default nginx-k8s-master 0m 1Mi default pod-dns-k8s-master 0m 1Mi dev my-nginx-7c4ff94949-9c9m2 0m 1Mi dev myapp-deploy1-6c468d6b6c-cq44z 0m 1Mi dev myapp-deploy1-6c468d6b6c-mzqwd 0m 1Mi dev myapp-deploy2-5fffdcccd5-25qz4 0m 1Mi dev myapp-deploy2-5fffdcccd5-vpcj9 0m 1Mi dev nginx-ingress-nginx-ingress-controller-667cb64f9c-zqg2q 4m 100Mi dev nginx-ingress-nginx-ingress-controller-default-backend-7ccrfv9m 1m 2Mi kube-system calico-kube-controllers-5c6f6b67db-4gwqx 11m 18Mi kube-system calico-node-g5r6h 65m 96Mi kube-system calico-node-tw24f 76m 61Mi kube-system coredns-6d56c8448f-m92h8 14m 20Mi kube-system coredns-6d56c8448f-wh66t 6m 11Mi kube-system etcd-k8s-master 49m 60Mi kube-system kube-apiserver-k8s-master 161m 472Mi kube-system kube-controller-manager-k8s-master 54m 70Mi kube-system kube-proxy-2ld6t 3m 14Mi kube-system kube-proxy-bx9jg 1m 18Mi kube-system kube-scheduler-k8s-master 13m 25Mi kube-system metrics-server-68cc74d4d-7cj44 4m 11Mi metallb-system controller-fb659dc8-c9r7p 3m 7Mi metallb-system speaker-dwl4w 8m 13Mi metallb-system speaker-ptc97 10m 13Mi test my-nginx-7c4ff94949-n5jh8 0m 3Mi test nginxingress-nginx-ingress-controller-cddb9cd67-nddz9 4m 95Mi test nginxingress-nginx-ingress-default-backend-6d7d9db4d7-dwl7n 1m 1Mi ","permalink":"https://deemoprobe.github.io/posts/tech/kubernetes/kubernetesmetrics/","summary":"使用Helm部署K8S资源指标获取工具: metrics-server 项目地址: https://github.com/kubernetes-sigs/metrics-server # 默认情况下下面资源指标是无法获取的 [root@k8s-master ingress]# kubectl top node error: Metrics API not available [root@k8s-master ingress]# kubectl top pod error: Metrics API not available 获取metrics-server-amd64镜像 # 在集群所有节点都需要执行 docker pull registry.cn-beijing.aliyuncs.com/google_registry/metrics-server-amd64:v0.3.6 docker tag registry.cn-beijing.aliyuncs.com/google_registry/metrics-server-amd64:v0.3.6 k8s.gcr.io/metrics-server-amd64:v0.3.6 docker rmi registry.cn-beijing.aliyuncs.com/google_registry/metrics-server-amd64:v0.3.6 配置metrics-server.yaml文件 [root@k8s-master kubernetes]# mkdir monitor;cd monitor","title":"KubernetesMetrics"},{"content":"Pod状态分析 状态 说明 Pending（挂起） Pod已被Kubernetes系统接收，但仍有一个或多个容器未被创建，可以通过kubectl describe查看原因 Running（运行中） Pod已被调度到某一节点上，内部所有容器已被创建，而且至少有一个处于运行状态，或者是正在启动或重启，可以kubectl logs查看pod日志 Succeeded（成功） 所有容器执行成功并终止，且不会重启，可以kubectl logs查看运行日志 Failed（失败） 所有容器已终止，并且至少有一个容器运行失败而终止，或者被系统终止，可以用logs和describe查看pod日志和状态信息 Unknown（未知状态） 通常是通信问题造成无法获取pod状态 ImagePullBackOff/ErrImagePull 镜像拉取失败，一般是镜像不存在、网络不通畅、或者需要认证造成的，可以用describe查看具体event CrashLoopBackOff 容器启动失败，可以用logs命令查看具体原因，一般是启动命令不正确、健康检查不通过等 OOMKilled 容器内存溢出，一般是容器的limit设置太小，或者程序本身造成内存溢出，logs查看日志分析 Terminating Pod正在被删除，可以用describe查看信息 SysctlForbidden Pod定义了内核配置，但kubelet没有添加该配置或者不兼容，用describe查看原因 Completed 容器内主进程退出，一般是计划任务执行完成显示该状态，用logs查看执行日志 ContainerCreating Pod正在创建，一般是正在下载镜像或者有配置不当的情况卡住，describe查看状态 常用命令 查看某个资源的定义和用法 kubectl explain 查看Pod的状态 kubectl get pods kubectl describe pods my-pod 监控Pod状态的变化 kubectl get pod -w 可以看到一个 namespace 中所有的 pod 的 phase 变化。\n查看 Pod 的日志 kubectl logs my-pod kubectl logs my-pod -c my-container kubectl logs -f my-pod kubectl logs -f my-pod -c my-container -f 参数可以 follow 日志输出。\n交互式 debug kubectl exec my-pod -it /bin/bash kubectl top pod POD_NAME --containers 查看访问地址\nEndpoint = Pod IP + Container Port\n[root@k8s-master ~]# kubectl get endpoints NAME ENDPOINTS AGE kubernetes 192.168.43.236:6443 28h mysql 192.168.36.85:3306 24h myweb 192.168.36.84:8080,192.168.36.86:8080 24h nginx 192.168.36.87:80 28h 查看Service详情信息 [root@k8s-master ~]# kubectl get svc myweb -o yaml apiVersion: v1 kind: Service metadata: creationTimestamp: \u0026#34;2020-10-26T06:14:53Z\u0026#34; managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:spec: f:externalTrafficPolicy: {} f:ports: .: {} k:{\u0026#34;port\u0026#34;:8080,\u0026#34;protocol\u0026#34;:\u0026#34;TCP\u0026#34;}: .: {} f:nodePort: {} f:port: {} f:protocol: {} f:targetPort: {} f:selector: .: {} f:app: {} f:sessionAffinity: {} f:type: {} manager: kubectl-create operation: Update time: \u0026#34;2020-10-26T06:14:53Z\u0026#34; name: myweb namespace: default resourceVersion: \u0026#34;24576\u0026#34; selfLink: /api/v1/namespaces/default/services/myweb uid: ac76191d-78e2-4786-a930-793a338753ea spec: clusterIP: 192.168.115.169 externalTrafficPolicy: Cluster ports: - nodePort: 30001 port: 8080 protocol: TCP targetPort: 8080 selector: app: myweb sessionAffinity: None type: NodePort status: loadBalancer: {} 查看namespace # namespace具有资源隔离的作用 # 查看namespace [root@k8s-master ~]# kubectl get namespaces NAME STATUS AGE default Active 29h kube-node-lease Active 29h kube-public Active 29h kube-system Active 29h # 默认查看namespace=default内的资源 [root@k8s-master ~]# kubectl get pods NAME READY STATUS RESTARTS AGE mysql-hdg66 1/1 Running 3 25h myweb-ctzhn 1/1 Running 3 25h myweb-dm94j 1/1 Running 3 25h nginx-6799fc88d8-xj4c4 1/1 Running 4 29h # 指定namespace [root@k8s-master ~]# kubectl get pods --namespace=kube-public 问题1：K8S安装完成之后，某一个节点的Swap内存打开了，对与K8S系统的影响是啥？ 排查方案:\n流程1: kubectl get nodes查看各节点状态\u0026ndash;\u0026gt;假设k8s-node1 NotReady\u0026ndash;\u0026gt;\nkubectl describe nodes k8s-node1查看节点详细状态\u0026ndash;\u0026gt;着重分析Events和Conditions下面输出的提示信息\u0026ndash;\u0026gt;\nkubectl describe pods mysql-hdg66查看对应pod的状态\u0026ndash;\u0026gt;主要查看Events输出的日志\u0026ndash;\u0026gt;\nkubectl describe svc mysql查看对应service的状态\u0026ndash;\u0026gt;主要查看Events输出的日志\n流程2:\nmaster节点中查看集群各组件的状态(如果是组件问题,重新拉取镜像即可)\nkubectl get pods -n kube-system\n流程3:使用下面命令在对应NotReady节点上获取k8s运行日志 journalctl -f -u kubelet\n问题2：如果K8S的某个node 出现了NotReady的状态,怎么排查\n问题3：kubectl get cs查看组件为unhealthy状态\n排查方案：\n流程： 1.kubectl get pods -A查看所有pod是否正常 2.检查kube-scheduler和kube-controller-manager组件配置是否禁用了非安全端口 vim /etc/kubernetes/manifests/kube-scheduler.yaml vim /etc/kubernetes/manifests/kube-controller-manager.yaml 删除--port=0这一行 3.重启kubelet systemctl restart kubelet\n","permalink":"https://deemoprobe.github.io/posts/tech/kubernetes/kubernetestroubleshooting/","summary":"Pod状态分析 状态 说明 Pending（挂起） Pod已被Kubernetes系统接收，但仍有一个或多个容器未被创建，可以通过kubectl describe查看原因 Running（运行中） Pod已被调度到某一节点上，内部所有容器已被创建，而且至少有一个处于运行状态，或者是正在启动或重","title":"KubernetesTroubleShooting"},{"content":"KubernetesUpgrade 2021-0726\n手动升级的还没有详细的方案，大多是基于管理工具部署和升级，比如juju、kubeadm、kops、kubespray等。\n1. 升级步骤 **注意：**该升级步骤是实验性的，建议在测试集群上使用，无法保证线上服务不中断，实际升级完成后无需对线上服务做任何操作。\n大体上的升级步骤是，先升级master节点，然后再一次升级每台node节点。\n2. 升级建议 主要包括以下建议：\n应用使用高级对象定义，如支持滚动更新的Deployment对象 应用要部署成多个实例 使用pod的preStop hook，加强pod的生命周期管理 使用就绪和健康检查探针来确保应用存活和及时阻拦应用流量的分发 2.1. 准备 备份kubernetes原先的二进制文件和配置文件。 下载最新版本的kubernetes二进制包，如1.8.5版本，下载二进制包，我们使用的是kubernetes-server-linux-amd64.tar.gz，分发到集群的每个节点上。 2.2. 升级master节点 停止master节点的进程\nsystemctl stop kube-apiserver systemctl stop kube-scheduler systemctl stop kube-controller-manager systemctl stop kube-proxy systemctl stop kubelet 使用新版本的kubernetes二进制文件替换原来老版本的文件，然后启动master节点上的进程：\nsystemctl start kube-apiserver systemctl start kube-scheduler systemctl start kube-controller-manager 因为我们的master节点同时也作为node节点，所有还要执行下面的”升级node节点“中的步骤。\n2.3. 升级node节点 关闭swap\n# 临时关闭 swapoff -a # 永久关闭，注释掉swap分区即可 vim /etc/fstab #UUID=65c9f92d-4828-4d46-bf19-fb78a38d2fd1 swap swap defaults 0 0 修改kubelet的配置文件\n将kubelet的配置文件/etc/kubernetes/kublet配置文件中的KUBELET_API_SERVER=\u0026quot;--api-servers=http://172.20.0.113:8080\u0026quot;行注释掉。\n注意：：kubernetes1.7及以上版本已经没有该配置了，API server的地址写在了kubeconfig文件中。\n停止node节点上的kubernetes进程：\nsystemctl stop kubelet systemctl stop kube-proxy 使用新版本的kubernetes二进制文件替换原来老版本的文件，然后启动node节点上的进程：\nsystemctl start kubelet systemctl start kube-proxy 启动新版本的kube-proxy报错找不到conntrack命令，使用yum install -y conntrack-tools命令安装后重启kube-proxy即可。\n3. 检查 到此升级完成，在master节点上检查节点状态：\nNAME STATUS ROLES AGE VERSION 172.20.0.113 Ready \u0026lt;none\u0026gt; 244d v1.8.5 172.20.0.114 Ready \u0026lt;none\u0026gt; 244d v1.8.5 172.20.0.115 Ready \u0026lt;none\u0026gt; 244d v1.8.5 所有节点的状态都正常，再检查下原先的运行在kubernetes之上的服务是否正常，如果服务正常的话说明这次升级无误。\n","permalink":"https://deemoprobe.github.io/posts/tech/kubernetes/kubernetesupgrade/","summary":"KubernetesUpgrade 2021-0726 手动升级的还没有详细的方案，大多是基于管理工具部署和升级，比如juju、kubeadm、kops、kubespray等。 1. 升级步骤 **注意：**该升级步骤是实验性的，建议在测试集群上使用，无法保证线上服务不中断，实际升级完成后无需对线上服务做任何操作。 大体上的升级步骤是，先升","title":"KubernetesUpgrade"},{"content":"污点和容忍 Taint在一类服务器上打上污点，让不能容忍这个污点的Pod不能部署在打了污点的服务器上。Toleration是让Pod容忍节点上配置的污点，可以让一些需要特殊配置的Pod能够调用到具有污点和特殊配置的节点上。\n节点可以设置多个污点，pod也可以通过tolerations:设置多个容忍策略，节点污点策略：\nNoSchedule：禁止调度到该节点，已经在该节点上的Pod不受影响 NoExecute：禁止调度到该节点，如果不符合这个污点，会立马被驱逐（或在一段时间后） PreferNoSchedule：尽量避免将Pod调度到指定的节点上，如果没有更合适的节点，可以部署到该节点 # 容忍方式：完全匹配 tolerations: - key: \u0026#34;taintKey\u0026#34; operator: \u0026#34;Equal\u0026#34; value: \u0026#34;taintValue\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; # 不完全匹配 tolerations: - key: \u0026#34;taintKey\u0026#34; operator: \u0026#34;Exists\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; # 配置容忍时间 tolerations: - key: \u0026#34;key1\u0026#34; operator: \u0026#34;Equal\u0026#34; value: \u0026#34;value1\u0026#34; effect: \u0026#34;NoExecute\u0026#34; tolerationSeconds: 3600 # 实例 [root@k8s-master01 ~]# mkdir -p yamls/schedule [root@k8s-master01 ~]# cd yamls/schedule/ [root@k8s-master01 schedule]# kubectl taint node k8s-node01 ssd=true:NoSchedule node/k8s-node01 tainted [root@k8s-master01 schedule]# kubectl describe nodes k8s-node01 | grep Taint -A 2 Taints: ssd=true:NoSchedule Unschedulable: false Lease: [root@k8s-master01 schedule]# kubectl label nodes k8s-node01 ssd=true [root@k8s-master01 schedule]# vim pod-toleration.yaml apiVersion: v1 kind: Pod metadata: name: nginx labels: env: test spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent nodeSelector: ssd: \u0026#34;true\u0026#34; tolerations: - key: \u0026#34;ssd\u0026#34; operator: \u0026#34;Exists\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; [root@k8s-master01 schedule]# kubectl get po nginx --show-labels -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES LABELS nginx 1/1 Running 0 38s 172.17.125.48 k8s-node01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; env=test [root@k8s-master01 schedule]# kubectl describe po nginx | grep -i node-selector -A 4 Node-Selectors: ssd=true Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s ssd:NoSchedule op=Exists Events: # 删除污点 # 直接使用key匹配 [root@k8s-master01 schedule]# kubectl taint node k8s-node01 ssd- node/k8s-node01 untainted [root@k8s-master01 schedule]# kubectl taint node k8s-node01 ssd=true:NoSchedule node/k8s-node01 tainted # key=value:Taint全量匹配 [root@k8s-master01 schedule]# kubectl taint node k8s-node01 ssd=true:NoSchedule- node/k8s-node01 untainted [root@k8s-master01 schedule]# kubectl taint node k8s-node01 ssd=true:NoSchedule node/k8s-node01 tainted # key:Taint匹配 [root@k8s-master01 schedule]# kubectl taint node k8s-node01 ssd:NoSchedule- node/k8s-node01 untainted # 修改污点 [root@k8s-master01 schedule]# kubectl taint node k8s-node01 ssd=true:NoSchedule node/k8s-node01 tainted [root@k8s-master01 schedule]# kubectl taint node k8s-node01 ssd=true:PreferNoSchedule --overwrite node/k8s-node01 modified Kubernetes集群内置污点：\nnode.kubernetes.io/not-ready：节点未准备好，相当于节点状态Ready的值为False。 node.kubernetes.io/unreachable：Node Controller访问不到节点，相当于节点状态Ready的值为Unknown。 node.kubernetes.io/-out-of-disk：节点磁盘耗尽。 node.kubernetes.io/memory-pressure：节点存在内存压力。 node.kubernetes.io/disk-pressure：节点存在磁盘压力。 node.kubernetes.io/network-unavailable：节点网络不可达。 node.kubernetes.io/unschedulable：节点不可调度。 node.cloudprovider.kubernetes.io/uninitialized：如果Kubelet启动时指定了一个外部的cloudprovider，它将给当前节点添加一个Taint将其标记为不可用。在cloud-controller-manager的一个controller初始化这个节点后，Kubelet将删除这个Taint。 # 为了防止有时部分节点健康检查太慢，300s无法完成，可以修改为容忍3000秒后再驱逐（默认是300秒）： tolerations: - key: \u0026#34;node.kubernetes.io/unreachable\u0026#34; operator: \u0026#34;Exists\u0026#34; effect: \u0026#34;NoExecute\u0026#34; tolerationSeconds: 5000 亲和力和反亲和力 requiredDuringSchedulingIgnoredDuringExecution：强制调度 preferredDuringSchedulingIgnoredDuringExecution：优先调度 节点亲和力 # 先删除污点 [root@k8s-master01 schedule]# kubectl taint node k8s-node01 ssd=true:PreferNoSchedule- node/k8s-node01 untainted [root@k8s-master01 schedule]# kubectl taint node k8s-node01 ssd=true:NoSchedule- node/k8s-node01 untainted [root@k8s-master01 schedule]# kubectl describe nodes k8s-node01 | grep Taint Taints: \u0026lt;none\u0026gt; # 为k8s-node01和k8s-node02打标签disktype=ssd [root@k8s-master01 schedule]# kubectl label nodes k8s-node01 k8s-node02 disktype=ssd node/k8s-node01 labeled node/k8s-node02 labeled # 确认标签 [root@k8s-master01 schedule]# kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS k8s-master01 Ready master 9d v1.23.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-master01,kubernetes.io/os=linux,node-role.kubernetes.io/master=,node.kubernetes.io/node= k8s-master02 Ready master 9d v1.23.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-master02,kubernetes.io/os=linux,node-role.kubernetes.io/master=,node.kubernetes.io/node= k8s-master03 Ready master 9d v1.23.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-master03,kubernetes.io/os=linux,node-role.kubernetes.io/master=,node.kubernetes.io/node= k8s-node01 Ready node 9d v1.23.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,disktype=ssd,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node01,kubernetes.io/os=linux,node-role.kubernetes.io/node=,node.kubernetes.io/node=,ssd=true k8s-node02 Ready node 9d v1.23.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,disktype=ssd,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node02,kubernetes.io/os=linux,node-role.kubernetes.io/node=,node.kubernetes.io/node= # 创建节点硬亲和力调度pod实例 [root@k8s-master01 schedule]# vim node-hardaffinity.yaml apiVersion: v1 kind: Pod metadata: name: node-hardaffinity spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: disktype operator: In values: - ssd containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent [root@k8s-master01 schedule]# kubectl get po node-hardaffinity -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES node-hardaffinity 1/1 Running 0 25s 172.27.14.210 k8s-node02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; # 为k8s-node01单独打上ssd:first的标签 [root@k8s-master01 schedule]# kubectl label nodes k8s-node01 ssd=first error: \u0026#39;ssd\u0026#39; already has a value (true), and --overwrite is false # 之前用过ssd=true，覆盖掉即可 [root@k8s-master01 schedule]# kubectl label nodes k8s-node01 ssd=first --overwrite node/k8s-node01 labeled # 创建节点软亲和力调度pod实例 apiVersion: v1 kind: Pod metadata: name: node-softaffinity spec: affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: ssd operator: Exists containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent [root@k8s-master01 schedule]# vim node-softaffinity.yaml [root@k8s-master01 schedule]# kubectl get po node-softaffinity -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES node-softaffinity 1/1 Running 0 34s 172.17.125.49 k8s-node01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; affinity配置详解：\nrequiredDuringSchedulingIgnoredDuringExecution：硬亲和力配置 nodeSelectorTerms：节点选择器配置，可以配置多个matchExpressions（满足其一即可） preferredDuringSchedulingIgnoredDuringExecution：软亲和力配置 weight：软亲和力的权重，权重越高优先级越大，范围1-100 preference：软亲和力配置项，和weight同级，可以配置多个，matchExpressions和硬亲和力一致 operator：标签匹配的方式 In：相当于key = value的形式 NotIn：相当于key != value的形式 Exists：节点存在label的key为指定的值即可，无需配置values字段 DoesNotExist：节点不存在label的key为指定的值即可，无需配置values字段 Gt：大于value指定的值 Lt：小于value指定的值 Pod亲和力反亲和力 requiredDuringSchedulingIgnoredDuringExecution：硬亲和力配置，强制匹配 preferredDuringSchedulingIgnoredDuringExecution：软亲和力配置，优先匹配 podAffinity：pod亲和力配置，表示和具有匹配标签的pod部署在一起 podAntiAffinity：pod反亲和力，表示和具有匹配标签的pod分开部署 labelSelector：Pod选择器配置，可以配置多个 matchExpressions：和节点亲和力配置一致 operator：配置和节点亲和力一致，但是没有Gt和Lt topologyKey：匹配的拓扑域的key，也就是节点上label的key，key和value相同的为同一个域，可以用于标注不同的机房和地区 namespaces: 和哪个命名空间的Pod进行匹配，为空为当前命名空间 # pod全配置示例，仅是示例，实际使用按需选择即可 apiVersion: v1 kind: Pod metadata: name: pod-affinity spec: affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: security operator: In values: - S1 topologyKey: failure-domain.beta.kubernetes.io/zone podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: security operator: In values: - S2 namespaces: - default topologyKey: failure-domain.beta.kubernetes.io/zone containers: - name: pod-affinity image: nginx 综合实例 实例一同一个应用副本互斥地部署在多个节点 这种操作避免同一个应用pod副本部署在同一个节点，如果节点故障，造成应用的不可用。即避免单点故障。\n# 先查看污点，有4个节点可以使用，不影响后续操作，可以不删污点 [root@k8s-master01 schedule]# kubectl describe nodes | grep taint -i Taints: role/k8s-master01:NoSchedule Taints: role/k8s-master02:PreferNoSchedule Taints: \u0026lt;none\u0026gt; Taints: \u0026lt;none\u0026gt; Taints: \u0026lt;none\u0026gt; # 创建3副本的应用myapp [root@k8s-master01 schedule]# vim myapp.yaml apiVersion: apps/v1 kind: Deployment metadata: labels: app: myapp-podantiaffinity # deployment标签和pod标签可以不一致 name: myapp-podantiaffinity namespace: kube-public spec: replicas: 3 selector: matchLabels: app: myapp # deployment匹配标签和要和下面template里pod标签一致 template: metadata: labels: app: myapp spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: # 匹配pod标签 - key: app operator: In values: - myapp topologyKey: kubernetes.io/hostname # 拓扑域，匹配主机名key containers: - image: nginx imagePullPolicy: IfNotPresent name: myapp [root@k8s-master01 schedule]# kubectl apply -f myapp.yaml deployment.apps/myapp-podantiaffinity created [root@k8s-master01 schedule]# kubectl get deployments.apps -n kube-public myapp-podantiaffinity NAME READY UP-TO-DATE AVAILABLE AGE myapp-podantiaffinity 3/3 3 3 105s [root@k8s-master01 schedule]# kubectl get rs,pod -n kube-public -owide --show-labels | grep myapp replicaset.apps/myapp-podantiaffinity-6f8dc8f64f 3 3 3 2m35s myapp nginx app=myapp,pod-template-hash=6f8dc8f64f app=myapp,pod-template-hash=6f8dc8f64f pod/myapp-podantiaffinity-6f8dc8f64f-772m6 1/1 Running 0 2m35s 172.27.14.211 k8s-node02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; app=myapp,pod-template-hash=6f8dc8f64f pod/myapp-podantiaffinity-6f8dc8f64f-7hxd4 1/1 Running 0 2m35s 172.17.125.50 k8s-node01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; app=myapp,pod-template-hash=6f8dc8f64f pod/myapp-podantiaffinity-6f8dc8f64f-rk7rf 1/1 Running 0 2m35s 172.18.195.21 k8s-master03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; app=myapp,pod-template-hash=6f8dc8f64f # 可用的调度节点为4，修改副本数为5，会发现有一个无法调度，一直处于pending状态 [root@k8s-master01 schedule]# kubectl scale deployment -n kube-public myapp-podantiaffinity --replicas=5 deployment.apps/myapp-podantiaffinity scaled [root@k8s-master01 schedule]# kubectl get po -n kube-public -owide | grep myapp myapp-podantiaffinity-6f8dc8f64f-772m6 1/1 Running 0 16m 172.27.14.211 k8s-node02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; myapp-podantiaffinity-6f8dc8f64f-7hxd4 1/1 Running 0 16m 172.17.125.50 k8s-node01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; myapp-podantiaffinity-6f8dc8f64f-rk7rf 1/1 Running 0 16m 172.18.195.21 k8s-master03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; myapp-podantiaffinity-6f8dc8f64f-sqxjm 0/1 Pending 0 10m \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; myapp-podantiaffinity-6f8dc8f64f-zrtkr 1/1 Running 0 10m 172.25.92.77 k8s-master02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; # 查看处于pending状态的pod事件，提示不匹配pod anti-affinity rules [root@k8s-master01 schedule]# kubectl describe po -n kube-public myapp-podantiaffinity-6f8dc8f64f-sqxjm | grep -i events -A 10 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 12s (x9 over 9m13s) default-scheduler 0/5 nodes are available: 1 node(s) had taint {role/k8s-master01: }, that the pod didn\u0026#39;t tolerate, 4 node(s) didn\u0026#39;t match pod anti-affinity rules. 实例二同一个应用副本互斥地部署在指定标签的节点 # 之前配置过只有k8s-node01和k8s-node02有disktype=ssd标签 [root@k8s-master01 schedule]# vim myapp.yaml apiVersion: apps/v1 kind: Deployment metadata: labels: app: myapp-podantiaffinity # deployment标签和pod标签可以不一致 name: myapp-podantiaffinity namespace: kube-public spec: replicas: 2 # 副本数改为2 selector: matchLabels: app: myapp # deployment匹配标签和要和下面template里pod标签一致 template: metadata: labels: app: myapp spec: nodeSelector: # 增加nodeSelector标签选择器，节点标签需要已存在 disktype: ssd affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: # 匹配pod标签 - key: app operator: In values: - myapp topologyKey: kubernetes.io/hostname # 拓扑域，匹配主机名key containers: - image: nginx imagePullPolicy: IfNotPresent name: myapp [root@k8s-master01 schedule]# kubectl apply -f myapp.yaml deployment.apps/myapp-podantiaffinity created [root@k8s-master01 schedule]# kubectl get po -n kube-public -owide | grep myapp myapp-podantiaffinity-65b6c697b7-gtqzc 1/1 Running 0 48s 172.27.14.212 k8s-node02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; myapp-podantiaffinity-65b6c697b7-mcv6j 1/1 Running 0 48s 172.17.125.51 k8s-node01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; # 副本数改为3，由于没有多余disktype=ssd节点互斥调度，会处于pending状态 [root@k8s-master01 schedule]# kubectl scale deployment -n kube-public myapp-podantiaffinity --replicas=3 deployment.apps/myapp-podantiaffinity scaled [root@k8s-master01 schedule]# kubectl get po -n kube-public -owide | grep myapp myapp-podantiaffinity-65b6c697b7-gtqzc 1/1 Running 0 2m17s 172.27.14.212 k8s-node02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; myapp-podantiaffinity-65b6c697b7-mcv6j 1/1 Running 0 2m17s 172.17.125.51 k8s-node01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; myapp-podantiaffinity-65b6c697b7-t7v7h 0/1 Pending 0 4s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; # 情况和实例一一致 [root@k8s-master01 schedule]# kubectl describe po -n kube-public myapp-podantiaffinity-65b6c697b7-t7v7h | grep -i event -A 10 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 18s (x2 over 83s) default-scheduler 0/5 nodes are available: 1 node(s) had taint {role/k8s-master01: }, that the pod didn\u0026#39;t tolerate, 2 node(s) didn\u0026#39;t match Pod\u0026#39;s node affinity/selector, 2 node(s) didn\u0026#39;t match pod anti-affinity rules. 实例三尽量将应用调度到ssd高性能节点 这种情况是部分应用侧重高性能服务器部署服务，但不在乎是否是部署到同一节点\n# k8s-node01和k8s-node02已有disktype=ssd高性能标签，假设这两台服务器就是高性能的 # k8s-master03是传统一般性能的服务器，打上标签 [root@k8s-master01 schedule]# kubectl label nodes k8s-master03 disktype=physical node/k8s-master03 labeled [root@k8s-master01 schedule]# vim myapp.yaml apiVersion: apps/v1 kind: Deployment metadata: labels: app: myapp-podantiaffinity # deployment标签和pod标签可以不一致 name: myapp-podantiaffinity namespace: kube-public spec: replicas: 2 # 副本数改为2 selector: matchLabels: app: myapp # deployment匹配标签和要和下面template里pod标签一致 template: metadata: labels: app: myapp spec: nodeSelector: # 增加nodeSelector标签选择器，节点标签需要已存在 disktype: ssd affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - preference: matchExpressions: - key: disktype operator: In values: - ssd weight: 100 # 高性能服务器权重 - preference: matchExpressions: - key: disktype operator: In values: - physical weight: 10 # 传统服务器权重 containers: - image: nginx imagePullPolicy: IfNotPresent name: myapp [root@k8s-master01 schedule]# kubectl apply -f myapp.yaml deployment.apps/myapp-podantiaffinity created [root@k8s-master01 schedule]# kubectl get po -n kube-public -owide | grep myapp myapp-podantiaffinity-7b655bd4fc-mw7g8 1/1 Running 0 34s 172.27.14.213 k8s-node02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; myapp-podantiaffinity-7b655bd4fc-sqg5f 1/1 Running 0 34s 172.17.125.52 k8s-node01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 实例四同一个应用调度在不同拓扑域 这种情况是应用更侧重的是不同地域服务的访问速度，就近部署是最好的，同时也避免了单点故障。前提是为所有机器划分拓扑域，否则未划分的机器也会被正常调度。\n# 假设k8s-node01和k8s-node02分别属于上海和北京数据中心，打上标签 [root@k8s-master01 schedule]# kubectl label nodes k8s-node01 k8s-master03 region=ShangHai node/k8s-node01 labeled node/k8s-master03 labeled [root@k8s-master01 schedule]# kubectl label nodes k8s-node02 region=BeiJing node/k8s-node02 labeled [root@k8s-master01 schedule]# vim myapp.yaml apiVersion: apps/v1 kind: Deployment metadata: labels: app: myapp-pod-region-antiaffinity # deployment标签和pod标签可以不一致 name: myapp-pod-region-antiaffinity namespace: kube-public spec: replicas: 2 selector: matchLabels: region: different # deployment匹配标签和要和下面template里pod标签一致 template: metadata: labels: region: different spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: # 匹配pod标签 - key: region operator: In values: - different topologyKey: region # 拓扑域，匹配region containers: - image: nginx imagePullPolicy: IfNotPresent name: myapp [root@k8s-master01 schedule]# kubectl apply -f myapp.yaml deployment.apps/myapp-pod-region-antiaffinity created [root@k8s-master01 schedule]# kubectl get po -n kube-public -owide | grep myapp myapp-pod-region-antiaffinity-6675849bf9-577j2 1/1 Running 0 16s 172.18.195.27 k8s-master03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; myapp-pod-region-antiaffinity-6675849bf9-5rmww 1/1 Running 0 16s 172.27.14.218 k8s-node02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; # 修改副本数为3 [root@k8s-master01 schedule]# kubectl scale deployment -n kube-public myapp-pod-region-antiaffinity --replicas=3 deployment.apps/myapp-pod-region-antiaffinity scaled [root@k8s-master01 schedule]# kubectl get po -n kube-public -owide | grep myapp myapp-pod-region-antiaffinity-6675849bf9-577j2 1/1 Running 0 91s 172.18.195.27 k8s-master03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; myapp-pod-region-antiaffinity-6675849bf9-5rmww 1/1 Running 0 91s 172.27.14.218 k8s-node02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; myapp-pod-region-antiaffinity-6675849bf9-7s9pc 1/1 Running 0 2s 172.25.92.78 k8s-master02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; [root@k8s-master01 schedule]# kubectl get nodes k8s-master02 --show-labels NAME STATUS ROLES AGE VERSION LABELS k8s-master02 Ready master 9d v1.23.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-master02,kubernetes.io/os=linux,node-role.kubernetes.io/master=,node.kubernetes.io/node= # k8s-master02并没有设置region，也被调度了，可见如果存在没有标记拓扑域的服务器，当拓扑域副本数溢出时，也会被正常调度 # 先调回正常副本数 [root@k8s-master01 schedule]# kubectl scale deployment -n kube-public myapp-pod-region-antiaffinity --replicas=2 deployment.apps/myapp-pod-region-antiaffinity scaled [root@k8s-master01 schedule]# kubectl get po -n kube-public -owide | grep myapp myapp-pod-region-antiaffinity-6675849bf9-577j2 1/1 Running 0 5m45s 172.18.195.27 k8s-master03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; myapp-pod-region-antiaffinity-6675849bf9-5rmww 1/1 Running 0 5m45s 172.27.14.218 k8s-node02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; # 为k8s-master02设置不可调度污点，扩充副本测试之 [root@k8s-master01 schedule]# kubectl taint node k8s-master02 region:NoSchedule node/k8s-master02 tainted # 此时只存在三个节点可以调度（master01是之前就设置的污点），而且都在region拓扑域中 [root@k8s-master01 schedule]# kubectl describe nodes | grep -i taint -A 1 Taints: role/k8s-master01:NoSchedule Unschedulable: false -- Taints: region:NoSchedule # 不可调度污点会覆盖掉PreferNoSchedule污点，不删除也可以 role/k8s-master02:PreferNoSchedule -- Taints: \u0026lt;none\u0026gt; Unschedulable: false -- Taints: \u0026lt;none\u0026gt; Unschedulable: false -- Taints: \u0026lt;none\u0026gt; Unschedulable: false # 扩充副本测试 [root@k8s-master01 schedule]# kubectl scale deployment -n kube-public myapp-pod-region-antiaffinity --replicas=3 deployment.apps/myapp-pod-region-antiaffinity scaled [root@k8s-master01 schedule]# kubectl get po -n kube-public -owide | grep myapp myapp-pod-region-antiaffinity-6675849bf9-577j2 1/1 Running 0 9m41s 172.18.195.27 k8s-master03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; myapp-pod-region-antiaffinity-6675849bf9-5rmww 1/1 Running 0 9m41s 172.27.14.218 k8s-node02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; myapp-pod-region-antiaffinity-6675849bf9-djkgc 0/1 Pending 0 5s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; [root@k8s-master01 schedule]# kubectl describe po -n kube-public myapp-pod-region-antiaffinity-6675849bf9-djkgc | grep -i event -A 5 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 43s default-scheduler 0/5 nodes are available: 1 node(s) had taint {region: }, that the pod didn\u0026#39;t tolerate, 1 node(s) had taint {role/k8s-master01: }, that the pod didn\u0026#39;t tolerate, 3 node(s) didn\u0026#39;t match pod anti-affinity rules. 需要注意的是标签的唯一性，不要和其他项目应用冲突。\n","permalink":"https://deemoprobe.github.io/posts/tech/kubernetes/kubernetespodschedulingstrategy/","summary":"污点和容忍 Taint在一类服务器上打上污点，让不能容忍这个污点的Pod不能部署在打了污点的服务器上。Toleration是让Pod容忍节点上配置的污点，可以让一些需要特殊配置的Pod能够调用到具有污点和特殊配置的节点上。 节点可以设置多个污点，pod也可以通过tolerations","title":"KubernetesPodSchedulingStrategy"},{"content":"Secret 概念 Secret 是一种包含少量敏感信息例如密码、令牌或密钥的资源对象。使用 Secret 意味着可以独立于应用之外存储敏感数据。由于创建 Secret 可以独立于使用它们的 Pod， 因此在创建、查看和编辑Pod的工作流程中暴露Secret的风险较小。Secret可以以Volume或者环境变量的方式使用\nSecret 类似于 ConfigMap 但专门用于保存机密数据。用户可以创建 secret,同时系统也创建了一些 secret。\n默认情况下，Kubernetes Secret未加密地存储在 API 服务器的底层数据存储（etcd）中。任何拥有 API 访问权限的人都可以检索或修改 Secret，任何有权访问 etcd 的人也可以。此外，任何有权限在命名空间中创建Pod的人都可以使用该访问权限读取该命名空间中的任何Secret；这包括间接访问，例如创建 Deployment 的能力。为了安全地使用 Secret，至少要为secret配置：\n1.Secret启用静态加密 2.启用或配置RBAC规则来限制读取Secret数据（包括通过间接方式） 3.在适当的情况下，还可以使用RBAC等机制来限制允许哪些主体创建新Secret或更新现有Secret Secret内置的type类型:\nOpaque：默认类型，用户定义的任意数据 kubernetes.io/service-account-token：ServiceAccount令牌 kubernetes.io/dockercfg：与~/.dockercfg文件一致，存储私有镜像仓库口令 kubernetes.io/dockerconfigjson：与~/.docker/config.json文件一致，存储私有镜像仓库口令，宿主机登陆后产生该文件 kubernetes.io/basic-auth：用于基本身份认证（账号/密码） kubernetes.io/ssh-auth：用于 SSH 身份认证 kubernetes.io/tls：用于存储TLS证书 bootstrap.kubernetes.io/token：用于创建新集群或加入新节点，自动颁发证书 类型实例 Secret创建和使用方式与ConfigMap一致。\nOpaque Secret [root@k8s-master01 ~]# mkdir -p yamls/secret [root@k8s-master01 ~]# cd yamls/secret/ # generic子命令表明创建的是普通类型的secret，即默认为opaque类型 [root@k8s-master01 secret]# kubectl create secret generic opaque-secret secret/opaque-secret created [root@k8s-master01 secret]# kubectl get secrets NAME TYPE DATA AGE opaque-secret Opaque 0 5s [root@k8s-master01 secret]# kubectl describe secrets opaque-secret Name: opaque-secret Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Type: Opaque Data # DATA==0 没有数据 ==== # 创建用户凭证，-n去除换行符，否则base64加密也会包含换行符 [root@k8s-master01 secret]# echo -n \u0026#39;admin\u0026#39; \u0026gt; ./username.txt [root@k8s-master01 secret]# echo -n \u0026#39;fag734ubjtb28821hhna\u0026#39; \u0026gt; ./password.txt # 默认文件名为秘钥名称，也可以指定--from-file=username=./username.txt [root@k8s-master01 secret]# kubectl create secret generic user-passwd --from-file=./username.txt --from-file=./password.txt secret/user-passwd created [root@k8s-master01 secret]# kubectl get secrets user-passwd NAME TYPE DATA AGE user-passwd Opaque 2 9s [root@k8s-master01 secret]# kubectl describe secrets user-passwd Name: user-passwd Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Type: Opaque Data ==== password.txt: 20 bytes username.txt: 5 bytes # 也可以直接使用参数方式创建凭证，密码如果有特殊字符（例如：$，\\，*，= 和 !），需要用单引号强制转为字符串使用 [root@k8s-master01 secret]# kubectl create secret generic user-password --from-literal=username=admin --from-literal=password=\u0026#39;ninda$\\ad*^\u0026#39; secret/user-password created [root@k8s-master01 secret]# kubectl describe secrets user-password Name: user-password Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Type: Opaque Data ==== password: 11 bytes username: 5 bytes # 获取secret加密内容 [root@k8s-master01 secret]# kubectl get secrets user-password -o jsonpath=\u0026#39;{.data}\u0026#39; {\u0026#34;password\u0026#34;:\u0026#34;bmluZGEkXGFkKl4=\u0026#34;,\u0026#34;username\u0026#34;:\u0026#34;YWRtaW4=\u0026#34;} # 解码，base64 --decode，简写base64 -d，这里没有换行符，所以效果如下 [root@k8s-master01 secret]# echo \u0026#39;YWRtaW4=\u0026#39; | base64 -d admin[root@k8s-master01 secret]# echo \u0026#39;bmluZGEkXGFkKl4=\u0026#39; | base64 -d ninda$\\ad*^ # 删除secret [root@k8s-master01 secret]# kubectl delete secrets user-password secret \u0026#34;user-password\u0026#34; deleted ServiceAccount 默认每个namespace都会生成一个default服务账户，Pod默认使用这个账户，如果Pod要使用指定的serviceaccount账户需要指定spec.serviceAccountName字段。证书令牌会自动挂载到Pod的 /run/secrets/kubernetes.io/serviceaccount目录中\n[root@k8s-master01 secret]# kubectl get sa NAME SECRETS AGE default 1 9d [root@k8s-master01 secret]# kubectl get sa default -oyaml apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: \u0026#34;2022-03-06T06:24:39Z\u0026#34; name: default namespace: default resourceVersion: \u0026#34;547\u0026#34; uid: 24a36bad-475a-4086-90f8-f030711edfda secrets: - name: default-token-wnp8m [root@k8s-master01 secret]# kubectl get secrets default-token-wnp8m -oyaml apiVersion: v1 data: ca.crt: 已省略 namespace: ZGVmYXVsdA== token: 已省略 kind: Secret metadata: annotations: kubernetes.io/service-account.name: default kubernetes.io/service-account.uid: 24a36bad-475a-4086-90f8-f030711edfda creationTimestamp: \u0026#34;2022-03-06T06:24:39Z\u0026#34; name: default-token-wnp8m namespace: default resourceVersion: \u0026#34;546\u0026#34; uid: a0c7b3cd-aed8-4e5f-b3c7-ee5d1eae90b3 type: kubernetes.io/service-account-token # 随便找一个pod，查看证书令牌的挂载目录 [root@k8s-master01 secret]# kubectl get po nginx-85b98978db-sfzmb NAME READY STATUS RESTARTS AGE nginx-85b98978db-sfzmb 1/1 Running 5 (71m ago) 2d19h [root@k8s-master01 secret]# kubectl exec -it nginx-85b98978db-sfzmb -- ls -al /run/secrets/kubernetes.io/serviceaccount total 0 drwxrwxrwt 3 root root 140 Mar 15 06:37 . drwxr-xr-x 3 root root 28 Mar 15 05:50 .. drwxr-xr-x 2 root root 100 Mar 15 06:37 ..2022_03_15_06_37_46.2287599106 lrwxrwxrwx 1 root root 32 Mar 15 06:37 ..data -\u0026gt; ..2022_03_15_06_37_46.2287599106 lrwxrwxrwx 1 root root 13 Mar 15 05:49 ca.crt -\u0026gt; ..data/ca.crt lrwxrwxrwx 1 root root 16 Mar 15 05:49 namespace -\u0026gt; ..data/namespace lrwxrwxrwx 1 root root 12 Mar 15 05:49 token -\u0026gt; ..data/token # 创建serviceaccount资源，简写sa [root@k8s-master01 secret]# kubectl create serviceaccount web-account serviceaccount/web-account created [root@k8s-master01 secret]# kubectl describe sa web-account Name: web-account Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Image pull secrets: \u0026lt;none\u0026gt; Mountable secrets: web-account-token-96q8n Tokens: web-account-token-96q8n Events: \u0026lt;none\u0026gt; # 可查看到默认为指定的sa配置了一个secret：web-account-token-96q8n [root@k8s-master01 secret]# kubectl get sa web-account -oyaml apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: \u0026#34;2022-03-15T06:29:59Z\u0026#34; name: web-account namespace: default resourceVersion: \u0026#34;181175\u0026#34; uid: dfcbd0e7-8ab4-4f8a-b897-32cb294c9eee secrets: - name: web-account-token-96q8n # 查看secret详情 [root@k8s-master01 secret]# kubectl get secrets web-account-token-96q8n -oyaml apiVersion: v1 data: ca.crt: ca证书内容已省略 namespace: ZGVmYXVsdA== # 加密后的namespace名称 token: token串内容已省略 kind: Secret metadata: annotations: kubernetes.io/service-account.name: web-account kubernetes.io/service-account.uid: dfcbd0e7-8ab4-4f8a-b897-32cb294c9eee creationTimestamp: \u0026#34;2022-03-15T06:29:59Z\u0026#34; name: web-account-token-96q8n namespace: default resourceVersion: \u0026#34;181174\u0026#34; uid: 2c393434-8793-4dc1-b314-2f3185bb28ca type: kubernetes.io/service-account-token # 解码看一下namespace名称 [root@k8s-master01 secret]# echo \u0026#39;ZGVmYXVsdA==\u0026#39; | base64 -d default dockerconfigjson 首先需要有一个镜像仓库, 比如Docker Hub或者 Harbor搭建一个镜像仓库\n通过以下方式去注册一个docker secret认证, 方可拉取私有镜像\nkubectl create secret docker-registry myregistrysecret --docker-server=\u0026#39;IP:Port\u0026#39; --docker-username=\u0026#39;name\u0026#39; --docker-password=\u0026#39;passwd\u0026#39; # 比如镜像仓库地址为172.16.1.1:5000, 用户名admin, 密码Test123456, 私有镜像为myapp [root@k8s-master01 secret]# kubectl create secret docker-registry myregistrysecret --docker-server=\u0026#39;172.16.1.1:5000\u0026#39; --docker-username=\u0026#39;admin\u0026#39; --docker-password=\u0026#39;Test123456\u0026#39; secret/myregistrysecret created [root@k8s-master01 secret]# kubectl get secret NAME TYPE DATA AGE myregistrysecret kubernetes.io/dockerconfigjson 1 8s [root@k8s-master01 secret]# kubectl describe secret myregistrysecret Name: myregistrysecret Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Type: kubernetes.io/dockerconfigjson Data ==== .dockerconfigjson: 112 bytes # 创建pod, 拉取私有镜像 [root@k8s-master01 secret]# cat pod_secret_registry.yaml apiVersion: v1 kind: Pod metadata: name: pod-secret-registry spec: containers: - name: myapp image: 172.16.1.1:5000/myapp imagePullSecrets: - name: myregistrysecret [root@k8s-master01 secret]# kubectl apply -f pod_secret_registry.yaml pod/pod-secret-registry created [root@k8s-master01 secret]# kubectl describe pod pod-secret-registry Name: pod-secret-registry Namespace: default ... Volumes: default-token-v48g4: Type: Secret (a volume populated by a Secret) SecretName: default-token-t65f5 Optional: false QoS Class: BestEffort ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 22s default-scheduler Successfully assigned default/pod-secret-registry to k8s-node1 Normal Pulling 22s kubelet, k8s-node1 Pulling image \u0026#34;172.16.1.1:5000/myapp\u0026#34; Normal Pulled 22s kubelet, k8s-node1 Successfully pulled image \u0026#34;172.16.1.1:5000/myapp\u0026#34; Normal Created 22s kubelet, k8s-node1 Created container myapp Normal Started 21s kubelet, k8s-node1 Started container myapp basic-auth kubernetes.io/basic-auth类型用来存放用于基本身份认证所需的凭据信息。使用这种Secret类型时，Secret的data字段必须包含以下两个键：\nusername: 用于身份认证的用户名； password: 用于身份认证的密码或令牌。 以上两个键的键值都是 base64 编码的字符串。当然也可以在创建Secret时使用stringData字段来提供明文形式的内容。提供基本身份认证类型的 Secret 仅仅是出于用户方便性考虑。\n# 明文内容 apiVersion: v1 kind: Secret metadata: name: secret-basic-auth type: kubernetes.io/basic-auth stringData: username: admin password: t0p-Secret ssh-auth Kubernetes 所提供的内置类型kubernetes.io/ssh-auth用来存放SSH身份认证中所需要的凭据。使用这种Secret类型时，你就必须在其data（或stringData）字段中提供一个ssh-privatekey键值对，作为要使用的SSH凭据。\napiVersion: v1 kind: Secret metadata: name: secret-ssh-auth type: kubernetes.io/ssh-auth data: ssh-privatekey: | MIIEpQIBAAKCAQEAulqb/Y...已省略 注意： SSH 私钥自身无法建立 SSH 客户端与服务器端之间的可信连接。 需要其它方式来建立这种信任关系，以缓解“中间人（Man In The Middle）” 攻击，例如向 ConfigMap 中添加一个 known_hosts 文件。\ntls Kubernetes 提供一种内置的kubernetes.io/tls Secret 类型，用来存放证书 及其相关密钥（通常用在 TLS 场合）。 此类数据主要提供给 Ingress 资源，用以终结 TLS 链接，不过也可以用于其他 资源或者负载。当使用此类型的 Secret 时，Secret 配置中的 data （或 stringData）字段必须包含tls.key和tls.crt主键，尽管 API 服务器 实际上并不会对每个键的取值作进一步的合法性检查。\n这里的公钥/私钥对都必须事先已存在。用于--cert的公钥证书必须是 .PEM 编码的 （Base64 编码的 DER 格式），且与--key所给定的私钥匹配。 私钥必须是通常所说的 PEM 私钥格式，且未加密。对这两个文件而言，PEM 格式数据 的第一行和最后一行（例如，证书所对应的--------BEGIN CERTIFICATE-----和-------END CERTIFICATE----）都不会包含在其中。\napiVersion: v1 kind: Secret metadata: name: secret-tls type: kubernetes.io/tls data: tls.crt: | MIIC2DCCAcCgAwIBAgIBATANBgkqh...已省略 tls.key: | MIIEpgIBAAKCAQEA7yn3bRHQ5FHMQ...已省略 # 也可以直接使用命令指定文件位置 kubectl create secret tls my-tls-secret \\ --cert=path/to/cert/file \\ --key=path/to/key/file 不可变的secret 类似不可变的ConfigMap，一经创建无法后期修改，除非删除重新创建。\napiVersion: v1 kind: Secret metadata: ... data: ... immutable: true 挂载secret [root@k8s-master01 secret]# kubectl get secrets NAME TYPE DATA AGE default-token-wnp8m kubernetes.io/service-account-token 3 9d opaque-secret Opaque 0 95m user-passwd Opaque 2 72m web-account-token-96q8n kubernetes.io/service-account-token 3 56m # 创建yaml [root@k8s-master01 secret]# vim pod-secret-vol.yaml apiVersion: v1 kind: Pod metadata: name: pod-secret-volume spec: containers: - name: myapp image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 volumeMounts: - name: secret-volume mountPath: /etc/secret readOnly: true volumes: - name: secret-volume secret: secretName: user-passwd # 使用这个已创建的secret [root@k8s-master01 secret]# kubectl apply -f pod_secret_volume.yaml pod/pod-secret-volume created [root@k8s-master01 secret]# kubectl get po pod-secret-volume NAME READY STATUS RESTARTS AGE pod-secret-volume 1/1 Running 0 26s [root@k8s-master01 secret]# kubectl exec -it pod-secret-volume -- cat /etc/secret/username.txt admin[root@k8s-master01 secret]# kubectl exec -it pod-secret-volume -- cat /etc/secret/password.txt fag734ubjtb28821hhna 由上可见,在pod中的secret信息实际已经被解密。\nSecret热更新 Secret由于是加密过的，无法通过edit的方式实现热更新，会报错* patch: Invalid value: \u0026quot;map[data:map[name:deemoprobe]]\u0026quot;: error decoding from json: illegal base64 data at input byte 8\n# 为Secret user-pwd添加一个name=deemo，直接创建会提示已存在 [root@k8s-master01 secret]# kubectl create secret generic user-pwd --from-literal name=deemo --from-file username.txt --from-file password.txt error: failed to create secret secrets \u0026#34;user-pwd\u0026#34; already exists # 使用 --dry-run=client -oyaml | kubectl replace -f - 替换原来配置重建Secret # --dry-run=client -oyaml 表示仅运行输出为yaml格式文件但不实际创建，后面Replace采用该yaml替换 [root@k8s-master01 secret]# kubectl create secret generic user-pwd --from-literal name=deemo --from-file username.txt --from-file password.txt --dry-run=client -oyaml | kubectl replace -f - secret/user-pwd replaced [root@k8s-master01 secret]# kubectl get secrets user-pwd -oyaml apiVersion: v1 data: name: ZGVlbW8= password.txt: ZmFnNzM0dWJqdGIyODgyMWhobmE= username.txt: YWRtaW4= kind: Secret metadata: creationTimestamp: \u0026#34;2023-04-30T06:24:42Z\u0026#34; name: user-pwd namespace: default resourceVersion: \u0026#34;128966\u0026#34; uid: 32264121-b6bd-4a85-ad79-5819cd60be7e type: Opaque ","permalink":"https://deemoprobe.github.io/posts/tech/kubernetes/kubernetessecret/","summary":"Secret 概念 Secret 是一种包含少量敏感信息例如密码、令牌或密钥的资源对象。使用 Secret 意味着可以独立于应用之外存储敏感数据。由于创建 Secret 可以独立于使用它们的 Pod， 因此在创建、查看和编辑Pod的工作流程中暴露Secret的风险较小。Secret可以以Volume或者环境变量的方式使用 Secret 类似于 ConfigMap 但专门","title":"KubernetesSecret"},{"content":"ConfigMap 概念 传统架构中，配置文件一般是保存在宿主机上，程序启动时指定特定配置文件即可。但kubernetes中容器部署是灵活的，容器所在节点并不固定，所以传统方式就不适用了。如果在构建镜像时将配置文件一起打包进镜像随容器一起启动，又不够方便，镜像经常变更配置文件是很麻烦的。基于此，kubernetes抽象出ConfigMap资源对象，将配置与Pod分开，更方便配置与管理。\nConfigMap主要用来存储不敏感、未加密的配置信息。ConfigMap 不提供加密功能。如果想存储的数据是加密的，可以使用Secret或其他第三方工具来保证数据的私密性。而不是用 ConfigMap。\nConfigMap的一些限制:\n如需获取ConfigMap的内容到容器，则ConfigMap必须在Pod之前创建 ConfigMap受Namespace限制，Pod只有和ConfigMap处于相同Namespace下的才可以引用它 静态Pod无法引用ConfigMap 使用volumeMounts给Pod挂载ConfigMap时，mountPath只能是目录，而不能是文件；容器内部该目录下如果已有文件，会被新挂载的ConfigMap覆盖 使用ConfigMap ConfigMap可以使用目录、单个文件和命令行键值的方式创建，命令格式如下：\nkubectl create configmap \u0026lt;configmap-name\u0026gt; \u0026lt;data-source\u0026gt; configmap-name：ConfigMap名称 data-source：配置来源，目录、文件、命令行键值 ConfigMap的数据最终都是以键值的方式存储的。\n目录方式 --from-file指定目录下所有文件都作为ConfigMap中的键值对来使用，键为文件名，值为文件内容。\n[root@k8s-master01 ~]# mkdir -p yamls/configmap [root@k8s-master01 ~]# cd yamls/configmap/ [root@k8s-master01 configmap]# cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; game.properties enemies=aliens lives=3 enemies.cheat=true enemies.cheat.level=noGoodRotten secret.code.passphrase=UUDDLRLRBABAs secret.code.allowed=true secret.code.lives=30 EOF [root@k8s-master01 configmap]# cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; ui.properties color.good=purple color.bad=yellow allow.textmode=true how.nice.to.look=fairlyNice EOF [root@k8s-master01 configmap]# kubectl create configmap game --from-file=/root/yamls/configmap/ configmap/game created [root@k8s-master01 configmap]# kubectl get cm NAME DATA AGE game 2 44s [root@k8s-master01 configmap]# kubectl get cm game -oyaml apiVersion: v1 data: game.properties: | # |后为文件键值对内容 enemies=aliens lives=3 enemies.cheat=true enemies.cheat.level=noGoodRotten secret.code.passphrase=UUDDLRLRBABAs secret.code.allowed=true secret.code.lives=30 ui.properties: | color.good=purple color.bad=yellow allow.textmode=true how.nice.to.look=fairlyNice kind: ConfigMap metadata: creationTimestamp: \u0026#34;2022-03-15T01:27:41Z\u0026#34; name: game namespace: default resourceVersion: \u0026#34;161625\u0026#34; uid: 784d08a8-1d6e-4f5f-ab2e-330348d4bf69 # 也可以通过describe命令查看 [root@k8s-master01 configmap]# kubectl describe cm game Name: game Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Data ==== game.properties: ---- enemies=aliens lives=3 enemies.cheat=true enemies.cheat.level=noGoodRotten secret.code.passphrase=UUDDLRLRBABAs secret.code.allowed=true secret.code.lives=30 ui.properties: ---- color.good=purple color.bad=yellow allow.textmode=true how.nice.to.look=fairlyNice BinaryData # 用来保存二进制数据作为base64编码字串 ==== Events: \u0026lt;none\u0026gt; 文件方式 --from-file指定对应配置文件, 可以多次使用引入不同的配置文件\n[root@k8s-master01 configmap]# kubectl create configmap game-ui --from-file=/root/yamls/configmap/ui.properties configmap/game-ui created [root@k8s-master01 configmap]# kubectl get cm NAME DATA AGE game 2 6m44s game-ui 1 14s [root@k8s-master01 configmap]# kubectl describe cm game-ui Name: game-ui Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Data ==== ui.properties: ---- color.good=purple color.bad=yellow allow.textmode=true how.nice.to.look=fairlyNice BinaryData ==== Events: \u0026lt;none\u0026gt; 命令行键值方式 --from-literal传递配置参数, 可多次使用\n[root@k8s-master01 configmap]# kubectl create configmap game-cli --from-literal=game.name=gogogo --from-literal=\u0026#34;game.date=2022-03\u0026#34; configmap/game-cli created [root@k8s-master01 configmap]# kubectl describe cm game-cli Name: game-cli Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Data ==== game.date: ---- 2022-03 game.name: ---- gogogo BinaryData ==== Events: \u0026lt;none\u0026gt; YAML文件方式 本质上还是上面三种方式之一\n[root@k8s-master01 configmap]# vim game-yaml.yaml apiVersion: v1 kind: ConfigMap metadata: name: game-yaml data: # 类属性键;每一个键都映射到一个简单的值 player_initial_lives: \u0026#34;3\u0026#34; ui_properties_file_name: \u0026#39;user-interface.properties\u0026#39; # 类文件键 game.properties: | enemy.types=aliens,monsters player.maximum-lives=5 user-interface.properties: | color.good=green color.bad=yellow [root@k8s-master01 configmap]# kubectl apply -f game-yaml.yaml configmap/game-yaml created [root@k8s-master01 configmap]# kubectl describe cm game-yaml Name: game-yaml Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Data ==== game.properties: ---- enemy.types=aliens,monsters player.maximum-lives=5 player_initial_lives: ---- 3 ui_properties_file_name: ---- user-interface.properties user-interface.properties: ---- color.good=green color.bad=yellow BinaryData ==== Events: \u0026lt;none\u0026gt; Pod中使用ConfigMap 在Pod中加载ConfigMap配置\n# 查看当前的configmap [root@k8s-master01 configmap]# kubectl get cm NAME DATA AGE game 2 16m game-cli 2 5m13s game-ui 1 10m game-yaml 4 2m32s 作为环境变量引用 [root@k8s-master01 configmap]# vim pod-from-cm.yaml apiVersion: v1 kind: Pod metadata: name: pod-from-cm spec: containers: - name: myapp image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;env\u0026#34;] ### 引用方式1，直接应用键值对 env: - name: GAME_NAME # 指定环境变量名 valueFrom: configMapKeyRef: name: game-cli ### 这个name的值来自 ConfigMap key: game.name ### 这个key的值为需要取值的键 - name: GAME_DATE valueFrom: configMapKeyRef: name: game-cli key: game.date ### 引用方式2，引用ConfigMap文件 envFrom: - configMapRef: name: game ### 这个name的值来自 ConfigMap restartPolicy: Never [root@k8s-master01 configmap]# kubectl apply -f pod-from-cm.yaml pod/pod-from-cm created [root@k8s-master01 configmap]# kubectl get po pod-from-cm NAME READY STATUS RESTARTS AGE pod-from-cm 0/1 Completed 0 5s # 查看Pod日志，可见相关环境变量已经导入 [root@k8s-master01 configmap]# kubectl logs pod-from-cm KUBERNETES_SERVICE_PORT=443 MYAPP_SVC_PORT_80_TCP_ADDR=10.98.57.156 KUBERNETES_PORT=tcp://10.96.0.1:443 ui.properties=color.good=purple color.bad=yellow allow.textmode=true how.nice.to.look=fairlyNice MYAPP_SVC_PORT_80_TCP_PORT=80 HOSTNAME=pod-from-cm SHLVL=1 GAME_DATE=2022-03 MYAPP_SVC_PORT_80_TCP_PROTO=tcp HOME=/root NGINX_PORT_80_TCP=tcp://10.100.71.223:80 game.properties=enemies=aliens lives=3 enemies.cheat=true enemies.cheat.level=noGoodRotten secret.code.passphrase=UUDDLRLRBABAs secret.code.allowed=true secret.code.lives=30 GAME_NAME=gogogo ... 输出环境变量 # 创建yaml文件 [root@k8s-master01 configmap]# vim pod-cm-env.yaml apiVersion: v1 kind: Pod metadata: name: pod-cm-env spec: containers: - name: myapp image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;echo \\\u0026#34;Game name is $(GAME_NAME_AGAIN)\\nGame date is $(GAME_DATE_AGAIN)\\\u0026#34;\u0026#34;] env: - name: GAME_NAME_AGAIN valueFrom: configMapKeyRef: name: game-cli key: game.name - name: GAME_DATE_AGAIN valueFrom: configMapKeyRef: name: game-cli key: game.date restartPolicy: Never [root@k8s-master01 configmap]# kubectl apply -f pod-cm-env.yaml pod/pod-cm-env created [root@k8s-master01 configmap]# kubectl get po pod-cm-env NAME READY STATUS RESTARTS AGE pod-cm-env 0/1 Completed 0 6s [root@k8s-master01 configmap]# kubectl logs pod-cm-env Game name is gogogo Game date is 2022-03 数据卷中使用 [root@k8s-master01 configmap]# vim po-cm-vol.yaml apiVersion: v1 kind: Pod metadata: name: pod-cm-vol spec: containers: - name: myapp image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;sleep 600\u0026#34;] # 延迟十分钟再进入completed volumeMounts: - name: config-volume mountPath: \u0026#34;/config\u0026#34; readOnly: true volumes: - name: config-volume configMap: name: game items: # 如果不指定，则调用该cm下所有配置 - key: \u0026#34;game.properties\u0026#34; path: \u0026#34;game.properties\u0026#34; restartPolicy: Never [root@k8s-master01 configmap]# kubectl apply -f pod-cm-vol.yaml pod/pod-cm-vol created [root@k8s-master01 configmap]# kubectl get po pod-cm-vol NAME READY STATUS RESTARTS AGE pod-cm-vol 1/1 Running 0 8s # 确认cm配置 [root@k8s-master01 configmap]# kubectl describe cm game Name: game Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Data ==== game.properties: ---- enemies=aliens lives=3 enemies.cheat=true enemies.cheat.level=noGoodRotten secret.code.passphrase=UUDDLRLRBABAs secret.code.allowed=true secret.code.lives=30 ui.properties: ---- color.good=purple color.bad=yellow allow.textmode=true how.nice.to.look=fairlyNice BinaryData ==== Events: \u0026lt;none\u0026gt; # 进入pod-cm-vol内查看配置挂载的文件 [root@k8s-master volume]# kubectl exec -it pod-configmap-volume sh [root@k8s-master01 configmap]# kubectl exec -it pod-cm-vol -- sh / # ls /config game.properties / # cat /config/game.properties # 此文件链接的文件为只读文件，通常是真实数据文件的软链接 enemies=aliens lives=3 enemies.cheat=true enemies.cheat.level=noGoodRotten secret.code.passphrase=UUDDLRLRBABAs secret.code.allowed=true secret.code.lives=30 ConfigMap热更新 准备工作 # 创建yaml文件,创建ConfigMap和Deploy [root@k8s-master01 configmap]# vim pod-cm-hot.yaml app: myapp matchLabels: selector: replicas: 2 release: v1 apiVersion: v1 kind: ConfigMap metadata: name: cm-loglevel namespace: default data: log_level: INFO --- apiVersion: apps/v1 kind: Deployment metadata: name: mydeploy namespace: default spec: replicas: 2 selector: matchLabels: app: myapp release: v1 template: metadata: labels: app: myapp release: v1 spec: containers: - name: myapp image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 imagePullPolicy: IfNotPresent ports: - containerPort: 80 volumeMounts: - name: config-volume mountPath: \u0026#34;/config\u0026#34; volumes: - name: config-volume configMap: name: cm-loglevel [root@k8s-master01 configmap]# kubectl apply -f pod-cm-hot.yaml configmap/cm-loglevel created deployment.apps/mydeploy created [root@k8s-master01 configmap]# kubectl get deployments.apps mydeploy NAME READY UP-TO-DATE AVAILABLE AGE mydeploy 2/2 2 2 16s [root@k8s-master01 configmap]# kubectl get cm cm-loglevel NAME DATA AGE cm-loglevel 1 28s [root@k8s-master01 configmap]# kubectl describe cm cm-loglevel Name: cm-loglevel Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Data ==== log_level: ---- INFO BinaryData ==== Events: \u0026lt;none\u0026gt; [root@k8s-master01 configmap]# kubectl get po -owide | grep mydeploy mydeploy-77c886996d-wbnrs 1/1 Running 0 100s 172.18.195.14 k8s-master03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; mydeploy-77c886996d-zrg8n 1/1 Running 0 101s 172.27.14.204 k8s-node02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; # 查看pod中的log_level [root@k8s-master01 configmap]# kubectl exec -it mydeploy-77c886996d-wbnrs -- cat /config/log_level INFO 热更新log_level # 修改log_level=DEBUG [root@k8s-master01 configmap]# kubectl edit cm cm-loglevel apiVersion: v1 data: log_level: DEBUG kind: ConfigMap ... # 查看对应的configmap, log_level已修改为DEBUG [root@k8s-master01 configmap]# kubectl describe cm cm-loglevel Name: cm-loglevel Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Data ==== log_level: ---- DEBUG BinaryData ==== Events: \u0026lt;none\u0026gt; # 查看pod中log_level已修改为DEBUG [root@k8s-master01 configmap]# kubectl exec -it mydeploy-77c886996d-wbnrs -- cat /config/log_level DEBUG 不可变的ConfigMap 为了确保ConfigMap不会被无意中修改，可以设置不可变的ConfigMap。此功能特性由ImmutableEphemeralVolumes控制。可以通过将immutable字段设置为true创建不可变更的ConfigMap。一旦某ConfigMap被标记为不可变更，则无法逆转这一变化，也无法更改data或binaryData字段的内容。只能删除并重建 ConfigMap。\n[root@k8s-master01 configmap]# vim cm-immutable.yaml apiVersion: v1 kind: ConfigMap metadata: name: cm-immutable data: name: deemoprobe date: 2022-03 label: test immutable: true [root@k8s-master01 configmap]# kubectl apply -f cm-immutable.yaml configmap/cm-immutable created [root@k8s-master01 configmap]# kubectl describe cm cm-immutable Name: cm-immutable Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Data ==== name: ---- deemoprobe date: ---- 2022-03 label: ---- test BinaryData ==== Events: \u0026lt;none\u0026gt; # 尝试修改yaml文件应用时会提示如下 [root@k8s-master01 configmap]# kubectl apply -f cm-immutable.yaml The ConfigMap \u0026#34;cm-immutable\u0026#34; is invalid: data: Forbidden: field is immutable when `immutable` is set # 尝试使用edit命令修改会提示如下 [root@k8s-master01 configmap]# kubectl edit cm cm-immutable # configmaps \u0026#34;cm-immutable\u0026#34; was not valid: # * data: Forbidden: field is immutable when `immutable` is set ","permalink":"https://deemoprobe.github.io/posts/tech/kubernetes/kubernetesconfigmap/","summary":"ConfigMap 概念 传统架构中，配置文件一般是保存在宿主机上，程序启动时指定特定配置文件即可。但kubernetes中容器部署是灵活的，容器所在节点并不固定，所以传统方式就不适用了。如果在构建镜像时将配置文件一起打包进镜像随容器一起启动，又不够方便，镜像经常变更配置文件是很麻烦的。基于此，ku","title":"KubernetesConfigMap"},{"content":"实例说明 创建运行在Tomcat里面的Web APP，实现JSP页面通过jdbc直接访问MySQL数据库在页面上展示并动态添加数据。基于书籍《Kubernetes权威指南》中的一个实验。 需要两个容器: Web APP 和 MySQL\n创建MySQL # 创建RC cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; mysql_rc.yaml apiVersion: v1 # 定义为 RC (副本控制器) # ReplicationSet目前在替代ReplicationController的写法,意义相同 kind: ReplicationController metadata: # RC的名称,全局唯一 name: mysql spec: # 希望创建的pod个数 replicas: 1 selector: # 选择符合该标签的pod app: mysql # 根据模板下的定义来创建pod template: metadata: labels: # pod的标签,对应RC的selector app: mysql # 定义pod规则 spec: # pod内容器的定义 containers: # 容器名称 - name: mysql # 容器所使用的的镜像(不指定版本的话就默认拉取最新版) # 由于最新版驱动的问题, 所以最好使用指定版本 image: mysql:5.6 ports: # 开放的端口号 - containerPort: 3306 # 容器环境变量 env: - name: MYSQL_ROOT_PASSWORD value: \u0026#34;123456\u0026#34; EOF kubectl create -f mysql_rc.yaml # 创建 SVC cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; mysql_svc.yaml apiVersion: v1 kind: Service metadata: name: mysql spec: ports: - port: 3306 selector: app: mysql EOF kubectl create -f mysql_svc.yaml [root@k8main manifests]# kubectl get pods,svc NAME READY STATUS RESTARTS AGE pod/mysql-8d27z 1/1 Running 0 6m42s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/mysql ClusterIP 192.168.68.128 \u0026lt;none\u0026gt; 3306/TCP 10s # 查看pod状态 kubectl describe po mysql 创建 MyWeb APP # 创建RC cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; myweb_rc.yaml apiVersion: v1 kind: ReplicationController metadata: name: myweb spec: replicas: 2 selector: app: myweb template: metadata: labels: app: myweb spec: containers: - name: myweb image: kubeguide/tomcat-app:v1 ports: - containerPort: 8080 env: - name: MYSQL_SERVICE_HOST # 这里的IP是名为MySQL的pod虚拟IP(CLUSTER-IP) value: 192.168.68.128 - name: MYSQL_SERVICE_PORT value: \u0026#34;3306\u0026#34; EOF kubectl create -f myweb_rc.yaml # 创建 SVC cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; myweb_svc.yaml apiVersion: v1 kind: Service metadata: name: myweb spec: selector: app: myweb type: NodePort ports: # 本地服务的8080端口映射到外部端口30001 - port: 8080 nodePort: 30001 EOF kubectl create -f myweb_svc.yaml 访问结果 访问地址：IP:30001/demo/\n","permalink":"https://deemoprobe.github.io/posts/tech/kubernetes/kuberneteswebdemo/","summary":"实例说明 创建运行在Tomcat里面的Web APP，实现JSP页面通过jdbc直接访问MySQL数据库在页面上展示并动态添加数据。基于书籍《Kubernetes权威指南》中的一个实验。 需要两个容器: Web APP 和 MySQL 创建MySQL # 创建RC cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; mysql_rc.yaml apiVersion: v1 # 定义为 RC (副本控制器) # Replic","title":"KubernetesWebDemo"},{"content":" Label：对k8s中各种资源进行分类、分组，添加一个属性标签 Selector：通过一个过滤的语法查找到对应标签的资源 这个deployment将启动3个Pod并监视始终要有3个Pod处于活跃状态，这个deployment管控Pod的方式就是通过标签app: nginx进行匹配；以标签为app: nginx的目标Service也是如此，通过选择标签进行对应标签Pod服务的发现。\nLabel Label（标签）是附加到 Kubernetes 对象（比如 Pods）上的key: value键值对。标签可以在创建时附加到对象，随后可以随时添加和修改。每个对象都可以定义一组键/值标签。\n标签键值对区分大小写 标签键值对必须以字母数字字符 ([a-z0-9A-Z]) 开头和结尾 标签键值可以包含短线(-)、下划线 (_)、点 (.) 标签键不能为空，值可以 标签键支持可选前缀，以斜杠 (/) 分隔 标签键（无前缀）和值必须为 63 个字符或更少 标签示例：\nrelease: stable, release: beta environment: dev, environment: test, environment: prod tier: frontend, tier: backend, tier: cache partition: customerA, partition: customerB track: daily, track: weekly # 常见配置方式 apiVersion: xxx metadata: labels: key1: value1 key2: value2 ... # 命令行用法 Usage: kubectl label [--overwrite] (-f FILENAME | TYPE NAME) KEY_1=VAL_1 ... KEY_N=VAL_N [--resource-version=version] # 默认情况下，节点都会预设下面的标签 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node01,kubernetes.io/os=linux # 创建实例deploy和service [root@k8s-master01 ~]# mkdir labelselector [root@k8s-master01 ~]# cd labelselector/ [root@k8s-master01 labelselector]# vim nginx-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: # has to match .spec.template.metadata.labels app: nginx template: metadata: labels: app: nginx # has to match .spec.selector.matchLabels spec: containers: - name: nginx image: nginx:1.18.0 imagePullPolicy: IfNotPresent ports: - containerPort: 80 [root@k8s-master01 labelselector]# vim my-service.yaml apiVersion: v1 kind: Service metadata: name: my-service spec: selector: app: nginx ports: - protocol: TCP port: 80 targetPort: 9376 [root@k8s-master01 labelselector]# kubectl apply -f . service/my-service created deployment.apps/nginx-deployment created [root@k8s-master01 labelselector]# kubectl get deployments.apps,po,svc NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-deployment 3/3 3 3 27s NAME READY STATUS RESTARTS AGE pod/nginx-deployment-79fccc485-pj665 1/1 Running 0 26s pod/nginx-deployment-79fccc485-pxp49 1/1 Running 0 27s pod/nginx-deployment-79fccc485-vw6np 1/1 Running 0 26s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/my-service ClusterIP 10.109.60.44 \u0026lt;none\u0026gt; 80/TCP 27s 查询Label [root@k8s-master01 labelselector]# kubectl get deployments.apps --show-labels NAME READY UP-TO-DATE AVAILABLE AGE LABELS nginx-deployment 3/3 3 3 80s app=nginx [root@k8s-master01 labelselector]# kubectl get po --show-labels NAME READY STATUS RESTARTS AGE LABELS nginx-deployment-79fccc485-pj665 1/1 Running 0 2m26s app=nginx,pod-template-hash=79fccc485 nginx-deployment-79fccc485-pxp49 1/1 Running 0 2m27s app=nginx,pod-template-hash=79fccc485 nginx-deployment-79fccc485-vw6np 1/1 Running 0 2m26s app=nginx,pod-template-hash=79fccc485 # 也可查看SELECTOR [root@k8s-master01 labelselector]# kubectl get deployments.apps -owide NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR nginx-deployment 3/3 3 3 73s nginx nginx:1.18.0 app=nginx 新增Label [root@k8s-master01 labelselector]# kubectl label deployments.apps nginx-deployment environment=test deployment.apps/nginx-deployment labeled [root@k8s-master01 labelselector]# kubectl get deployments.apps --show-labels NAME READY UP-TO-DATE AVAILABLE AGE LABELS nginx-deployment 3/3 3 3 6m59s app=nginx,environment=test 修改Label # 修改已存在的标签 [root@k8s-master01 labelselector]# kubectl label --overwrite deployments.apps nginx-deployment environment=uat deployment.apps/nginx-deployment unlabeled [root@k8s-master01 labelselector]# kubectl get deployments.apps --show-labels NAME READY UP-TO-DATE AVAILABLE AGE LABELS nginx-deployment 3/3 3 3 8m8s app=nginx,environment=uat 删除Label [root@k8s-master01 labelselector]# kubectl label deployments.apps nginx-deployment environment- deployment.apps/nginx-deployment unlabeled [root@k8s-master01 labelselector]# kubectl get deployments.apps --show-labels NAME READY UP-TO-DATE AVAILABLE AGE LABELS nginx-deployment 3/3 3 3 12m app=nginx # 查看集群节点的标签 [root@k8s-master01 labelselector]# kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS k8s-master01 Ready control-plane,master 36d v1.23.0 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-master01,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node-role.kubernetes.io/master=,node.kubernetes.io/exclude-from-external-load-balancers= k8s-master02 Ready control-plane,master 36d v1.23.0 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-master02,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node-role.kubernetes.io/master=,node.kubernetes.io/exclude-from-external-load-balancers= k8s-master03 Ready control-plane,master 36d v1.23.0 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-master03,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node-role.kubernetes.io/master=,node.kubernetes.io/exclude-from-external-load-balancers= k8s-node01 Ready node 36d v1.23.0 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node01,kubernetes.io/os=linux,node-role.kubernetes.io/node= k8s-node02 Ready node 36d v1.23.0 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node02,kubernetes.io/os=linux,node-role.kubernetes.io/node= # 比如节点角色标签node-role.kubernetes.io/control-plane=,node-role.kubernetes.io/node= # 删除k8s-node02的节点角色标签 [root@k8s-master01 labelselector]# kubectl label nodes k8s-node02 node-role.kubernetes.io/node- node/k8s-node02 unlabeled # Role变为\u0026lt;none\u0026gt; [root@k8s-master01 labelselector]# kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS k8s-master01 Ready control-plane,master 36d v1.23.0 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-master01,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node-role.kubernetes.io/master=,node.kubernetes.io/exclude-from-external-load-balancers= k8s-master02 Ready control-plane,master 36d v1.23.0 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-master02,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node-role.kubernetes.io/master=,node.kubernetes.io/exclude-from-external-load-balancers= k8s-master03 Ready control-plane,master 36d v1.23.0 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-master03,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node-role.kubernetes.io/master=,node.kubernetes.io/exclude-from-external-load-balancers= k8s-node01 Ready node 36d v1.23.0 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node01,kubernetes.io/os=linux,node-role.kubernetes.io/node= k8s-node02 Ready \u0026lt;none\u0026gt; 36d v1.23.0 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node02,kubernetes.io/os=linux Selector Selector（标签选择器）主要用于资源的匹配，只有符合条件的资源才会被调用或使用，可以使用该方式对集群中的各类资源进行分配。\nKubernetes API目前支持两种类型的Selector：基于等值的 和 基于集合的。Selector可以同时匹配多个Label，此时需要多个Label同时满足才会被Select\n# 匹配同时具备app=nginx和role=lb两个标签的Pod selector: app: nginx role: lb 等值需求 基于等值或基于不等值 的需求允许按标签键和值进行过滤。 匹配对象必须满足所有指定的标签约束，尽管它们也可能具有其他标签。 可接受的运算符有=、==和!=三种。 前两个表示相等（并且只是同义词），而后者表示不相等。\n集合需求 基于集合的标签需求允许你通过一组值来过滤键。支持三种操作符：in、notin和exists(只可以用在键标识符上)。\n# --selector参数进行标签的选择，可简写为-l [root@k8s-master01 labelselector]# kubectl get deployments.apps --selector app=nginx NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 3/3 3 3 14m # 新创建一个deployment，作为对照 [root@k8s-master01 labelselector]# kubectl create deployment --image=nginx other deployment.apps/other created [root@k8s-master01 labelselector]# kubectl get deployments.apps --show-labels -owide NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR LABELS nginx-deployment 3/3 3 3 17m nginx nginx:1.18.0 app=nginx app=nginx other 1/1 1 1 58s nginx nginx app=other app=other # 等值查询，并在查询后展示标签 [root@k8s-master01 labelselector]# kubectl get deployments.apps -l app=other --show-labels NAME READY UP-TO-DATE AVAILABLE AGE LABELS other 1/1 1 1 2m6s app=other # 等值反向查询 [root@k8s-master01 labelselector]# kubectl get deployments.apps -l app!=other --show-labels NAME READY UP-TO-DATE AVAILABLE AGE LABELS nginx-deployment 3/3 3 3 18m app=nginx # 集合查询 [root@k8s-master01 labelselector]# kubectl get deployments.apps -l \u0026#39;app in (other,nginx)\u0026#39; --show-labels NAME READY UP-TO-DATE AVAILABLE AGE LABELS nginx-deployment 3/3 3 3 21m app=nginx other 1/1 1 1 5m11s app=other # 再创建一个对照 [root@k8s-master01 labelselector]# vim busybox.yaml apiVersion: v1 kind: Pod metadata: name: busybox namespace: default spec: containers: - name: busybox image: busybox:1.28 command: - sleep - \u0026#34;3600\u0026#34; imagePullPolicy: IfNotPresent restartPolicy: Always [root@k8s-master01 labelselector]# kubectl apply -f busybox.yaml pod/busybox created [root@k8s-master01 labelselector]# kubectl get po --show-labels NAME READY STATUS RESTARTS AGE LABELS busybox 1/1 Running 0 17s \u0026lt;none\u0026gt; nginx-deployment-79fccc485-pj665 1/1 Running 0 27m app=nginx,pod-template-hash=79fccc485 nginx-deployment-79fccc485-pxp49 1/1 Running 0 27m app=nginx,pod-template-hash=79fccc485 nginx-deployment-79fccc485-vw6np 1/1 Running 0 27m app=nginx,pod-template-hash=79fccc485 other-657995d88d-pcf7h 1/1 Running 0 11m app=other,pod-template-hash=657995d88d [root@k8s-master01 labelselector]# kubectl label po busybox name=busybox pod/busybox labeled [root@k8s-master01 labelselector]# kubectl get po --show-labels NAME READY STATUS RESTARTS AGE LABELS busybox 1/1 Running 0 61s name=busybox nginx-deployment-79fccc485-pj665 1/1 Running 0 28m app=nginx,pod-template-hash=79fccc485 nginx-deployment-79fccc485-pxp49 1/1 Running 0 28m app=nginx,pod-template-hash=79fccc485 nginx-deployment-79fccc485-vw6np 1/1 Running 0 28m app=nginx,pod-template-hash=79fccc485 other-657995d88d-pcf7h 1/1 Running 0 11m app=other,pod-template-hash=657995d88d # 集合反向查询 [root@k8s-master01 labelselector]# kubectl get po -l \u0026#39;app notin (nginx,other)\u0026#39; --show-labels NAME READY STATUS RESTARTS AGE LABELS busybox 1/1 Running 0 4m49s name=busybox # 删除匹配了相应标签的资源 [root@k8s-master01 labelselector]# kubectl delete deployments.apps -l app=other deployment.apps \u0026#34;other\u0026#34; deleted [root@k8s-master01 labelselector]# kubectl get deployments.apps NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 3/3 3 3 33m ","permalink":"https://deemoprobe.github.io/posts/tech/kubernetes/kuberneteslabelselector/","summary":"Label：对k8s中各种资源进行分类、分组，添加一个属性标签 Selector：通过一个过滤的语法查找到对应标签的资源 这个deployment将启动3个Pod并监视始终要有3个Pod处于活跃状态，这个deployment管控Pod的方式就是通过标签app: nginx进行匹配；以标","title":"KubernetesLabelSelector"},{"content":"Kubernetes 中内建了很多 Controller(控制器)，用来确保pod资源符合预期的状态，控制 Pod 的状态和行为。\n控制器类型 ReplicationController(rc) ReplicaSet(rs) Deployment(deploy) DaemonSet(ds) StatefulSet(sts) Job/CronJob(cj) Horizontal Pod Autoscaling(hpa) # 可以使用kubectl explain命令查看k8s API资源对象描述信息 kubectl explain \u0026lt;apiresource-name\u0026gt; | egrep -v \u0026#34;^$|^\\s*http\u0026#34; | sed s/\u0026#34;More info:\u0026#34;// # 查看API资源列表 kubectl api-resources RS和RC ReplicationController(RC)用来确保容器应用的副本数始终保持在用户定义的副本数，即如果有容器异常退出，会自动创建新的 Pod 来替代；而如果异常多出来的容器也会自动回收。\n在新版本的Kubernetes中建议使用ReplicaSet来取代ReplicationController，ReplicaSet规则跟ReplicationController没有本质的不同，唯一区别是ReplicaSet支持selector选择器。\nReplicaSet也很少单独被使用，都是使用更高级的资源Deployment、DaemonSet、StatefulSet进行管理Pod，ReplicaSet主要用作Deployment协调创建、删除和更新Pod。\n# RC实例，也是后面的rc-demo.yaml中的内容 apiVersion: v1 kind: ReplicationController metadata: name: nginx spec: replicas: 2 selector: app: nginx template: metadata: name: nginx labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - containerPort: 80 # RS实例，和RC配置的主要区别是多了个selector apiVersion: apps/v1 kind: ReplicaSet metadata: name: frontend labels: app: guestbook tier: frontend spec: replicas: 2 selector: matchLabels: tier: frontend template: metadata: labels: tier: frontend spec: containers: - name: php-redis image: gcr.io/google_samples/gb-frontend:v3 # 以上面RC实例配置为例 [root@k8s-master01 ~]# kubectl apply -f rc-demo.yaml replicationcontroller/nginx created [root@k8s-master01 ~]# kubectl get po,rc NAME READY STATUS RESTARTS AGE pod/nginx-5w6v6 1/1 Running 0 6s pod/nginx-8265c 1/1 Running 0 6s NAME DESIRED CURRENT READY AGE replicationcontroller/nginx 2 2 2 6s # RC的Pod动态缩放 [root@k8s-master01 ~]# kubectl scale rc nginx --replicas=3 replicationcontroller/nginx scaled # 可以看到正在扩容 [root@k8s-master01 ~]# kubectl get po,rc NAME READY STATUS RESTARTS AGE pod/nginx-5w6v6 1/1 Running 0 97s pod/nginx-8265c 1/1 Running 0 97s pod/nginx-jhsjr 0/1 ContainerCreating 0 3s NAME DESIRED CURRENT READY AGE replicationcontroller/nginx 3 3 2 97s # 扩容成功 [root@k8s-master01 ~]# kubectl get po,rc NAME READY STATUS RESTARTS AGE pod/nginx-5w6v6 1/1 Running 0 2m17s pod/nginx-8265c 1/1 Running 0 2m17s pod/nginx-jhsjr 1/1 Running 0 43s # 缩容测试 [root@k8s-master01 ~]# kubectl scale rc nginx --replicas=1 replicationcontroller/nginx scaled [root@k8s-master01 ~]# kubectl get po,rc NAME READY STATUS RESTARTS AGE pod/nginx-5w6v6 1/1 Running 0 4m26s NAME DESIRED CURRENT READY AGE replicationcontroller/nginx 1 1 1 4m26s Deployment 用于部署无状态的服务，最常用的控制器。一般用于管理维护企业内部无状态的微服务，比如configserver、zuul、springboot。可以管理多个副本的Pod实现无缝迁移、自动扩容缩容、自动灾难恢复、一键回滚等功能。\nDeployment是一种更高级别的 API 资源对象，为 Pods 和 ReplicaSets 提供声明式的更新能力。它以类似于 kubectl rolling-update 的方式更新其底层 ReplicaSet 及其 Pod。\nDeployments 的典型用例：\n创建Deployment将ReplicaSet上线。ReplicaSet 在后台创建 Pods.检查 ReplicaSet 的上线状态，查看其是否成功 通过更新 Deployment 的 PodTemplateSpec，声明 Pod 的新状态。新的 ReplicaSet 会被创建，Deployment 将 Pod 从旧 ReplicaSet 迁移到新 ReplicaSet。 每个新的 ReplicaSet 都会更新 Deployment 的修订版本 如果 Deployment 的当前状态不稳定，回滚到较早的 Deployment 版本。每次回滚都会更新 Deployment 的修订版本 扩大 Deployment 规模以承担更多负载 暂停 Deployment 以应用对 PodTemplateSpec 所作的多项修改，然后恢复其执行以启动新的上线版本 使用 Deployment 状态来判定上线过程是否出现停滞 清理较旧的不再需要的 ReplicaSet 创建Deployment Deployment创建一个ReplicaSet，该RS负责启动管理三个Pod。\n# 创建yaml文件 [root@k8s-master01 ~]# vim nginx-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deploy labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.16.1 imagePullPolicy: IfNotPresent ports: - containerPort: 80 [root@k8s-master01 ~]# kubectl apply -f nginx-deploy.yaml deployment.apps/nginx-deploy created [root@k8s-master01 ~]# kubectl get deploy,rs,po NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-deploy 3/3 3 3 58s NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-deploy-ff6655784 3 3 3 58s NAME READY STATUS RESTARTS AGE pod/nginx-deploy-ff6655784-2jxbv 1/1 Running 0 58s pod/nginx-deploy-ff6655784-b8gq8 1/1 Running 0 58s pod/nginx-deploy-ff6655784-qj2k2 1/1 Running 0 58s # 查看标签 [root@k8s-master01 ~]# kubectl get po --show-labels NAME READY STATUS RESTARTS AGE LABELS nginx-deploy-ff6655784-2jxbv 1/1 Running 0 98s app=nginx,pod-template-hash=ff6655784 nginx-deploy-ff6655784-b8gq8 1/1 Running 0 98s app=nginx,pod-template-hash=ff6655784 nginx-deploy-ff6655784-qj2k2 1/1 Running 0 98s app=nginx,pod-template-hash=ff6655784 # 扩缩容 [root@k8s-master01 ~]# kubectl scale deployment nginx-deploy --replicas=2 deployment.apps/nginx-deploy scaled [root@k8s-master01 ~]# kubectl get deploy,rs,po NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-deploy 2/2 2 2 2m18s NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-deploy-ff6655784 2 2 2 2m18s NAME READY STATUS RESTARTS AGE pod/nginx-deploy-ff6655784-b8gq8 1/1 Running 0 2m18s pod/nginx-deploy-ff6655784-qj2k2 1/1 Running 0 2m18s # 查看pod详情 [root@k8s-master01 ~]# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-deploy-ff6655784-b8gq8 1/1 Running 0 2m40s 172.25.92.66 k8s-master02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deploy-ff6655784-qj2k2 1/1 Running 0 2m40s 172.17.125.18 k8s-node01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; # 访问nginx [root@k8s-master01 ~]# curl 172.25.92.66 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; ... \u0026lt;/html\u0026gt; [root@k8s-master01 ~]# curl 172.17.125.18 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; ... \u0026lt;/html\u0026gt; 说明:\nReplicaSet 的名称始终被格式化为[DEPLOYMENT-NAME]-[RANDOM-STRING]。RANDOM-STRING随机生成。并使用pod-template-hash=[RANDOM-STRING]作为选择器和标签 Deployment 控制器将pod-template-hash=[RANDOM-STRING]标签添加到 Deployment 创建或使用的ReplicaSet和Pod。此标签可确保 Deployment的子ReplicaSets不重叠，因此不可修改 注意Deployment、ReplicaSet和Pod三者的名称关系 [root@k8s-master01 ~]# kubectl get rs --show-labels NAME DESIRED CURRENT READY AGE LABELS nginx-deploy-ff6655784 2 2 2 6m49s app=nginx,pod-template-hash=ff6655784 [root@k8s-master01 ~]# kubectl get deploy,rs,po --show-labels NAME READY UP-TO-DATE AVAILABLE AGE LABELS deployment.apps/nginx-deploy 2/2 2 2 8m2s app=nginx NAME DESIRED CURRENT READY AGE LABELS replicaset.apps/nginx-deploy-ff6655784 2 2 2 8m2s app=nginx,pod-template-hash=ff6655784 NAME READY STATUS RESTARTS AGE LABELS pod/nginx-deploy-ff6655784-b8gq8 1/1 Running 0 8m2s app=nginx,pod-template-hash=ff6655784 pod/nginx-deploy-ff6655784-qj2k2 1/1 Running 0 8m2s app=nginx,pod-template-hash=ff6655784 更新Deployment Deployment 可确保在更新时仅关闭一定数量的 Pods.默认情况下，它确保至少 75% 所需 Pods 在运行(25%为容忍的最大不可用量)。更新时不会先删除旧的pod，而是先新建一个pod.新pod运行时，才会删除对应老的pod。一切的前提都是为了满足上述的条件。\n备注: 如果需要更新Deployment，最好通过yaml文件更新，这样回滚到任何版本都非常便捷，而且更容易追溯。\n# 方式一: 直接修改镜像[不推荐] # 执行下面命令后修改对于镜像版本即可, 该方法不会记录命令,通过kubectl rollout history deployment/nginx-deploy 无法查询 [root@k8s-master01 ~]# kubectl edit deploy/nginx-deploy # 方式二: 命令行更新image[可使用]，但也提示--record即将废弃 [root@k8s-master01 ~]# kubectl set image deploy/nginx-deploy nginx=nginx:1.18.0 --record Flag --record has been deprecated, --record will be removed in the future deployment.apps/nginx-deploy image updated [root@k8s-master01 ~]# kubectl rollout history deployment nginx-deploy deployment.apps/nginx-deploy REVISION CHANGE-CAUSE 1 \u0026lt;none\u0026gt; 2 kubectl set image deploy/nginx-deploy nginx=nginx:1.18.0 --record=true [root@k8s-master01 ~]# kubectl get deployments.apps -owide NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR nginx-deploy 2/2 2 2 16m nginx nginx:1.18.0 app=nginx # 方式三: 使用yaml文件更新版本 # 当前nginx 版本是1.16.1, 更新到1.18.0 # 先删除之前更新的deploy，在重新启动 [root@k8s-master01 ~]# kubectl delete -f nginx-deploy.yaml deployment.apps \u0026#34;nginx-deploy\u0026#34; deleted [root@k8s-master01 ~]# kubectl apply -f nginx-deploy.yaml deployment.apps/nginx-deploy created [root@k8s-master01 ~]# kubectl get deployments.apps -owide NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR nginx-deploy 3/3 3 3 35s nginx nginx:1.16.1 app=nginx # 拷贝一份yaml文件，二者重命名（老版保留以便有问题时回退），编辑新文件镜像为1.18.0 [root@k8s-master01 ~]# ll total 8 -rw-r--r-- 1 root root 337 Dec 21 09:59 nginx-deploy-1161.yaml -rw-r--r-- 1 root root 337 Dec 21 11:01 nginx-deploy-1180.yaml [root@k8s-master01 ~]# cat nginx-deploy-1161.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deploy labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.16.1 ports: - containerPort: 80 [root@k8s-master01 ~]# cat nginx-deploy-1180.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deploy labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.18.0 ports: - containerPort: 80 # 应用镜像为1180的同样的配置文件 # --record 参数可以记录命令,通过 kubectl rollout history deployment/nginx-deployment 可查询 [root@k8s-master01 ~]# kubectl apply -f nginx-deploy-1180.yaml --record deployment.apps/nginx-deploy configured # 如果正在更新, 可看到下面日志 [root@k8s-master01 ~]# kubectl rollout status deploy/nginx-deploy Waiting for deployment \u0026#34;nginx-deploy\u0026#34; rollout to finish: 1 out of 3 new replicas have been updated... Waiting for deployment \u0026#34;nginx-deploy\u0026#34; rollout to finish: 1 out of 3 new replicas have been updated... Waiting for deployment \u0026#34;nginx-deploy\u0026#34; rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \u0026#34;nginx-deploy\u0026#34; rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \u0026#34;nginx-deploy\u0026#34; rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \u0026#34;nginx-deploy\u0026#34; rollout to finish: 2 old replicas are pending termination... Waiting for deployment \u0026#34;nginx-deploy\u0026#34; rollout to finish: 1 old replicas are pending termination... Waiting for deployment \u0026#34;nginx-deploy\u0026#34; rollout to finish: 1 old replicas are pending termination... deployment \u0026#34;nginx-deploy\u0026#34; successfully rolled out # 更新结束后, 可看到下面日志 [root@k8s-master01 ~]# kubectl rollout status deploy/nginx-deploy deployment \u0026#34;nginx-deploy\u0026#34; successfully rolled out 回滚Deployment # 回滚到上一个版本 [root@k8s-master01 ~]# kubectl rollout undo deployment nginx-deploy deployment.apps/nginx-deploy rolled back [root@k8s-master01 ~]# kubectl get deploy,rs,po -owide NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR deployment.apps/nginx-deploy 3/3 3 3 11m nginx nginx:1.16.1 app=nginx NAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTOR replicaset.apps/nginx-deploy-79fccc485 0 0 0 7m46s nginx nginx:1.18.0 app=nginx,pod-template-hash=79fccc485 replicaset.apps/nginx-deploy-ff6655784 3 3 3 11m nginx nginx:1.16.1 app=nginx,pod-template-hash=ff6655784 NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/nginx-deploy-ff6655784-c9gf9 1/1 Running 0 2m7s 172.25.92.69 k8s-master02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/nginx-deploy-ff6655784-xl4rx 1/1 Running 0 2m12s 172.27.14.228 k8s-node02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/nginx-deploy-ff6655784-zzpvm 1/1 Running 0 2m9s 172.17.125.22 k8s-node01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; # 直接用Yaml文件回退[更新]为对应镜像, 比如 1180--\u0026gt;1161 [root@k8s-master01 ~]# kubectl apply -f nginx-deploy-1161.yaml --record 回退历史版本 # 查看历史版本 [root@k8s-master01 ~]# kubectl rollout history deployment nginx-deploy deployment.apps/nginx-deploy REVISION CHANGE-CAUSE 2 kubectl apply --filename=nginx-deploy.yaml --record=true 3 \u0026lt;none\u0026gt; # 查看历史版本信息 [root@k8s-master01 ~]# kubectl rollout history deployment nginx-deploy --revision=2 deployment.apps/nginx-deploy with revision #2 Pod Template: Labels: app=nginx pod-template-hash=79fccc485 Annotations: kubernetes.io/change-cause: kubectl apply --filename=nginx-deploy.yaml --record=true Containers: nginx: Image: nginx:1.18.0 Port: 80/TCP Host Port: 0/TCP Environment: \u0026lt;none\u0026gt; Mounts: \u0026lt;none\u0026gt; Volumes: \u0026lt;none\u0026gt; # 可以使用--to-revision参数指定回退到某个历史版本 [root@k8s-master01 ~]# kubectl rollout undo deployment nginx-deploy --to-revision=2 deployment.apps/nginx-deploy rolled back [root@k8s-master01 ~]# kubectl get deploy,rs,po -owide NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR deployment.apps/nginx-deploy 3/3 3 3 14m nginx nginx:1.18.0 app=nginx NAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTOR replicaset.apps/nginx-deploy-79fccc485 3 3 3 10m nginx nginx:1.18.0 app=nginx,pod-template-hash=79fccc485 replicaset.apps/nginx-deploy-ff6655784 0 0 0 14m nginx nginx:1.16.1 app=nginx,pod-template-hash=ff6655784 NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/nginx-deploy-79fccc485-dwhhr 1/1 Running 0 49s 172.25.92.70 k8s-master02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/nginx-deploy-79fccc485-kp2pd 1/1 Running 0 51s 172.17.125.23 k8s-node01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/nginx-deploy-79fccc485-z425l 1/1 Running 0 53s 172.27.14.229 k8s-node02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 更新详情 [root@k8s-master01 ~]# kubectl describe deployments.apps nginx-deploy Name: nginx-deploy Namespace: default CreationTimestamp: Wed, 12 Jan 2022 13:17:12 +0800 Labels: app=nginx Annotations: deployment.kubernetes.io/revision: 4 kubernetes.io/change-cause: kubectl apply --filename=nginx-deploy.yaml --record=true Selector: app=nginx Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge # 更新策略 Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:1.18.0 Port: 80/TCP Host Port: 0/TCP Environment: \u0026lt;none\u0026gt; Mounts: \u0026lt;none\u0026gt; Volumes: \u0026lt;none\u0026gt; Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: \u0026lt;none\u0026gt; NewReplicaSet: nginx-deploy-79fccc485 (3/3 replicas created) Events: # 可以看到之前的操作记录和更新记录 Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 7m30s deployment-controller Scaled up replica set nginx-deploy-ff6655784 to 1 Normal ScalingReplicaSet 7m27s deployment-controller Scaled down replica set nginx-deploy-79fccc485 to 2 Normal ScalingReplicaSet 7m25s (x2 over 17m) deployment-controller Scaled up replica set nginx-deploy-ff6655784 to 3 Normal ScalingReplicaSet 7m23s (x3 over 7m27s) deployment-controller (combined from similar events): Scaled down replica set nginx-deploy-79fccc485 to 0 Normal ScalingReplicaSet 3m16s (x2 over 13m) deployment-controller Scaled up replica set nginx-deploy-79fccc485 to 1 Normal ScalingReplicaSet 3m14s (x2 over 13m) deployment-controller Scaled down replica set nginx-deploy-ff6655784 to 2 Normal ScalingReplicaSet 3m14s (x2 over 13m) deployment-controller Scaled up replica set nginx-deploy-79fccc485 to 2 Normal ScalingReplicaSet 3m12s (x2 over 13m) deployment-controller Scaled down replica set nginx-deploy-ff6655784 to 1 Normal ScalingReplicaSet 3m12s (x2 over 13m) deployment-controller Scaled up replica set nginx-deploy-79fccc485 to 3 Normal ScalingReplicaSet 3m10s (x2 over 10m) deployment-controller Scaled down replica set nginx-deploy-ff6655784 to 0 # 查看回滚状态 [root@k8s-master01 ~]# kubectl rollout status deploy nginx-deploy deployment \u0026#34;nginx-deploy\u0026#34; successfully rolled out # 历史记录 [root@k8s-master01 ~]# kubectl rollout history deploy nginx-deploy deployment.apps/nginx-deploy REVISION CHANGE-CAUSE 3 \u0026lt;none\u0026gt; 4 kubectl set image deployment/nginx-deploy nginx=nginx:1.18.0 --record=true 暂停与恢复 # 暂停 deployment 的更新，暂停后除非恢复更新，否则操作不会生效，当需要多项更新时可以使用 [root@k8s-master01 ~]# kubectl rollout pause deployment nginx-deploy deployment.apps/nginx-deploy paused [root@k8s-master01 ~]# kubectl set image deploy/nginx-deploy nginx=nginx:1.16.1 --record Flag --record has been deprecated, --record will be removed in the future deployment.apps/nginx-deploy image updated [root@k8s-master01 ~]# kubectl get deployments.apps -owide NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR nginx-deploy 3/3 0 3 25m nginx nginx:1.16.1 app=nginx # 可以看到镜像确实是更新了，但RS和Pod在用的还是老版本的，也就是并未生效 [root@k8s-master01 ~]# kubectl get rs,pod -owide NAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTOR replicaset.apps/nginx-deploy-79fccc485 3 3 3 22m nginx nginx:1.18.0 app=nginx,pod-template-hash=79fccc485 replicaset.apps/nginx-deploy-ff6655784 0 0 0 26m nginx nginx:1.16.1 app=nginx,pod-template-hash=ff6655784 NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/nginx-deploy-79fccc485-dwhhr 1/1 Running 0 12m 172.25.92.70 k8s-master02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/nginx-deploy-79fccc485-kp2pd 1/1 Running 0 12m 172.17.125.23 k8s-node01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/nginx-deploy-79fccc485-z425l 1/1 Running 0 12m 172.27.14.229 k8s-node02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; # 接着进行下一项更新，配置CPU和内存资源 [root@k8s-master01 ~]# kubectl set resources deploy nginx-deploy -c nginx --limits=cpu=100m,memory=128Mi --requests=cpu=10m,memory=16Mi deployment.apps/nginx-deploy resource requirements updated # 恢复deployment的更新 [root@k8s-master01 ~]# kubectl rollout resume deployment nginx-deploy deployment.apps/nginx-deploy resumed # 正在更新 [root@k8s-master01 ~]# kubectl get rs NAME DESIRED CURRENT READY AGE nginx-deploy-79fccc485 1 1 1 28m nginx-deploy-975d4fc74 3 3 2 5s nginx-deploy-ff6655784 0 0 0 32m # 更新完成后 [root@k8s-master01 ~]# kubectl get deploy,rs,po -owide NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR deployment.apps/nginx-deploy 3/3 3 3 32m nginx nginx:1.16.1 app=nginx NAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTOR replicaset.apps/nginx-deploy-79fccc485 0 0 0 28m nginx nginx:1.18.0 app=nginx,pod-template-hash=79fccc485 replicaset.apps/nginx-deploy-975d4fc74 3 3 3 25s nginx nginx:1.16.1 app=nginx,pod-template-hash=975d4fc74 replicaset.apps/nginx-deploy-ff6655784 0 0 0 32m nginx nginx:1.16.1 app=nginx,pod-template-hash=ff6655784 NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/nginx-deploy-975d4fc74-6d92q 1/1 Running 0 25s 172.27.14.230 k8s-node02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/nginx-deploy-975d4fc74-7c7f2 1/1 Running 0 22s 172.17.125.24 k8s-node01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/nginx-deploy-975d4fc74-snpkb 1/1 Running 0 20s 172.25.92.71 k8s-master02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; # 可以看到资源配额和镜像版本均已更新 [root@k8s-master01 ~]# kubectl describe deployments.apps nginx-deploy ... Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:1.16.1 Port: 80/TCP Host Port: 0/TCP Limits: cpu: 100m memory: 128Mi Requests: cpu: 10m memory: 16Mi Environment: \u0026lt;none\u0026gt; Mounts: \u0026lt;none\u0026gt; Volumes: \u0026lt;none\u0026gt; ... 其他配置 .spec.revisionHistoryLimit：设置保留RS旧的revision的个数，设置为0的话，不保留历史数据 .spec.minReadySeconds：可选参数，指定新创建的Pod在没有任何容器崩溃的情况下视为Ready最小的秒数，默认为0，即一旦被创建就视为可用。 滚动更新的策略： .spec.strategy.type：更新deployment的方式，默认是RollingUpdate RollingUpdate：滚动更新，可以指定maxSurge和maxUnavailable maxUnavailable：指定在回滚或更新时最大不可用的Pod的数量，可选字段，默认25%，可以设置成数字或百分比，如果该值为0，那么maxSurge就不能0 maxSurge：可以超过期望值的最大Pod数，可选字段，默认为25%，可以设置成数字或百分比，如果该值为0，那么maxUnavailable不能为0 Recreate：重建，先删除旧的Pod，在创建新的Pod # spec中的配置实例 spec: progressDeadlineSeconds: 600 replicas: 2 revisionHistoryLimit: 10 selector: matchLabels: app: nginx strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: ... StatefulSet StatefulSet常用于部署有状态的且需要有序启动的应用程序，比如在生产环境中，可以部署ElasticSearch集群、MongoDB集群或者需要持久化的RabbitMQ集群、Redis集群、Kafka集群和ZooKeeper集群等。\nStatefulSet 中的 Pod 拥有独一无二的身份标识。这个标识基于 StatefulSet 控制器分配给每个 Pod 的唯一顺序索引。Pod 的名称的形式为\u0026lt;statefulset name\u0026gt;-\u0026lt;ordinal index\u0026gt; 。例如: web的StatefulSet 拥有两个副本，所以它创建了两个 Pod: web-0和web-1。\n和 Deployment 相同的是，StatefulSet 管理了基于相同容器定义的一组 Pod；但和 Deployment 不同的是，StatefulSet 为它们的每个 Pod 维护了一个固定的ID标识。这些 Pod 是基于相同的声明来创建的，但是不能相互替换: 无论怎么调度，每个 Pod 都有一个永久不变的ID标识。\nStatefulSet创建的Pod一般使用Headless Service（无头服务）进行通信，和普通的Service的区别在于Headless Service没有ClusterIP，它使用的是Endpoint进行互相通信，Headless一般的格式为：statefulSetName-{0..N-1}.serviceName.namespace.svc.cluster.local。\nserviceName为Headless Service的名字，创建StatefulSet时，必须指定Headless Service名称 0..N-1为Pod所在的序号，从0开始到N-1 statefulSetName为StatefulSet的名字 namespace为服务所在的命名空间 .cluster.local为Cluster Domain（集群域） 使用场景 稳定的、唯一的网络标识符,即Pod重新调度后其PodName和HostName不变(当然IP是会变的) 稳定的、持久的存储,即Pod重新调度后还是能访问到相同的持久化数据,基于PVC实现 有序的、优雅的部署和缩放 有序的、自动的滚动更新 如上面,稳定意味着 Pod 调度或重新调度的整个过程是有持久性的。\n如果应用程序不需要任何稳定的标识符或有序的部署、删除或伸缩，则应该使用由一组无状态的副本控制器提供的工作负载来部署应用程序，比如使用 Deployment 或者 ReplicaSet 可能更适用于无状态应用部署需要。\n限制 给定 Pod 的存储必须由 PersistentVolume(PV) 驱动基于所请求的 storage class 来提供,或者由管理员预先提供 删除或者收缩 StatefulSet 并不会删除它关联的存储卷.这样做是为了保证数据安全 StatefulSet 当前需要 headless 服务来负责 Pod 的网络标识。需要先创建此服务。 删除StatefulSet时，StatefulSet 不提供任何终止 Pod 的保证。为了实现 StatefulSet 中的 Pod 可以有序和优雅的终止，可以在删除之前将 StatefulSet缩放为0。 在默认Pod管理策略(OrderedReady) 时使用滚动更新，可能进入需要人工干预才能修复的损坏状态 部署 对于包含 N 个 副本的 StatefulSet，当部署 Pod 时，它们是依次创建的，顺序为 0~(N-1) 当删除 Pod 时，它们是逆序终止的,顺序为 (N-1)~0 在将缩放操作应用到 Pod 之前，它前面的所有 Pod 必须是 Running 和 Ready 状态 在Pod 终止之前，所有的继任者必须完全关闭 StatefulSet 不应将 pod.Spec.TerminationGracePeriodSeconds 设置为 0。这种做法是不安全的，要强烈阻止。\n部署顺序 在下面的 nginx 示例被创建后，会按照 web-0、web-1、web-2 的顺序部署三个 Pod。在 web-0 进入 Running 和 Ready 状态前不会部署 web-1。在 web-1 进入 Running 和 Ready 状态前不会部署 web-2。\n如果 web-1 已经处于 Running 和 Ready 状态，而 web-2 尚未部署，在此期间发生了 web-0 运行失败，那么 web-2 将不会被部署，要等到 web-0 部署完成并进入 Running 和 Ready 状态后，才会部署 web-2。\n收缩顺序 如果想将示例中的 StatefulSet 收缩为 replicas=1，首先被终止的是 web-2。在 web-2 没有被完全停止和删除前，web-1 不会被终止。当 web-2 已被终止和删除；但web-1 尚未被终止，如果在此期间发生 web-0 运行失败，那么就不会终止 web-1，必须等到 web-0 进入 Running 和 Ready 状态后才会终止 web-1\nSTS实例 [root@k8s-master01 ~]# cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; statefulset.yaml apiVersion: v1 kind: Service metadata: name: nginx labels: app: nginx spec: ports: - port: 80 name: http type: ClusterIP clusterIP: None selector: app: nginx --- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: selector: matchLabels: app: nginx # has to match .spec.template.metadata.labels serviceName: nginx replicas: 3 # by default is 1 template: metadata: labels: app: nginx # has to match .spec.selector.matchLabels spec: terminationGracePeriodSeconds: 10 # 默认30秒 containers: - name: nginx image: nginx:1.18.0 imagePullPolicy: IfNotPresent ports: - containerPort: 80 name: http EOF [root@k8s-master01 ~]# kubectl apply -f statefulset.yaml service/nginx created statefulset.apps/web created [root@k8s-master01 ~]# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 30d nginx ClusterIP None \u0026lt;none\u0026gt; 80/TCP 27s [root@k8s-master01 ~]# kubectl get sts -owide NAME READY AGE CONTAINERS IMAGES web 3/3 63s nginx nginx:1.18.0 [root@k8s-master01 ~]# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES web-0 1/1 Running 0 83s 172.27.14.233 k8s-node02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; web-1 1/1 Running 0 81s 172.17.125.28 k8s-node01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; web-2 1/1 Running 0 79s 172.25.92.73 k8s-master02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; # 创建busybox Pod进去查看三个web-Pod的域名信息 [root@k8s-master01 ~]# cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; busybox.yaml apiVersion: v1 kind: Pod metadata: name: busybox namespace: default spec: containers: - name: busybox image: busybox:1.28 command: - sleep - \u0026#34;3600\u0026#34; imagePullPolicy: IfNotPresent restartPolicy: Always EOF [root@k8s-master01 ~]# kubectl apply -f busybox.yaml pod/busybox created # 进入busybox解析域名 [root@k8s-master01 ~]# kubectl exec busybox -it -- sh / # nslookup web-0.nginx Server: 10.96.0.10 Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local Name: web-0.nginx Address 1: 172.27.14.233 web-0.nginx.default.svc.cluster.local / # nslookup web-1.nginx Server: 10.96.0.10 Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local Name: web-1.nginx Address 1: 172.17.125.28 web-1.nginx.default.svc.cluster.local / # nslookup web-2.nginx Server: 10.96.0.10 Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local Name: web-2.nginx Address 1: 172.25.92.73 web-2.nginx.default.svc.cluster.local / # exit 删除STS 删除STS有两种方式：级联删除和非级联删除。非级联删除时，STS被删除后Pod依旧存活，变成了Orphan Pod（即孤儿Pod），孤儿Pod可以被单独操作。\n# 使用--cascade=orphan参数实现非级联删除 [root@k8s-master01 yamls]# kubectl delete sts web --cascade=orphan [root@k8s-master01 yamls]# kubectl get sts web Error from server (NotFound): statefulsets.apps \u0026#34;web\u0026#34; not found [root@k8s-master01 yamls]# kubectl get pod | grep web web-0 1/1 Running 0 30m web-1 1/1 Running 0 29m web-2 1/1 Running 0 28m # 删除Pod web-1 [root@k8s-master01 yamls]# kubectl delete po web-1 pod \u0026#34;web-1\u0026#34; deleted [root@k8s-master01 yamls]# kubectl get pod | grep web web-0 1/1 Running 0 30m web-2 1/1 Running 0 29m # 重建STS，并把副本数量修改为2 [root@k8s-master01 yamls]# cat statefulset.yaml | grep repli replicas: 2 # by default is 1 [root@k8s-master01 yamls]# kubectl apply -f statefulset.yaml service/nginx unchanged statefulset.apps/web created [root@k8s-master01 yamls]# kubectl get po | grep web web-0 1/1 Running 0 32m web-1 1/1 Running 0 13s # 级联删除 [root@k8s-master01 yamls]# kubectl delete sts web statefulset.apps \u0026#34;web\u0026#34; deleted [root@k8s-master01 yamls]# kubectl get po | grep web # 此时STS和Pod都删掉了 非级联删除时，由于web-1被删了，而且副本数变成了2，所以重建STS（用原来的配置文件）后，会先删除web-2，然后重新创建Pod web-1，web-0已存在，不发生变化。\nDaemonSet DaemonSet守护进程确保全部（或匹配的一部分）节点上部署一个Pod。当有新的节点加入集群时，也会为它们新增一个Pod，当有节点从集群移除时，这些Pod也会被回收。\nDaemonSet典型用法:\n在每个节点上运行集群守护进程，例如 glusterd、ceph 在每个节点上运行日志守护进程，例如 fluentd、logstash 在每个节点上运行监控守护进程，例如 Prometheus Node Exporter、Flowmill、Sysdig 代理、collectd、Dynatrace OneAgent、AppDynamics 代理、Datadog 代理、New Relic 代理、Ganglia gmond 或者 Instana 代理 DaemonSet实例 # 创建yaml [root@k8s-master01 ~]# cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; daemonset.yaml apiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd-elasticsearch namespace: default labels: k8s-app: fluentd-logging spec: selector: matchLabels: name: fluentd-elasticsearch template: metadata: labels: name: fluentd-elasticsearch spec: tolerations: # 容忍在master节点运行 - key: node-role.kubernetes.io/master effect: NoSchedule containers: - name: fluentd-elasticsearch image: registry.cn-beijing.aliyuncs.com/google_registry/fluentd:v2.5.2 resources: limits: cpu: 1 memory: 200Mi requests: cpu: 100m memory: 200Mi volumeMounts: - name: varlog mountPath: /var/log - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true # 优雅关闭应用,时间设置.超过该时间会强制关闭,默认30秒 terminationGracePeriodSeconds: 30 volumes: - name: varlog hostPath: path: /var/log - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers EOF [root@k8s-master01 ~]# kubectl apply -f daemonset.yaml daemonset.apps/fluentd-elasticsearch created [root@k8s-master01 ~]# kubectl get ds -owide NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE CONTAINERS IMAGES SELECTOR fluentd-elasticsearch 5 5 0 5 0 \u0026lt;none\u0026gt; 11s fluentd-elasticsearch registry.cn-beijing.aliyuncs.com/google_registry/fluentd:v2.5.2 name=fluentd-elasticsearch # 可见DaemonSet在每个节点上都有相应的pod [root@k8s-master01 ~]# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES fluentd-elasticsearch-blmjp 1/1 Running 0 2m9s 172.18.195.19 k8s-master03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; fluentd-elasticsearch-t8bpk 1/1 Running 0 2m9s 172.17.125.29 k8s-node01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; fluentd-elasticsearch-v8dwj 1/1 Running 0 2m9s 172.27.14.236 k8s-node02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; fluentd-elasticsearch-w9wpb 1/1 Running 0 2m9s 172.25.92.74 k8s-master02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; fluentd-elasticsearch-xrphv 1/1 Running 0 2m9s 172.25.244.193 k8s-master01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; DaemonSet更新与回滚和Deployment一致\nJob Job 负责仅执行一次的任务，它保证批处理任务的一个或多个 Pod 成功结束。\n[root@k8s-master01 ~]# kubectl explain job KIND: Job VERSION: batch/v1 DESCRIPTION: Job represents the configuration of a single job. FIELDS: ... # 创建yaml [root@k8s-master01 ~]# cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; job-example.yaml apiVersion: batch/v1 kind: Job metadata: name: pi spec: template: metadata: name: pi spec: containers: - name: pi image: perl command: [\u0026#34;perl\u0026#34;, \u0026#34;-Mbignum=bpi\u0026#34;, \u0026#34;-wle\u0026#34;, \u0026#34;print bpi(1000)\u0026#34;] restartPolicy: Never EOF [root@k8s-master01 ~]# kubectl get po NAME READY STATUS RESTARTS AGE daemonset-example-f57kg 1/1 Running 0 13m pi-2nlj5 0/1 Completed 0 4m [root@k8s-master01 ~]# kubectl get job NAME COMPLETIONS DURATION AGE pi 1/1 3m19s 4m15s [root@k8s-master01 ~]# kubectl get po pi-2nlj5 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pi-2nlj5 0/1 Completed 0 4m54s 172.16.36.119 k8s-node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; # 查看日志可以看到job执行的结果 # 计算出了圆周率后1000位 CronJob Cron Job 管理基于时间的 Job，即:\n在给定时间点只运行一次 周期性地在设定时间点运行 典型的用法示例:\n在给你写的时间点调度 Job 运行 创建周期性运行的 Job，例如: 数据库备份、发送邮件 [root@k8s-master01 ~]# kubectl explain cj KIND: CronJob VERSION: batch/v1 DESCRIPTION: CronJob represents the configuration of a single cron job. FIELDS: ... # 创建cronjob yaml文件 [root@k8s-master01 ~]# cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; cronjob-example.yaml apiVersion: batch/v1 kind: CronJob metadata: name: hello spec: schedule: \u0026#34;*/1 * * * *\u0026#34; # 分 时 日 月 星期 successfulJobsHistoryLimit: 3 # 保留成功的记录数 failedJobHistoryLimit: 1 # 保留失败的记录数 concurrencyPolicy: Allow # 并发调度策略 suspend: false # 是否暂停后续任务，默认false jobTemplate: spec: template: metadata: labels: run: hello spec: containers: - name: hello image: busybox args: - /bin/sh - -c - date; echo Hello CronJob imagePolicy: Always restartPolicy: OnFailure EOF [root@k8s-master01 ~]# kubectl apply -f cronjob-example.yaml cronjob.batch/hello created [root@k8s-master01 ~]# kubectl get cj NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE hello */1 * * * * False 0 44s 69s [root@k8s-master01 ~]# kubectl get po NAME READY STATUS RESTARTS AGE hello-27366655-8mm5c 0/1 Completed 0 84s hello-27366656-pjbv6 0/1 Completed 0 24s # 查看输出日志 [root@k8s-master01 ~]# kubectl logs hello-27366655-8mm5c Wed Jan 12 14:55:17 UTC 2022 Hello CronJob [root@k8s-master01 ~]# kubectl logs hello-27366656-pjbv6 Wed Jan 12 14:56:15 UTC 2022 Hello CronJob [root@k8s-master01 ~]# kubectl get job NAME COMPLETIONS DURATION AGE hello-27366655 1/1 17s 2m29s hello-27366656 1/1 16s 89s hello-27366657 0/1 29s 29s # 删除cronjob [root@k8s-master01 ~]# kubectl delete cronjob hello cronjob.batch \u0026#34;hello\u0026#34; deleted [root@k8s-master01 ~]# kubectl get job No resources found in default namespace. 并发调度策略concurrencyPolicy:\nAllow：允许多个任务同时进行 Forbid：不允许并发，当前任务未完成，新任务不会创建 Replace：如果之前的任务尚未完成，则创建新的代替之 HPA 顾名思义，Horizontal Pod Autoscaling（Pod 水平自动缩放），可以基于 CPU 利用率自动伸缩 ReplicationController、Deployment、ReplicaSet 和 StatefulSet 中的 Pod 数量。除了 CPU 利用率，也可以基于其他应程序提供的自定义度量指标来执行自动扩缩。 Pod 自动扩缩不适用于无法扩缩的对象，比如 DaemonSet。\nPod 水平自动扩缩特性由 Kubernetes API 资源和控制器实现。资源决定了控制器的行为。控制器会周期性地调整副本控制器或 Deployment 中的副本数量，以使得类似 Pod 平均 CPU 利用率、平均内存利用率这类观测到的度量值与用户所设定的目标值匹配。\n下面是HPA工作机制模型图:\nHPA两个版本区别:\nautoscaling/v1：稳定版本，只支持 cpu autoscaling/v2beta：测试阶段 v2beta1(支持CPU、内存和自定义指标) v2beta2(支持CPU、内存、自定义指标Custom和额外指标ExternalMetrics) Kubernetes目前（v1.23.0后）autoscaling/v2版本已进入稳定版，支持CPU、内存、自定义指标Custom和额外指标ExternalMetrics\n指标 Pod 水平自动伸缩器的实现是一个控制回路，由控制器管理器的--horizontal-pod-autoscaler-sync-period参数指定周期（默认值为 15 秒）。\n每个周期内，控制器管理器根据每个HorizontalPodAutoscaler定义中指定的指标查询资源利用率。控制器管理器可以从资源度量指标 API（按 Pod 统计的资源用量）和自定义度量指标 API（其他指标）获取度量值。\n对于按 Pod 统计的资源指标（如 CPU），控制器从资源指标 API 中获取每一个HorizontalPodAutoscaler指定的 Pod 的度量值，如果设置了目标使用率，控制器获取每个 Pod 中的容器资源使用情况，并计算资源使用率。 如果设置了 target 值，将直接使用原始数据（不再计算百分比）。 接下来，控制器根据平均的资源使用率或原始值计算出扩缩的比例，进而计算出目标副本数。 如果 Pod 使用自定义指标，控制器机制与资源指标类似，区别在于自定义指标只使用原始值，而不是使用率。 如果 Pod 使用对象指标和外部指标（每个指标描述一个对象信息）。这个指标将直接根据目标设定值相比较，并生成一个上面提到的扩缩比例。在autoscaling/v2beta2版本 API 中，这个指标也可以根据 Pod 数量平分后再计算。 通常情况下，控制器将从一系列的聚合API（metrics.k8s.io、custom.metrics.k8s.io 和 external.metrics.k8s.io）中获取度量值。metrics.k8s.io API 通常由 Metrics 服务器（需要提前启动）提供。\nHPA使用 1.使用kubectl\n# 自动伸缩deploy/php-apache，目标CPU使用率50%，deploy副本数在1~10之间 kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10 2.使用yaml创建\n# 作用同上 apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: php-apache namespace: default spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: php-apache minReplicas: 1 maxReplicas: 10 targetCPUUtilizationPercentage: 50 示例 下载国内示例镜像\n# 集群所有可调度Pod的节点先获取要使用的hpa示例镜像 docker pull registry.cn-beijing.aliyuncs.com/google_registry/hpa-example # yaml [root@k8s-master01 ~]# cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; php-apache.yaml apiVersion: apps/v1 kind: Deployment metadata: name: php-apache spec: selector: matchLabels: hpa: php-apache replicas: 1 template: metadata: labels: hpa: php-apache spec: containers: - name: php-apache image: registry.cn-beijing.aliyuncs.com/google_registry/hpa-example imagePullPolicy: IfNotPresent ports: - containerPort: 80 resources: limits: cpu: 500m requests: cpu: 200m --- apiVersion: v1 kind: Service metadata: name: php-apache labels: hpa: php-apache spec: ports: - port: 80 selector: hpa: php-apache EOF [root@k8s-master01 ~]# kubectl apply -f php-apache.yaml deployment.apps/php-apache created service/php-apache created [root@k8s-master01 ~]# kubectl get deployments.apps NAME READY UP-TO-DATE AVAILABLE AGE php-apache 1/1 1 1 9s # 创建HPA # 当pod中CPU使用率达50%就扩容.最小1个,最大10个 [root@k8s-master01 ~]# kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10 horizontalpodautoscaler.autoscaling/php-apache autoscaled [root@k8s-master01 ~]# kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE php-apache Deployment/php-apache 0%/50% 1 10 1 18s 压测php-apache # 创建压测Pod并进入, 先不要退出 [root@k8s-master01 ~]# kubectl run -it load-test --image=busybox /bin/sh / # # 另起一窗口可查看到pod在运行 [root@k8s-master01 ~]# kubectl get po | grep load load-test 1/1 Running 0 26s # 在load-test继续中操作，连续访问php-apache的Service / # while true; do wget -q -O- http://php-apache.default.svc.cluster.local; done OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK! # 一分钟左右，在另外一个窗口可看到，负载飙升, 副本数增多 [root@k8s-master01 ~]# kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE php-apache Deployment/php-apache 254%/50% 1 10 3 21m # 自动扩容 [root@k8s-master01 ~]# kubectl get deploy,rs,po NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/php-apache 6/6 6 6 23m NAME DESIRED CURRENT READY AGE replicaset.apps/php-apache-7db9c95858 6 6 6 23m NAME READY STATUS RESTARTS AGE pod/php-apache-7db9c95858-6wrp2 1/1 Running 0 32s pod/php-apache-7db9c95858-7dbjv 1/1 Running 0 10m pod/php-apache-7db9c95858-cfwrk 1/1 Running 0 32s pod/php-apache-7db9c95858-ksbgh 1/1 Running 0 47s pod/php-apache-7db9c95858-m9mtn 1/1 Running 0 47s pod/php-apache-7db9c95858-znczt 1/1 Running 0 32s # 停止压测 # Ctrl+C停止在跑的循环 / # while true; do wget -q -O- http://php-apache.default.svc.cluster.local; done OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!.\u0026lt;省略很多输出\u0026gt;..!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!^C # 停止压测后负载降下来 [root@k8s-master01 ~]# kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE php-apache Deployment/php-apache 0%/50% 1 10 6 24m # 再过几分钟副本数也降了 [root@k8s-master01 ~]# kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE php-apache Deployment/php-apache 0%/50% 1 10 1 29m [root@k8s-master01 ~]# kubectl get deploy,rs,po NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/php-apache 1/1 1 1 31m NAME DESIRED CURRENT READY AGE replicaset.apps/php-apache-7db9c95858 1 1 1 31m NAME READY STATUS RESTARTS AGE pod/php-apache-7db9c95858-7dbjv 1/1 Running 0 18m 说明:停止压测，pod数量也不会立即降下来.而是过段时间后才会慢慢降下来.这是为了防止由于网络原因或者间歇性流量突增、突降，导致pod回收太快后面流量上来后Pod数量不够\n","permalink":"https://deemoprobe.github.io/posts/tech/kubernetes/kubernetespodcontroller/","summary":"Kubernetes 中内建了很多 Controller(控制器)，用来确保pod资源符合预期的状态，控制 Pod 的状态和行为。 控制器类型 ReplicationController(rc) ReplicaSet(rs) Deployment(deploy) DaemonSet(ds) StatefulSet(sts) Job/CronJob(cj) Horizontal Pod Autoscaling(hpa) # 可以使用kubectl explain命令查看k8s API资源对象描述信息 kubectl explain \u0026lt;apiresource-name\u0026gt; | egrep -v \u0026#34;^$|^\\s*http\u0026#34; | sed s/\u0026#34;More info:\u0026#34;// # 查看API资源列表 kubectl api-resources RS和RC Replication","title":"KubernetesPodController"},{"content":"概念 Pod是Kubernetes中创建管理和可部署的最小计算单元，它由一个或多个容器组成，每个Pod包含了一个Pause容器，Pause容器是Pod的父容器，负责僵尸进程的回收管理，通过Pause容器可以使同一个Pod里面的多个容器共享存储、网络、PID、IPC等。\n定义Pod Pod 中的容器会被调度到集群中的同一计算节点上。通常情况下，很少去直接创建Pod单实例，一般是通过Pod控制器（Deployment、DaemonSet、StatefulSet、CronJob等）进行创建和管理，并且这些高级资源具有Pod所不具备的很多特性（如无状态服务控制器deployment的伸缩回滚，有状态服务控制器StatefulSet的实例绑定）。控制器创建Pod后Scheduler根据预设条件和调度算法将Pod调度到集群合适的节点上，Pod会保持在该节点上运行，直到Pod结束执行、Pod对象被删除、Pod因资源不足而被驱逐或者节点失效为止。\n说明：重启 Pod 中的容器不应与重启 Pod 混淆。 Pod 不是进程，而是容器运行的环境。在被删除之前，Pod 会一直存在。\nPod yaml文件示例\n[root@k8s-master01 ~]# kubectl explain pod | egrep -v \u0026#34;^$|^\\s*http\u0026#34;|sed s/\u0026#34;More info:\u0026#34;// KIND: Pod VERSION: v1 DESCRIPTION: Pod is a collection of containers that can run on a host. This resource is created by clients and scheduled onto hosts. FIELDS: apiVersion \u0026lt;string\u0026gt; APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. kind \u0026lt;string\u0026gt; Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. metadata \u0026lt;Object\u0026gt; Standard object\u0026#39;s metadata. spec \u0026lt;Object\u0026gt; Specification of the desired behavior of the pod. status \u0026lt;Object\u0026gt; Most recently observed status of the pod. This data may not be up to date. Populated by the system. Read-only. # yaml文件示例 apiVersion: v1 # 必选，API的版本号 kind: Pod # 必选，类型Pod metadata: # 必选，元数据 name: nginx # 必选，符合RFC1035规范的Pod名称 namespace: default # 可选，Pod所在的命名空间，默认为default labels: # 可选，标签选择器，一般用于过滤和区分Pod app: nginx role: frontend # 可以写多个 annotations: # 可选，注释键值对列表，可以写多个 app: nginx spec: # 必选，用于定义容器的详细信息 initContainers: # 初始化容器，在容器启动之前执行的一些初始化操作 - command: - sh - -c - echo \u0026#34;I am InitContainer for init some configuration\u0026#34; image: busybox imagePullPolicy: IfNotPresent name: init-container containers: # 必选，容器列表 - name: nginx # 必选，符合RFC 1035规范的容器名称 image: nginx:latest # 必选，容器所用的镜像的地址 imagePullPolicy: Always # 可选，镜像拉取策略 command: # 可选，容器启动执行的命令 - nginx - -g - \u0026#34;daemon off;\u0026#34; workingDir: /usr/share/nginx/html # 可选，容器的工作目录 volumeMounts: # 可选，存储卷配置，可以配置多个 - name: webroot # 存储卷名称 mountPath: /usr/share/nginx/html # 挂载目录 readOnly: true # 只读 ports: # 可选，容器需要暴露的端口号列表 - name: http # 端口名称 containerPort: 80 # 端口号 protocol: TCP # 端口协议，默认TCP env: # 可选，环境变量配置列表 - name: TZ # 变量名 value: Asia/Shanghai # 变量的值 - name: LANG value: en_US.utf8 resources: # 可选，资源限制和资源请求限制 limits: # 最大限制设置 cpu: 1000m memory: 1024Mi requests: # 启动所需的资源 cpu: 100m memory: 512Mi # startupProbe: # 可选，检测容器内进程是否完成启动。注意三种检查方式同时只能使用一种。 # httpGet: # httpGet检测方式，生产环境建议使用httpGet实现接口级健康检查，健康检查由应用程序提供。 # path: /api/successStart # 检查路径 # port: 80 readinessProbe: # 可选，健康检查。注意三种检查方式同时只能使用一种。 httpGet: # httpGet检测方式，生产环境建议使用httpGet实现接口级健康检查，健康检查由应用程序提供。 path: / # 检查路径 port: 80 # 监控端口 #livenessProbe: # 可选，健康检查 #exec: # 执行容器命令检测方式 #command: #- cat #- /health #httpGet: # httpGet检测方式 # path: /_health # 检查路径 # port: 8080 # httpHeaders: # 检查的请求头 # - name: end-user # value: Jason tcpSocket: # 端口检测方式 port: 80 initialDelaySeconds: 60 # 初始化时间 timeoutSeconds: 2 # 超时时间 periodSeconds: 20 # 检测间隔 successThreshold: 2 # 检查成功为2次表示就绪 failureThreshold: 1 # 检测失败1次表示未就绪 lifecycle: postStart: # 容器创建完成后执行的指令, 可以是exec httpGet TCPSocket exec: command: - sh - -c - \u0026#39;mkdir /data/\u0026#39; preStop: httpGet: path: / port: 80 # exec: # command: # - sh # - -c # - sleep 9 restartPolicy: Always # 可选，默认为Always #nodeSelector: # 可选，指定Node节点 # region: subnet7 imagePullSecrets: # 可选，拉取镜像使用的secret，可以配置多个 - name: default-dockercfg-86258 hostNetwork: false # 可选，是否为主机模式，如是，会占用主机端口 volumes: # 共享存储卷列表 - name: webroot # 名称，与上面volumeMounts对应 emptyDir: {} # 挂载目录 #hostPath: # 挂载本机目录 # path: /etc/hosts 在高级资源中创建Pod是通过文件中template Pod模板字段来创建Pod\n# 例如创建Job， apiVersion: batch/v1 kind: Job metadata: name: hello spec: template: # 这里是Pod对象模版 spec: containers: - name: hello image: busybox command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo \u0026#34;Hello, Kubernetes!\u0026#34; \u0026amp;\u0026amp; sleep 3600\u0026#39;] restartPolicy: OnFailure ... 零宕机部署 Pod的零宕机上下线，可以通过加入初始化容器（InitContainers）、Pod上线接收流量前的**健康检查（探针StartupProbe/LivenessProbe/ReadinessProbe）和Pod生命周期回调(postStart/preStop)**来实现。借此可以实现deployment零宕机滚动更新。\n初始化容器 初始化容器即InitContainer，主要作用是在主应用容器启动之前，做一些初始化的操作，比如创建文件、修改内核参数、等待依赖程序启动或其他需要在主程序启动之前需要做的工作。常见用途有：\n安装应用容器中不存在的实用工具或个性化代码 安全地运行这些工具，避免这些工具导致应用镜像的安全性降低 root身份执行一些高权限命令 初始化容器相关操作执行完成后退出，不会给业务容器带来安全隐患\nInitContainer与postStart的区别：\nInitContainer：不依赖主容器，可以有更高的权限，一定会在主容器启动之前完成 postStart：依赖主容器，这个回调在主容器被创建之后立即执行。但是不能保证会在容器启动前执行，即入口点（ENTRYPOINT）之前执行。 InitContainer与普通容器的区别：\n普通容器考虑到安全性，通常权限不会给很大，InitContainer可以执行高权限的操作，操作完成立即退出 总是运行到完成，不完成不会启动主容器 多个初始化容器，上一个运行完成才会运行下一个 如果Pod的InitContainer失败，Kubernetes会不断地重启该Pod，直到成功为止，除非Pod的restartPolicy值为Never 不支持 lifecycle、livenessProbe、readinessProbe 和 startupProbe # 实例 [root@k8s-master01 yamls]# vim initcontainer.yml apiVersion: apps/v1 kind: Deployment metadata: labels: app: test-init name: test-init namespace: kube-public spec: replicas: 1 selector: matchLabels: app: test-init template: # pod配置 metadata: labels: app: test-init spec: volumes: - name: data emptyDir: {} initContainers: # 在应用容器启动前使用root权限创建相关文件 - command: - sh - -c - touch /mnt/test-init.txt image: nginx imagePullPolicy: IfNotPresent name: init-touch volumeMounts: - name: data mountPath: /mnt containers: - image: nginx imagePullPolicy: IfNotPresent name: test-init volumeMounts: - name: data mountPath: /mnt [root@k8s-master01 yamls]# kubectl apply -f initcontainer.yml [root@k8s-master01 yamls]# kubectl get deploy,po -n kube-public NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/test-init 1/1 1 1 5m5s NAME READY STATUS RESTARTS AGE pod/test-init-6d4c45d5b7-4qnl8 1/1 Running 0 5m5s [root@k8s-master01 yamls]# kubectl exec -it -n kube-public test-init-6d4c45d5b7-4qnl8 -- ls /mnt Defaulted container \u0026#34;test-init\u0026#34; out of: test-init, init-touch (init) test-init.txt Pod探针 生产环境中，应用容器能正常启动并不代表能正常处理请求，所以要合理配置健康检查。Pod健康检查有三种探针：\nstartupProbe（1.16版本后）：用于判断容器内应用程序是否已经启动。如果配置了startupProbe，会先禁止其他的探测，直到容器成功内程序启动为止。如果探测失败，kubelet会杀死容器并根据重启策略处理；如果探测成功，或未配置startupProbe探针，则标记为success livenessProbe：用于探测容器是否在运行（存活），如果探测失败，kubelet会杀死容器根据重启策略处理。若没有配置该探针，则标记为success readinessProbe：一般用于探测容器内的程序是否准备就绪（接收处理请求），如果探测失败，Endpoint Controller将从Service的Endpoints中删除该Pod的IP地址。若未配置该探针则默认标记为success Pod探针的实现方式 ExecAction：在容器内执行一个命令，如果返回值为0，则认为容器健康 TCPSocketAction：通过TCP连接检查容器内的端口是否是通的，如果是通的就认为容器健康 HTTPGetAction：通过应用程序暴露的API地址来检查程序是否是正常的，如果状态码为200~400之间，则认为容器健康 Pod健康检查结果：\nSuccess：检查通过 Failed：检查失败，kubelet根据Pod重启策略进一步处理 Unknown：检查失败，状态未知，不进行任何处理 Pod镜像拉取策略（imagePullPolicy）：\nAlways：总是拉取直至成功，镜像tag未指定时（即latest），默认为Always Never：无论镜像是否存在都不拉取 IfNotPresent：镜像不存在时拉取 Pod重启策略（restartPolicy）：\nAlways：默认策略，容器失效时，自动重启 OnFailure：容器以不为0的状态码终止，自动重启 Never：无论何种状态，都不重启 下面案例coredns的健康检查配置中livenessProbe通过httpGet的方式检测提供8080/health接口，检查容器是否处于运行状态，检查通过后接着进行ReadinessProbe探针检测，httpGet方式检测提供的8181/ready接口，检测通过则表示容器内程序运行正常，可以接收流量。健康检查接口需要在镜像中预定程序源码中写好，为后续接入健康检查做准备。\n# 以coredns为例，可以看到健康检查配置了LivenessProbe和ReadinessProbe [root@k8s-master01 ~]# kubectl get deploy -n kube-system coredns -oyaml ... spec: containers: ... livenessProbe: failureThreshold: 5 httpGet: path: /health port: 8080 scheme: HTTP initialDelaySeconds: 60 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 5 ... readinessProbe: failureThreshold: 3 httpGet: path: /ready port: 8181 scheme: HTTP periodSeconds: 10 successThreshold: 1 timeoutSeconds: 11 ... 特殊情况：如果遇到容器内程序启动时间太长，超过了initialDelaySeconds设定的容器初始化时间，那么健康检查也不会通过，容器会不断被杀掉重启，造成服务无法正常运行。像上面这些情况就需要检测容器内程序是否健康，此时可以使用startupProbe。\n探针检查参数配置 initialDelaySeconds: 10 # 容器启动且存活，等待10s后开始检测。若不配置，默认为0s timeoutSeconds: 2 # 检测超时后的等待时间，默认为1s periodSeconds: 20 # 检测间隔时间，默认为10s successThreshold: 1 # 检测连续成功1次后标记Pod就绪 failureThreshold: 2 # 检测连续失败2次后标记Pod失败 startupProbe实例 采用HTTPGetAction探测方式\n实例使用的nginx:latest镜像并没有预设下面所写的健康检查接口，所以检测是不会通过的，可以通过Events查看。重启策略如果不指定，默认是restartPolicy: Always，失败后会一直重启\n[root@k8s-master01 ~]# cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; startupprobe-httpget.yaml apiVersion: v1 kind: Pod metadata: labels: test: startupprobe-httpget name: startupprobe-httpget spec: containers: - name: nginx image: nginx ports: - containerPort: 8080 startupProbe: httpGet: path: /health port: 8080 initialDelaySeconds: 15 timeoutSeconds: 1 EOF [root@k8s-master01 ~]# kubectl apply -f startupprobe-httpget.yaml pod/startupprobe-httpget created [root@k8s-master01 ~]# kubectl describe po startupprobe-httpget ... Startup: http-get http://:8080/health delay=15s timeout=1s period=10s #success=1 #failure=3 ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 56s default-Scheduler Successfully assigned default/startupprobe-httpget to k8s-node02 Normal Pulling 56s kubelet Pulling image \u0026#34;nginx\u0026#34; Normal Pulled 40s kubelet Successfully pulled image \u0026#34;nginx\u0026#34; in 15.591936231s Normal Created 40s kubelet Created container nginx Normal Started 40s kubelet Started container nginx Warning Unhealthy 7s (x2 over 17s) kubelet Startup probe failed: Get \u0026#34;http://172.27.14.202:8080/health\u0026#34;: dial tcp 172.27.14.202:8080: connect: connection refused # 查看Pod状态，健康检查不通过，处于Running但并没有Ready，并且还在不断重启 [root@k8s-master01 ~]# kubectl get po NAME READY STATUS RESTARTS AGE startupprobe-httpget 0/1 Running 5 (20s ago) 4m40s livenessProbe实例 采用ExecAction探测方式\n# 创建liveness.yaml [root@k8s-master01 ~]# cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; livenessprobe-exec.yaml apiVersion: v1 kind: Pod metadata: labels: test: livenessprobe-exec name: livenessprobe-exec spec: containers: - name: liveness image: busybox args: - /bin/sh - -c - echo ok \u0026gt; /tmp/healthy; sleep 10; rm -rf /tmp/healthy; sleep 60 livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 15 periodSeconds: 5 EOF [root@k8s-master01 ~]# kubectl apply -f livenessprobe-exec.yaml pod/livenessprobe-exec created [root@k8s-master01 ~]# kubectl describe po livenessprobe-exec ... Liveness: exec [cat /tmp/healthy] delay=15s timeout=1s period=5s #success=1 #failure=3 ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 34s default-Scheduler Successfully assigned default/livenessprobe-exec to k8s-node02 Normal Pulling 33s kubelet Pulling image \u0026#34;busybox\u0026#34; Normal Pulled 20s kubelet Successfully pulled image \u0026#34;busybox\u0026#34; in 13.224110856s Normal Created 20s kubelet Created container liveness Normal Started 20s kubelet Started container liveness Warning Unhealthy 4s kubelet Liveness probe failed: cat: can not open \u0026#39;/tmp/healthy\u0026#39;: No such file or directory # 观察Pod状态可以发现一开始是正常的，后面因为健康检查失败重启了 [root@k8s-master01 ~]# kubectl get po NAME READY STATUS RESTARTS AGE livenessprobe-exec 1/1 Running 0 75s [root@k8s-master01 ~]# kubectl get po NAME READY STATUS RESTARTS AGE livenessprobe-exec 1/1 Running 1 (21s ago) 91s 在这个配置文件中，Pod 中只有一个容器。periodSeconds字段指定了 kubelet 应该每 5 秒执行一次健康检查。initialDelaySeconds字段告诉 kubelet 在执行第一次探测前应该等待 15 秒。 kubelet 在容器内执行命令cat /tmp/healthy来进行探测。 如果命令执行成功并且返回值为 0，kubelet 就会认为这个容器是健康存活的。 如果这个命令返回非 0 值，kubelet 会杀死这个容器并重新启动它。\nreadinessProbe实例 采用TCPSocketAction探测方式\nnginx启动后默认开启80端口，下面健康检查会通过\n[root@k8s-master01 ~]# vim readinessprobe-tcpsocket.yaml apiVersion: v1 kind: Pod metadata: labels: test: readinessprobe-tcpsocket name: readinessprobe-tcpsocket spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - containerPort: 80 readinessProbe: tcpSocket: port: 80 initialDelaySeconds: 5 periodSeconds: 10 [root@k8s-master01 ~]# kubectl apply -f readinessprobe-tcpsocket.yaml pod/readinessprobe-tcpsocket created [root@k8s-master01 ~]# kubectl get po readinessprobe-tcpsocket NAME READY STATUS RESTARTS AGE readinessprobe-tcpsocket 1/1 Running 0 20s [root@k8s-master01 ~]# kubectl describe po readinessprobe-tcpsocket ... Readiness: tcp-socket :80 delay=5s timeout=1s period=10s #success=1 #failure=3 ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 112s default-Scheduler Successfully assigned default/readinessprobe-tcpsocket to k8s-node02 Normal Pulling 112s kubelet Pulling image \u0026#34;nginx\u0026#34; Normal Pulled 102s kubelet Successfully pulled image \u0026#34;nginx\u0026#34; in 9.699109421s Normal Created 102s kubelet Created container nginx Normal Started 102s kubelet Started container nginx Successfully pulled image \u0026quot;nginx\u0026quot; in 9.699109421s拉取镜像花时间约9.7s，健康检查间隔时间是10s，所以大约在19.7sPod标记为Running状态\nPod生命周期 以下Pod创建和删除流程建立在容器运行时为containerd基础上进行\nPod创建流程 用户通过kubectl、RESTful API或其他客户端向APIServer发起创建pod请求 APIServer将pod对象写入etcd持久化存储，写入成功后，APIServer将收到etcd的确认信息 APIServer标记新的pod被创建 Scheduler监听到APIServer处有新的pod被创建。首先检查pod是否已经调度（检查spec.nodeName字段），如果未被调度，那么Scheduler会为pod分配一个合适的调度节点，并将状态写入spec.nodeName字段中，完成pod的绑定 Scheduler反馈pod的绑定信息到APIServer后写入etcd，同时Scheduler也持续监听pod对象的变化，若spec.nodeName字段不为空，则不进行任何处理 kubelet监听APIServer处pod的变化，当发现pod被调度到自己所在的节点时（即spec.nodeName字段值为自己所在节点），kubelet调用CRI gRPC申请启动容器 kubelet首先调用RunPodSandbox接口。containerd确认PodSandbox的pause镜像是否存在。由于所有PodSandbox使用同一个pause镜像，如果节点中已有其他在运行的pod，则pause就已经存在。接着创建Network Namespace，调用CNI接口设置容器网络，containerd使用这个网络启动PodSandbox kubelet申请在PodSandbox下创建容器，如果镜像不存在，则调用CRI的PullImage接口通过containerd拉取镜像 kubelet调用CRI的CreateContainer接口通过containerd创建容器 kubelet调用StartContainer接口通过containerd启动容器 无论容器是否启动成功，kubelet都会将最新的容器状态更新到pod的status字段中，其他控制器组件通过该字段获取pod的状态 APIServer将pod状态写入etcd，kubelet将持续监听APIServer Pod删除流程 用户通过kubectl、RESTful API或其他客户端向APIServer发起删除pod请求 pod对象不会被APIServer立即删除，APIServer在pod中添加deletionTimestamp和deletionGracePeriodSeconds（默认30s）字段，并写入etcd中 APIServer将pod已删除的信息返回给用户，用户查看pod状态已被更新为Terminating kubelet监听到APIServer处的自身的pod对象deletionTimestamp字段被设置时，开始准备删除pod kubelet调用StopContainer接口通过containerd停止容器，containerd首先会调用runc向容器发送SIGTERM信号，容器停止或deletionGracePeriodSeconds超时后，runc发送SIGKILL信号杀死所有容器进程，完成容器停止 kubelet收到containerd的容器已停止信息后，将状态更新到pod的status字段中 APIServer更新pod状态并写入etcd endpoint监听到pod状态发生改变后，删除pod相关的endpoint对象 APIServer更新endpoint状态并写入etcd kube-proxy监听到endpoint变化后，移除pod相关的转发规则 pod内所有容器被停止后，containerd调用CNI将容器的网络删除，然后通过StopPodSandbox接口停止PodSandbox，停止后，kubelet进行一系列的清理工作，例如清理pod的CGroup 如果pod有finalizer，即使PodSandbox被停止，这个pod也不会消失，需要等待其他控制器完成finalizer相关的清理工作，finalizer清空后，canBeDeleted方法返回true，kubelet发起最终的deletionGracePeriodSeconds=0的删除请求 APIServer将pod从etcd中彻底删除，kubelet实时监听 Pod优雅退出 Pod正常情况下退出（删除）流程有两个时间线：网络规则清理和Pod本身的删除。这两个时间线是并行的，大致如下：\n网络规则清理流程：\nAPIServer会收到Pod删除的请求，在Etcd中更新Pod的状态为Terminating Endpoint Controller将该Pod的ip从Endpoint对象中删除 Kube-proxy根据Endpoint对象的改变更新网络规则（iptables或ipvs），不再将流量路由到被删除的Pod Pod删除流程：\nAPIServer 会收到Pod删除的请求，在Etcd中更新Pod的状态为Terminating； kubelet在节点上清理容器的相关资源，例如存储，网络 kubelet发送SIGTERM进程给容器，如果容器中的进程未做任何配置，则容器立即退出 如果容器未在默认的30秒时间内退出，Kubelet发送SIGKILL给容器，强制让容器退出 实际环境可能会遇到的情况：\npod的请求未处理完就被退出 pod已经退出还有流量流向pod（即网络规则未清理完成Pod已经被删除了） 上面这两种情况需要设置pod优雅退出参数来优化退出流程。可以增加preStop生命周期参数或修改terminationGracePeriodSeconds参数。preStop用来完成pod退出前要执行的操作，\n如果 preStop 回调所需要的时间长于默认30s，则必须修改 terminationGracePeriodSeconds 属性值来使其正常工作。\n# 实例 apiVersion: v1 kind: Pod metadata: name: lifecycle-demo spec: terminationGracePeriodSeconds: 45 # set terminationGracePeriodSeconds containers: - name: lifecycle-demo-container image: nginx lifecycle: postStart: exec: command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;echo Hello from the postStart \u0026gt; /usr/share/message\u0026#34;] preStop: exec: command: [\u0026#34;/bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;nginx -s quit; while killall -0 nginx; do sleep 1; done\u0026#34;] ","permalink":"https://deemoprobe.github.io/posts/tech/kubernetes/kubernetespod/","summary":"概念 Pod是Kubernetes中创建管理和可部署的最小计算单元，它由一个或多个容器组成，每个Pod包含了一个Pause容器，Pause容器是Pod的父容器，负责僵尸进程的回收管理，通过Pause容器可以使同一个Pod里面的多个容器共享存储、网络、PID、IPC等。 定义Pod Pod 中","title":"KubernetesPod"},{"content":"简介 Kubernetes支持多个虚拟集群，这些虚拟集群依赖于同一个物理集群，这些虚拟集群就可以称作namespace（中文习惯称为\u0026quot;名称空间\u0026quot;或\u0026quot;命名空间\u0026quot;, 个人觉得理解成\u0026quot;作用域\u0026quot;更为合理一些）\nNamespace有资源隔离的作用，类似Linux系统的多用户的概念（同一物理环境,可以为多用户划分多个相互隔离的操作空间）\nNamespace在多用户之间通过资源配额（resource-quotas）进行集群资源的划分\n使用Namespace # 位于namespace作用域中的资源 [root@k8s-master01 ~]# kubectl api-resources --namespaced NAME SHORTNAMES APIVERSION NAMESPACED KIND bindings v1 true Binding configmaps cm v1 true ConfigMap endpoints ep v1 true Endpoints events ev v1 true Event limitranges limits v1 true LimitRange persistentvolumeclaims pvc v1 true PersistentVolumeClaim pods po v1 true Pod podtemplates v1 true PodTemplate replicationcontrollers rc v1 true ReplicationController resourcequotas quota v1 true ResourceQuota secrets v1 true Secret serviceaccounts sa v1 true ServiceAccount services svc v1 true Service controllerrevisions apps/v1 true ControllerRevision daemonsets ds apps/v1 true DaemonSet deployments deploy apps/v1 true Deployment replicasets rs apps/v1 true ReplicaSet statefulsets sts apps/v1 true StatefulSet localsubjectaccessreviews authorization.k8s.io/v1 true LocalSubjectAccessReview horizontalpodautoscalers hpa autoscaling/v2 true HorizontalPodAutoscaler cronjobs cj batch/v1 true CronJob jobs batch/v1 true Job leases coordination.k8s.io/v1 true Lease networkpolicies crd.projectcalico.org/v1 true NetworkPolicy networksets crd.projectcalico.org/v1 true NetworkSet endpointslices discovery.k8s.io/v1 true EndpointSlice events ev events.k8s.io/v1 true Event pods metrics.k8s.io/v1beta1 true PodMetrics ingresses ing networking.k8s.io/v1 true Ingress networkpolicies netpol networking.k8s.io/v1 true NetworkPolicy poddisruptionbudgets pdb policy/v1 true PodDisruptionBudget rolebindings rbac.authorization.k8s.io/v1 true RoleBinding roles rbac.authorization.k8s.io/v1 true Role csistoragecapacities storage.k8s.io/v1 true CSIStorageCapacity # 不被namespace限制的资源 [root@k8s-master01 ~]# kubectl api-resources --namespaced=false NAME SHORTNAMES APIVERSION NAMESPACED KIND componentstatuses cs v1 false ComponentStatus namespaces ns v1 false Namespace nodes no v1 false Node persistentvolumes pv v1 false PersistentVolume mutatingwebhookconfigurations admissionregistration.k8s.io/v1 false MutatingWebhookConfiguration validatingwebhookconfigurations admissionregistration.k8s.io/v1 false ValidatingWebhookConfiguration customresourcedefinitions crd,crds apiextensions.k8s.io/v1 false CustomResourceDefinition apiservices apiregistration.k8s.io/v1 false APIService tokenreviews authentication.k8s.io/v1 false TokenReview selfsubjectaccessreviews authorization.k8s.io/v1 false SelfSubjectAccessReview selfsubjectrulesreviews authorization.k8s.io/v1 false SelfSubjectRulesReview subjectaccessreviews authorization.k8s.io/v1 false SubjectAccessReview certificatesigningrequests csr certificates.k8s.io/v1 false CertificateSigningRequest bgpconfigurations crd.projectcalico.org/v1 false BGPConfiguration bgppeers crd.projectcalico.org/v1 false BGPPeer blockaffinities crd.projectcalico.org/v1 false BlockAffinity caliconodestatuses crd.projectcalico.org/v1 false CalicoNodeStatus clusterinformations crd.projectcalico.org/v1 false ClusterInformation felixconfigurations crd.projectcalico.org/v1 false FelixConfiguration globalnetworkpolicies crd.projectcalico.org/v1 false GlobalNetworkPolicy globalnetworksets crd.projectcalico.org/v1 false GlobalNetworkSet hostendpoints crd.projectcalico.org/v1 false HostEndpoint ipamblocks crd.projectcalico.org/v1 false IPAMBlock ipamconfigs crd.projectcalico.org/v1 false IPAMConfig ipamhandles crd.projectcalico.org/v1 false IPAMHandle ippools crd.projectcalico.org/v1 false IPPool ipreservations crd.projectcalico.org/v1 false IPReservation kubecontrollersconfigurations crd.projectcalico.org/v1 false KubeControllersConfiguration flowschemas flowcontrol.apiserver.k8s.io/v1beta3 false FlowSchema prioritylevelconfigurations flowcontrol.apiserver.k8s.io/v1beta3 false PriorityLevelConfiguration nodes metrics.k8s.io/v1beta1 false NodeMetrics ingressclasses networking.k8s.io/v1 false IngressClass runtimeclasses node.k8s.io/v1 false RuntimeClass clusterrolebindings rbac.authorization.k8s.io/v1 false ClusterRoleBinding clusterroles rbac.authorization.k8s.io/v1 false ClusterRole priorityclasses pc scheduling.k8s.io/v1 false PriorityClass csidrivers storage.k8s.io/v1 false CSIDriver csinodes storage.k8s.io/v1 false CSINode storageclasses sc storage.k8s.io/v1 false StorageClass volumeattachments storage.k8s.io/v1 false VolumeAttachment 查看Namespace [root@k8s-master01 ~]# kubectl explain namespace | egrep -v \u0026#34;^\\s*https|^$\u0026#34; | sed s/\u0026#34;More info:\u0026#34;// KIND: Namespace VERSION: v1 DESCRIPTION: Namespace provides a scope for Names. Use of multiple namespaces is optional. FIELDS: apiVersion \u0026lt;string\u0026gt; APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. kind \u0026lt;string\u0026gt; Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. metadata \u0026lt;Object\u0026gt; Standard object\u0026#39;s metadata. spec \u0026lt;Object\u0026gt; Spec defines the behavior of the Namespace. status \u0026lt;Object\u0026gt; Status describes the current status of a Namespace. # namespace可简写为ns [root@k8s-master01 ~]# kubectl get ns --show-kind --show-labels NAME STATUS AGE LABELS namespace/default Active 7d kubernetes.io/metadata.name=default namespace/kube-node-lease Active 7d kubernetes.io/metadata.name=kube-node-lease namespace/kube-public Active 7d kubernetes.io/metadata.name=kube-public namespace/kube-system Active 7d kubernetes.io/metadata.name=kube-system 初始状态下，Kubernetes具有四个namespace：\ndefault：默认的namespace，默认用户创建的资源都是在这个namespace下 kube-node-lease：该namespace用于与各个节点相关的租约（Lease）对象；节点租期允许kubelet发送心跳，由此控制面能够检测到节点故障 kube-public：自动创建且所有用户可读的namespace（包括未经身份认证的） kube-system：Kubernetes系统创建的对象所在的namespace # 列出特定的ns [root@k8s-master01 ~]# kubectl get ns kube-public NAME STATUS AGE kube-public Active 7d # 获取特定的ns的描述信息 [root@k8s-master01 ~]# kubectl describe ns kube-public Name: kube-public Labels: kubernetes.io/metadata.name=kube-public Annotations: \u0026lt;none\u0026gt; Status: Active No resource quota. No LimitRange resource. # 如果配置了资源配额，可查看类似下面信息 [root@k8s-master01 ~]# kubectl describe ns quotatest Name: quotatest Labels: kubernetes.io/metadata.name=quotatest Annotations: \u0026lt;none\u0026gt; Status: Active Resource Quotas Name: resource-test Resource Used Hard -------- --- --- pods 1 1 Resource Limits Type Resource Min Max Default Request Default Limit Max Limit/Request Ratio ---- -------- --- --- --------------- ------------- ----------------------- Container cpu 100m 1 500m 1 - Container memory 100Mi 1Gi 256Mi 512Mi - PersistentVolumeClaim storage 1Gi 2Gi - - - 创建和删除 说明：避免使用前缀kube-创建新的namespace，因为它是为Kubernetes系统namespace保留的。\nNamespace命名规则:\n最多63个字符 只能包含字母数字,以及\u0026rsquo;-' 须以字母或数字开头 须以字母或数字结尾 # 1. yaml文件方式创建 [root@k8s-master01 ~]# vim new_namespace_deemoprobe.yaml apiVersion: v1 kind: Namespace metadata: name: deemoprobe annotations: type: create by yaml [root@k8s-master01 ~]# kubectl apply -f new_namespace_deemoprobe.yaml namespace/deemoprobe created [root@k8s-master01 ~]# kubectl get ns deemoprobe --show-labels --show-kind NAME STATUS AGE LABELS namespace/deemoprobe Active 36s kubernetes.io/metadata.name=deemoprobe # 2. 命令创建 [root@k8s-master01 ~]# kubectl create ns deemoprobe2 namespace/deemoprobe2 created [root@k8s-master01 ~]# kubectl get ns deemoprobe2 --show-labels --show-kind NAME STATUS AGE LABELS namespace/deemoprobe2 Active 12s kubernetes.io/metadata.name=deemoprobe2 # 删除namespace=deemoprobe2 [root@k8s-master01 ~]# kubectl delete ns deemoprobe2 namespace \u0026#34;deemoprobe2\u0026#34; deleted [root@k8s-master01 ~]# kubectl get ns deemoprobe2 --show-labels --show-kind Error from server (NotFound): namespaces \u0026#34;deemoprobe2\u0026#34; not found # 指定namespace操作其他在namespace作用域中的资源，如kubectl get pods --namespace=deemoprobe # 简写 -n deemoprobe [root@k8s-master01 ~]# kubectl get configmap -n deemoprobe NAME DATA AGE kube-root-ca.crt 1 3m33s 切换默认namespace # 查看当前所在namespace，新集群如果查不到，就是default [root@k8s-master01 ~]# kubectl config view | grep namespace: # 设置当前namespace上下文，使得相应namespace成为默认值 [root@k8s-master01 ~]# kubectl config set-context --current --namespace deemoprobe Context \u0026#34;kubernetes-admin@kubernetes\u0026#34; modified. [root@k8s-master01 ~]# kubectl config view | grep namespace: namespace: deemoprobe # 回切 [root@k8s-master01 ~]# kubectl config set-context --current --namespace default Context \u0026#34;kubernetes-admin@kubernetes\u0026#34; modified. [root@k8s-master01 ~]# kubectl config view | grep namespace: namespace: default namespace工具 安装namespace管理工具（kubens）更快捷地管理namespace\n[root@k8s-master01 ~]# curl -L https://github.com/ahmetb/kubectx/releases/download/v0.9.4/kubens -o /bin/kubens % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 643 100 643 0 0 163 0 0:00:03 0:00:03 --:--:-- 163 100 5555 100 5555 0 0 638 0 0:00:08 0:00:08 --:--:-- 1704 [root@k8s-master01 ~]# chmod +x /bin/kubens [root@k8s-master01 ~]# kubens -h USAGE: kubens : list the namespaces in the current context kubens \u0026lt;NAME\u0026gt; : change the active namespace of current context kubens - : switch to the previous namespace in this context kubens -c, --current : show the current namespace kubens -h,--help : show this message # 列出所有namespace， [root@k8s-master01 ~]# kubens deemoprobe default kube-node-lease kube-public kube-system # 查看当前namespace [root@k8s-master01 ~]# kubens -c default # 切换为deemoprobe [root@k8s-master01 ~]# kubens deemoprobe Context \u0026#34;kubernetes-admin@kubernetes\u0026#34; modified. Active namespace is \u0026#34;deemoprobe\u0026#34;. [root@k8s-master01 ~]# kubens -c deemoprobe # 切换为上一个namespace [root@k8s-master01 ~]# kubens - Context \u0026#34;kubernetes-admin@kubernetes\u0026#34; modified. Active namespace is \u0026#34;default\u0026#34;. [root@k8s-master01 ~]# kubens -c default 使用场景 比如：我们需要\u0026quot;测试\u0026quot;-\u0026ldquo;开发\u0026rdquo;-\u0026ldquo;生产\u0026quot;这几个环境，可以划分三个Namespace进行操作，为后续资源配额准备基础环境。\n[root@k8s-master01 ~]# vi namespace.yaml --- apiVersion: v1 kind: Namespace metadata: name: test labels: name: test --- apiVersion: v1 kind: Namespace metadata: name: dev labels: name: dev --- apiVersion: v1 kind: Namespace metadata: name: prod labels: name: prod [root@k8s-master01 ~]# kubectl apply -f namespace.yaml namespace/test created namespace/dev created namespace/prod created [root@k8s-master01 ~]# kubectl get ns --show-labels NAME STATUS AGE LABELS dev Active 64s name=dev prod Active 64s name=prod test Active 65s name=test ... # 全新的Namespace, 未设置任何资源配额和限制 [root@k8s-master01 ~]# kubectl describe ns test Name: test Labels: name=test Annotations: \u0026lt;none\u0026gt; Status: Active No resource quota. No LimitRange resource. [root@k8s-master01 ~]# kubectl describe ns dev Name: dev Labels: name=dev Annotations: \u0026lt;none\u0026gt; Status: Active No resource quota. No LimitRange resource. [root@k8s-master01 ~]# kubectl describe ns prod Name: prod Labels: name=prod Annotations: \u0026lt;none\u0026gt; Status: Active No resource quota. No LimitRange resource. ","permalink":"https://deemoprobe.github.io/posts/tech/kubernetes/kubernetesnamespace/","summary":"简介 Kubernetes支持多个虚拟集群，这些虚拟集群依赖于同一个物理集群，这些虚拟集群就可以称作namespace（中文习惯称为\u0026quot;名称空间\u0026quot;或\u0026quot;命名空间\u0026quot;, 个人觉得理解成\u0026quot;作用域\u0026quot;更为合理一些） Namespace有资源","title":"KubernetesNamespace"},{"content":"总览：配置命令自动补全功能，查看kubernetes集群的配置信息，kubernetes资源的增删改查，执行pod容器命令和查看资源日志。\n常用：Kubernetes官方kubectl备忘录\n命令自动补全 # 安装配置bash-completion yum install -y bash-completion # kubectl配置到bash-completion中 source \u0026lt;(kubectl completion bash) echo \u0026#34;source \u0026lt;(kubectl completion bash)\u0026#34; \u0026gt;\u0026gt; ~/.bashrc # 可以先用whereis bash-completion查出命令所在目录 source /usr/share/bash-completion/bash_completion echo \u0026#34;source /usr/share/bash-completion/bash_completion\u0026#34; \u0026gt;\u0026gt; ~/.bashrc 集群配置信息 # 查看集群简略信息，来自 ~/.kube/config 文件 或 /etc/kubernetes/admin.kubeconfig文件 kubectl config view # 切换集群 kubectl config use-context \u0026lt;cluster-name\u0026gt; # 查看集群所有用户信息 kubectl config view -o jsonpath=\u0026#39;{.users[*].name}\u0026#39; 查询资源 列出一个或多个资源（下面以pod为例，其他资源亦如此） # 查看k8s支持的资源类型 kubectl api-resources # 命名空间纳管的资源，false为非纳管资源 kubectl api-resources --namespaced=true # 支持list和get的资源 kubectl api-resources --verbs=list,get # 查看某资源的详细信息: 版本，支持的字段等 kubectl explain \u0026lt;apiresource-name\u0026gt; # 查看某资源下特定实例的详情 kubectl describe \u0026lt;apiresorce-name\u0026gt; \u0026lt;instance-name\u0026gt; # 列出当前namespace下所有 pod kubectl get pod # 列出所有namespace下的pod kubectl get pod -A # 扩展格式列出当前空间的所有pod kubectl get pod -owide # 查看pod并排序，根据name kubectl get pod --sort-by=.metadata.name # 查看pod并排序，根据重启次数 kubectl get pods --sort-by=.status.containerStatuses[0].restartCount # 列出在节点 k8s-node1 上运行的所有 pod kubectl get pods --field-selector=spec.nodeName=k8s-node1 or kubectl get pods -owide | grep k8s-node1 # 将单个资源的详细信息输出为 YAML 格式的对象 kubectl get pod nginx-6799fc88d8-5779h -oyaml kubectl get deploy nginx -oyaml # 列出pod便签信息 kubectl get pods --show-labels 创建资源 以文件或标准输入为准创建或更新资源 首次创建资源也可以使用kubectl create -f # 使用 example.yaml 中的定义创建资源 kubectl apply -f example.yaml # 创建多个文件 kubectl apply -f file1.yaml -f file2.yaml # 创建文件夹下所有资源 kubectl apply -f ./dir # 创建来自URL的资源 kubectl apply -f https://example.io/file.yaml # 创建单实例资源 kubectl create deployment nginx --image=nginx # 仅生成yaml文件但不创建实例 kubectl create deployment nginx --image=nginx --dry-run=client -oyaml \u0026gt; nginx.yaml # 创建名为hello的Job并打印hello kubectl create job hello --image=busybox -- echo \u0026#34;hello\u0026#34; # 创建名为hello的CronJob每分钟打印一次hello kubectl create cronjob hello --image=busybox --schedule=\u0026#34;*/1 * * * *\u0026#34; -- echo \u0026#34;hello\u0026#34; 删除资源 从文件、stdin 或指定标签选择器、名称、资源选择器或资源中删除资源 # 从文件 kubectl delete -f file.yaml # 从标签 kubectl delete pods -l name=\u0026lt;label-name\u0026gt; # 删除所有同类资源，如果有上层资源（如deploy）未删除，可能会自动重新创建pod kubectl delete \u0026lt;apiresorce-name\u0026gt; --all # 强制删除pod kubectl delete pod \u0026lt;name\u0026gt; --grace-period=0 --force -n \u0026lt;namespace\u0026gt; 修改资源 # kubectl set 支持更改如下字段 [root@k8s-master01 ~]# kubectl set env (Update environment variables on a pod template) image (Update the image of a pod template) resources (使用 Pod 模板更新对象的资源请求/限制) selector (为资源设置选择器) serviceaccount (Update the service account of a resource) subject (Update the user, group, or service account in a role binding or cluster role binding) # 例如：更改名为nginx的deployment的nginx镜像为v2版本 kubectl set image deploy nginx nginx=nginx:v2 # 查看更改历史 kubectl rollout history deployment nginx # 回滚到上一个版本 kubectl rollout undo deployment nginx # 回滚到指定版本 kubectl rollout undo deployment nginx --to-revision=3 # 直接应用编辑好的文件 kubectl apply -f name.yaml # kubectl edit 命令编辑某资源，调用默认编辑器操作（如vim） kubectl edit deployment nginx 扩缩容 # 调整名为nginx的deployment副本数目为2 kubectl scale --replicas=2 deployment nginx # 更改多个资源的副本数 kubectl scale --replicas=2 deploy/nginx deploy/tomcat deploy/redis # 如果副本数为3，则调整为2 kubectl scale --current-replicas=3 --replicas=2 deploy nginx 执行pod # 交互式命令行运行容器 kubectl run -it busybox --image=busybox -- sh # 运行nginx pod，如果没有该pod则自动创建 kubectl run nginx --image=nginx # 映射本地端口到pod, 将deployments/nginx中pod的80端口映射到本地的8080端口 kubectl port-forward TYPE/NAME [options] [LOCAL_PORT:]REMOTE_PORT kubectl port-forward deployments/nginx 8080:80 # 在 pod 中运行 date命令。默认情况下，输出来自第一个容器 kubectl exec \u0026lt;pod-name\u0026gt; -- date # 指定容器 kubectl exec \u0026lt;pod-name\u0026gt; -c \u0026lt;container-name\u0026gt; -- date # 交互运行，默认和第一个容器 kubectl exec -it \u0026lt;pod-name\u0026gt; -- sh 查看pod日志 打印 Pod 中容器的日志 # 获取pod日志 kubectl logs \u0026lt;pod-name\u0026gt; # pod日志流转，类似于 tail -f 命令 kubectl logs -f \u0026lt;pod-name\u0026gt; # 当pod中含有多个容器，需指定特定容器查看日志 kubectl logs \u0026lt;pod-name\u0026gt; -c \u0026lt;container-name\u0026gt; 节点和集群 # 标记节点为不可调度节点 kubectl cordon \u0026lt;node-name\u0026gt; # 解除不可调度标记 kubectl uncordon \u0026lt;node-name\u0026gt; # 安全驱逐准备回收的节点上所有pod到其他节点 kubectl drain \u0026lt;node-name\u0026gt; # 查看节点的度量信息 kubectl top node k8s-node # 查看集群主节点的地址 kubectl cluster-info kubectl插件 kubectl插件是一个独立的可执行文件，可以用任何编程语言或脚本编写插件，名称以kubectl-开头。要安装插件，将其可执行文件移动到PATH中的任何位置。插件无法覆盖已存在的kubectl命令（例如自定义kubectl-version插件，无法覆盖kubectl version命令，即和已有命令冲突时，插件不生效）\n# 创建一个简单的插件脚本，并为生成的可执行文件命名 # 以\u0026#34;kubectl-\u0026#34;为前缀 [root@k8s-master01 ~]# vim kubectl-hello #!/bin/bash echo \u0026#34;hello world\u0026#34; [root@k8s-master01 ~]# chmod +x kubectl-hello [root@k8s-master01 ~]# mv kubectl-hello /usr/local/bin/ [root@k8s-master01 ~]# kubectl hello hello world # \u0026#34;卸载\u0026#34;插件，删除脚本即可 [root@k8s-master01 ~]# rm /usr/local/bin/kubectl-hello # 查看插件 [root@k8s-master01 ~]# kubectl plugin list The following compatible plugins are available: /usr/local/bin/kubectl-hello # 创建一个实用场景，查看当前Kubernetes集群基本信息 [root@k8s-master01 ~]# vim kubectl-info #!/bin/bash echo \u0026#34;当前Kubernetes集群上下文为：\u0026#34; kubectl config view | grep current-context echo \u0026#34;当前集群用户为：\u0026#34; kubectl config view --template=\u0026#39;{{ range .contexts }}{{ if eq .name \u0026#34;\u0026#39;$(kubectl config current-context)\u0026#39;\u0026#34; }}Current user: {{ .context.user }}{{ end }}{{ end }}\u0026#39; echo \u0026#34;\u0026#34; [root@k8s-master01 ~]# chmod +x kubectl-info [root@k8s-master01 ~]# mv kubectl-info /usr/local/bin/ [root@k8s-master01 ~]# kubectl info 当前Kubernetes集群上下文为： current-context: kubernetes-admin@kubernetes 当前集群用户为： Current user: kubernetes-admin 实用工具 kubectx：管理kubernetes集群上下文（切换集群等）CLI工具 kubens：管理kubernetes namespace的CLI工具 上述CLI工具GitHub地址为：https://github.com/ahmetb/kubectx，为方便国内访问，已同步到本人阿里云OSS存储。\n[root@k8s-master01 ~]# curl https://deemoprobe.oss-cn-shanghai.aliyuncs.com/software/kubectx -o /usr/local/bin/kubectx % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 6108 100 6108 0 0 24527 0 --:--:-- --:--:-- --:--:-- 24530 [root@k8s-master01 ~]# curl https://deemoprobe.oss-cn-shanghai.aliyuncs.com/software/kubens -o /usr/local/bin/kubens % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 5555 100 5555 0 0 20118 0 --:--:-- --:--:-- --:--:-- 20200 [root@k8s-master01 ~]# chmod +x /usr/local/bin/kubens /usr/local/bin/kubectx [root@k8s-master01 ~]# kubectx kubernetes-admin@kubernetes [root@k8s-master01 ~]# kubens default kube-node-lease kube-public kube-system [root@k8s-master01 ~]# kubectx -h USAGE: kubectx : list the contexts kubectx \u0026lt;NAME\u0026gt; : switch to context \u0026lt;NAME\u0026gt; kubectx - : switch to the previous context kubectx -c, --current : show the current context name kubectx \u0026lt;NEW_NAME\u0026gt;=\u0026lt;NAME\u0026gt; : rename context \u0026lt;NAME\u0026gt; to \u0026lt;NEW_NAME\u0026gt; kubectx \u0026lt;NEW_NAME\u0026gt;=. : rename current-context to \u0026lt;NEW_NAME\u0026gt; kubectx -d \u0026lt;NAME\u0026gt; [\u0026lt;NAME...\u0026gt;] : delete context \u0026lt;NAME\u0026gt; (\u0026#39;.\u0026#39; for current-context) (this command won\u0026#39;t delete the user/cluster entry that is used by the context) kubectx -u, --unset : unset the current context kubectx -h,--help : show this message [root@k8s-master01 ~]# kubens -h USAGE: kubens : list the namespaces in the current context kubens \u0026lt;NAME\u0026gt; : change the active namespace of current context kubens - : switch to the previous namespace in this context kubens -c, --current : show the current namespace kubens -h,--help : show this message ","permalink":"https://deemoprobe.github.io/posts/tech/kubernetes/kubernetescli/","summary":"总览：配置命令自动补全功能，查看kubernetes集群的配置信息，kubernetes资源的增删改查，执行pod容器命令和查看资源日志。 常用：Kubernetes官方kubectl备忘录 命令自动补全 # 安装配置bash-completion yum install -y bash-completion # kubectl配置到bash-","title":"Kuberntes CLI"},{"content":"环境说明 宿主机系统：Windows 10 虚拟机版本：VMware® Workstation 16 Pro IOS镜像版本：CentOS Linux release 7.9.2009 Kubernetes版本：1.26.4 Runtime：Containerd v1.6.20 Etcd版本：3.5.6 集群操作用户：root 更新时间：2023-04-21 CentOS7安装请参考博客文章：LINUX之VMWARE WORKSTATION安装CENTOS-7\n资源分配 网段划分\nKubernetes集群需要规划三个网段：\n宿主机网段：Kubernetes集群节点的网段 Pod网段：集群内Pod的网段，相当于容器的IP Service网段：集群内服务发现使用的网段，service用于集群容器通信 生产环境根据申请到的IP资源进行分配即可，原则是三个网段不允许有重合IP。IP网段计算可以参考：在线IP地址计算。本文虚拟机练习环境IP地址网段分配如下：\n宿主机网段：192.168.43.1/24 Pod网段：172.16.0.0/12 Service：10.96.0.0/12 节点分配\n采用3管理节点2工作节点的高可用Kubernetes集群模式：\nk8s-master01/k8s-master02/k8s-master03 集群的Master节点 三个master节点同时做etcd集群 k8s-node01/k8s-node02 集群的Node节点 k8s-master-vip做高可用k8s-master01~03的VIP，不占用物理资源 主机节点名称 IP CPU核心数 内存大小 磁盘大小 k8s-master-vip 192.168.43.200 / / / k8s-master01 192.168.43.201 2 2G 40G k8s-master02 192.168.43.202 2 2G 40G k8s-master03 192.168.43.203 2 2G 40G k8s-node01 192.168.43.204 2 2G 40G k8s-node02 192.168.43.205 2 2G 40G 操作步骤 标题后小括号注释表明操作范围：\nALL 所有节点（k8s-master01/k8s-master02/k8s-master03/k8s-node01/k9s-node02）执行 Master 只需要在master节点（k8s-master01/k8s-master02/k8s-master03）执行 Node 只需要在node节点（k8s-node01/k8s-node02）执行 已标注的个别命令只需要在某一台机器执行，会在操作前说明 未标注的会在操作时说明 使用cat \u0026lt;\u0026lt; \u0026quot;EOF\u0026quot; \u0026gt;\u0026gt; file或cat \u0026gt;\u0026gt; file \u0026lt;\u0026lt; \u0026quot;EOF\u0026quot;添加文件内容注意cat后面的EOF一定要加上双引号（标准输入的），否则不会保留输入时的缩进格式而且会直接解析输入时的变量，进而造成文件可读性差甚至不可用；同时注意文件的\u0026gt;重写与\u0026gt;\u0026gt;追加。虽然单独转义输入时的变量也能避免变量被解析，但是不推荐，漏转义会造成不必要的麻烦。\n准备工作(ALL) 添加主机信息、关闭防火墙、关闭swap、关闭SELinux、dnsmasq、NetworkManager\n# 添加主机信息 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt;\u0026gt; /etc/hosts 192.168.43.200 k8s-master-vip 192.168.43.201 k8s-master01 192.168.43.202 k8s-master02 192.168.43.203 k8s-master03 192.168.43.204 k8s-node01 192.168.43.205 k8s-node02 EOF # 关闭防火墙、dnsmasq、NetworkManager，--now参数表示关闭服务并移除开机自启 # 这些服务是否可以关闭视情况而定，本文是虚拟机实践，没有用到这些服务 systemctl disable --now firewalld systemctl disable --now dnsmasq systemctl disable --now NetworkManager # 关闭swap，并注释fstab文件swap所在行 swapoff -a sed -i \u0026#39;/swap/s/^\\(.*\\)$/#\\1/g\u0026#39; /etc/fstab # 关闭SELinux，并更改selinux配置文件 setenforce 0 sed -i \u0026#34;s/=enforcing/=disabled/g\u0026#34; /etc/selinux/config 值得注意的是/etc/sysconfig/selinux文件是/etc/selinux/config文件的软连接，用sed -i命令修改软连接文件会破坏软连接属性，将/etc/sysconfig/selinux变为一个独立的文件，即使该文件被修改了，但源文件/etc/selinux/config配置是没变的。此外，使用vim等编辑器编辑源文件或链接文件（编辑模式不会修改文件属性）修改也可以。软链接原理可参考博客：LINUX之INODE详解\n# 默认的yum源太慢，更新为阿里源，同时用sed命令删除文件中不需要的两个URL的行 curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo sed -i -e \u0026#39;/mirrors.cloud.aliyuncs.com/d\u0026#39; -e \u0026#39;/mirrors.aliyuncs.com/d\u0026#39; /etc/yum.repos.d/CentOS-Base.repo # 安装常用工具包 yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git -y # 配置ntpdate，同步服务器时间 rpm -ivh http://mirrors.wlnmp.com/centos/wlnmp-release-centos.noarch.rpm yum install ntpdate -y # 同步时区和时间 ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime echo \u0026#39;Asia/Shanghai\u0026#39; \u0026gt;/etc/timezone ntpdate time2.aliyun.com # 可以加入计划任务，保证集群时钟是一致的 # /var/spool/cron/root文件也是crontab -e写入的文件 # crontab执行日志查看可用：tail -f /var/log/cron cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt;\u0026gt; /var/spool/cron/root */5 * * * * /usr/sbin/ntpdate time2.aliyun.com EOF # 须知：如果设置了定时任务，会经常收到提示“You have new mail in /var/spool/mail/root” # （可选）禁用提示：echo \u0026#34;unset MAILCHECK\u0026#34; \u0026gt;\u0026gt; ~/.bashrc;source ~/.bashrc # 禁用提示后/var/spool/mail/root文件依旧会记录root操作日志，可随时查看 # 保证文件句柄不会限制集群的可持续发展，配置limits ulimit -SHn 65500 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt;\u0026gt; /etc/security/limits.conf * soft nofile 65500 * hard nofile 65500 * soft nproc 65500 * hard nproc 65500 * soft memlock unlimited * hard memlock unlimited EOF # 配置免密登录，k8s-master01到其他节点 # 生成密钥对（在k8s-master01节点配置即可） ssh-keygen -t rsa # 拷贝公钥到其他节点，首次需要认证一下各个节点的root密码，以后就可以免密ssh到其他节点 for i in k8s-master02 k8s-master03 k8s-node01 k8s-node02;do ssh-copy-id -i .ssh/id_rsa.pub $i;done # 克隆二进制仓库1.26分支的文件（k8s-master01上操作即可） cd /root;git clone https://gitee.com/deemoprobe/k8s-ha-install.git -b manual-installation-v1.26.x # 所有节点系统升级 yum update --exclude=kernel* -y 升级内核，4.17以下的内核cgroup存在内存泄漏的BUG，具体分析过程浏览器搜Kubernetes集群为什么要升级内核会有很多文章讲解\n内核备用下载（下载到本地后上传到服务器，尽量不要用wget）：\nkernel-ml-devel-4.19.12-1.el7.elrepo.x86_64.rpm kernel-ml-4.19.12-1.el7.elrepo.x86_64.rpm # 下载4.19版本内核，如果无法下载，可以用上面提供的备用下载 cd /root wget http://193.49.22.109/elrepo/kernel/el7/x86_64/RPMS/kernel-ml-devel-4.19.12-1.el7.elrepo.x86_64.rpm wget http://193.49.22.109/elrepo/kernel/el7/x86_64/RPMS/kernel-ml-4.19.12-1.el7.elrepo.x86_64.rpm # 可以在k8s-master01节点下载后，免密传到其他节点 for i in k8s-master02 k8s-master03 k8s-node01 k8s-node02;do scp kernel-ml-* $i:/root;done # 所有节点安装内核 cd /root \u0026amp;\u0026amp; yum localinstall -y kernel-ml* # 所有节点更改内核启动顺序 grub2-set-default 0 \u0026amp;\u0026amp; grub2-mkconfig -o /etc/grub2.cfg grubby --args=\u0026#34;user_namespace.enable=1\u0026#34; --update-kernel=\u0026#34;$(grubby --default-kernel)\u0026#34; # 查看默认内核，并重启节点 grubby --default-kernel reboot # 确认内核版本 uname -a # （可选）删除老版本的内核，避免以后被升级取代默认的开机4.19内核 rpm -qa | grep kernel yum remove -y kernel-3* # 升级系统软件包（如果跳过内核升级加参数 --exclude=kernel*） yum update -y # 安装IPVS相关工具，由于IPVS在资源消耗和性能上均已明显优于iptables，所以推荐开启 # 具体原因可参考官网介绍 https://kubernetes.io/zh/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/ yum install ipvsadm ipset sysstat conntrack libseccomp -y # 加载模块，最后一条4.18及以下内核使用nf_conntrack_ipv4，4.19已改为nf_conntrack modprobe -- ip_vs modprobe -- ip_vs_rr modprobe -- ip_vs_wrr modprobe -- ip_vs_sh modprobe -- nf_conntrack # 编写参数文件 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/modules-load.d/ipvs.conf ip_vs ip_vs_lc ip_vs_wlc ip_vs_rr ip_vs_wrr ip_vs_lblc ip_vs_lblcr ip_vs_dh ip_vs_sh ip_vs_fo ip_vs_nq ip_vs_sed ip_vs_ftp ip_vs_sh nf_conntrack ip_tables ip_set xt_set ipt_set ipt_rpfilter ipt_REJECT ipip EOF # systemd-modules-load加入开机自启 systemctl enable --now systemd-modules-load # 自定义内核参数优化配置文件 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/sysctl.d/kubernetes.conf net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 fs.may_detach_mounts = 1 net.ipv4.conf.all.route_localnet = 1 vm.overcommit_memory=1 vm.panic_on_oom=0 fs.inotify.max_user_watches=89100 fs.file-max=52706963 fs.nr_open=52706963 net.netfilter.nf_conntrack_max=2310720 net.ipv4.tcp_keepalive_time = 600 net.ipv4.tcp_keepalive_probes = 3 net.ipv4.tcp_keepalive_intvl =15 net.ipv4.tcp_max_tw_buckets = 36000 net.ipv4.tcp_tw_reuse = 1 net.ipv4.tcp_max_orphans = 327680 net.ipv4.tcp_orphan_retries = 3 net.ipv4.tcp_syncookies = 1 net.ipv4.tcp_max_syn_backlog = 16384 net.ipv4.ip_conntrack_max = 65536 net.ipv4.tcp_max_syn_backlog = 16384 net.ipv4.tcp_timestamps = 0 net.core.somaxconn = 16384 EOF # 加载 sysctl --system # 重启查看IPVS模块是否依旧加载 reboot lsmod | grep -e ip_vs -e nf_conntrack 保证每台服务器中IPVS加载成功，以k8s-master01为例，如图： 部署Containerd(ALL) Kubernetes1.24版本以后将不再支持Docker作为Runtime，本文安装使用Containerd作为Runtime。\n# 配置阿里docker源 yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo # 安装最新版本docker和containerd.io，安装docker是为了使用docker CLI yum install docker-ce docker-ce-cli containerd.io -y # （可选）也可以按需安装指定版本 yum install docker-ce-20.10.* docker-ce-cli-20.10.* containerd -y # 配置Containerd模块 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/modules-load.d/containerd.conf overlay br_netfilter EOF # 加载模块 modprobe -- overlay modprobe -- br_netfilter # 配置内核参数 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/sysctl.d/containerd.conf net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 EOF # 加载内核参数 sysctl --system # 生成默认配置文件 mkdir -p /etc/containerd containerd config default | tee /etc/containerd/config.toml # 更改Cgroup为Systemd，在containerd.runtimes.runc.options行后的SystemdCgroup = false修改为true，如果配置项不存在就自行添加，缩进俩空格添加SystemdCgroup = true一行 vim /etc/containerd/config.toml ... [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc.options] SystemdCgroup = true ... # 将sandbox_image的Pause镜像地址改成国内：registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.7 vim /etc/containerd/config.toml sandbox_image = \u0026#34;registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.7\u0026#34; # （可选）也可以在k8s-master01编辑/etc/containerd/config.toml文件后，将编辑后同名文件同步到其他服务器，自动覆盖 for i in k8s-master02 k8s-master03 k8s-node01 k8s-node02;do scp /etc/containerd/config.toml $i:/etc/containerd/config.toml;done # 查看确认是否配置成功 cat /etc/containerd/config.toml | grep -e Systemd -e sandbox_image # 启动并加入开机自启 systemctl daemon-reload systemctl enable --now containerd # containerd运行时的CLI是ctr [root@k8s-master01 ~]# ctr images ls REF TYPE DIGEST SIZE PLATFORMS LABELS [root@k8s-master01 ~]# ctr version Client: Version: 1.6.20 Revision: 2806fc1057397dbaeefbea0e4e17bddfbd388f38 Go version: go1.19.7 Server: Version: 1.6.20 Revision: 2806fc1057397dbaeefbea0e4e17bddfbd388f38 UUID: 9958928e-300c-4778-b83c-6c0073414f3e # 配置crictl连接的运行时socket接口（指向containerd\u0026#39;s GRPC server：/run/containerd/containerd.sock） # crictl 默认连接到unix:///var/run/dockershim.sock cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/crictl.yaml runtime-endpoint: unix:///run/containerd/containerd.sock image-endpoint: unix:///run/containerd/containerd.sock timeout: 10 debug: false EOF # crictl按需选择，https://github.com/kubernetes-sigs/cri-tools/releases # containerd容器管理CLI是crictl，该命令使用和docker命令类似，下载包上传到k8s-master01 [root@k8s-master01 ~]# tar -zxvf crictl-v1.27.0-linux-amd64.tar.gz [root@k8s-master01 ~]# mv crictl /usr/local/bin/ [root@k8s-master01 ~]# crictl version Version: 0.1.0 RuntimeName: containerd RuntimeVersion: 1.6.20 RuntimeApiVersion: v1 # 二进制可执行文件发送到其他节点 [root@k8s-master01 ~]# for i in k8s-master02 k8s-master03 k8s-node01 k8s-node02;do scp /usr/local/bin/crictl $i:/usr/local/bin/;done crictl 是CRI（Container Runtime Interface）兼容的容器运行时的CLI（Command-Line Interface）。可以这个命令来检查和调试 Kubernetes 节点上的容器运行时和应用程序。介绍和安装方式可见：critools或Kubernetes官方介绍\n二进制包和证书 k8s-master01节点上下载并安装Kubernetes二进制安装包（server-binaries，选择对应的架构即可）和ETCD二进制安装包，可在GitHub上查看Kubernetes1.23.x版本的信息，官方GitHub-Kubernetes1.23版本链接，ETCD官方链接\n# 以下在k8s-master01执行 # 下载太慢的话可以在相应链接网页上下载好传到服务器 wget https://dl.k8s.io/v1.26.4/kubernetes-server-linux-amd64.tar.gz wget https://github.com/etcd-io/etcd/releases/download/v3.5.6/etcd-v3.5.6-linux-amd64.tar.gz # 解压安装，--strip-components=N表示解压时忽略解压后的N层目录，直接获取N层目录后的目标文件。kubernetes/server/bin/是三层，etcd-v3.5.6-linux-amd64/是一层 tar -zxvf kubernetes-server-linux-amd64.tar.gz --strip-components=3 -C /usr/local/bin kubernetes/server/bin/kube{let,ctl,-apiserver,-controller-manager,-scheduler,-proxy} tar -zxvf etcd-v3.5.6-linux-amd64.tar.gz --strip-components=1 -C /usr/local/bin etcd-v3.5.6-linux-amd64/etcd{,ctl} # 确认版本 kubectl version kubelet --version etcdctl version # 拷贝组件到其他节点，Node节点只需要kubelet和kube-proxy for i in k8s-master02 k8s-master03; do scp /usr/local/bin/kube{let,ctl,-apiserver,-controller-manager,-scheduler,-proxy} $i:/usr/local/bin/; scp /usr/local/bin/etcd* $i:/usr/local/bin/; done for i in k8s-node01 k8s-node02; do scp /usr/local/bin/kube{let,-proxy} $i:/usr/local/bin/; done 配置证书 # master节点创建etcd证书目录 mkdir -p /etc/etcd/ssl # 所有节点创建pki证书目录和CNI目录（后面calico使用） mkdir -p /etc/kubernetes/pki mkdir -p /opt/cni/bin # 以下在k8s-master01操作 # 安装证书生成工具，如果速度慢可以浏览器下载后上传至服务器改名即可 wget \u0026#34;https://pkg.cfssl.org/R1.2/cfssl_linux-amd64\u0026#34; -O /usr/local/bin/cfssl wget \u0026#34;https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64\u0026#34; -O /usr/local/bin/cfssljson chmod +x /usr/local/bin/cfssl /usr/local/bin/cfssljson # 在master01节点生成etcd证书 cd /root/k8s-ha-install/pki cfssl gencert -initca etcd-ca-csr.json | cfssljson -bare /etc/etcd/ssl/etcd-ca cfssl gencert \\ -ca=/etc/etcd/ssl/etcd-ca.pem \\ -ca-key=/etc/etcd/ssl/etcd-ca-key.pem \\ -config=ca-config.json \\ -hostname=127.0.0.1,k8s-master01,k8s-master02,k8s-master03,192.168.43.201,192.168.43.202,192.168.43.203 \\ -profile=kubernetes \\ etcd-csr.json | cfssljson -bare /etc/etcd/ssl/etcd # 复制etcd证书到其他master节点 for i in k8s-master02 k8s-master03; do for FILE in etcd-ca-key.pem etcd-ca.pem etcd-key.pem etcd.pem; do scp /etc/etcd/ssl/${FILE} $i:/etc/etcd/ssl/${FILE} done done # 生成Kubernetes集群ca证书 cd /root/k8s-ha-install/pki cfssl gencert -initca ca-csr.json | cfssljson -bare /etc/kubernetes/pki/ca # k8s service网段10.96.0.0/12，填入网段第一个IP；集群VIP地址192.168.43.200 cfssl gencert -ca=/etc/kubernetes/pki/ca.pem -ca-key=/etc/kubernetes/pki/ca-key.pem -config=ca-config.json -hostname=10.96.0.1,192.168.43.200,127.0.0.1,kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.default.svc.cluster.local,192.168.43.201,192.168.43.202,192.168.43.203 -profile=kubernetes apiserver-csr.json | cfssljson -bare /etc/kubernetes/pki/apiserver # 生成apiserver的第三方组件使用的聚合证书，告警可以忽略 cfssl gencert -initca front-proxy-ca-csr.json | cfssljson -bare /etc/kubernetes/pki/front-proxy-ca cfssl gencert -ca=/etc/kubernetes/pki/front-proxy-ca.pem -ca-key=/etc/kubernetes/pki/front-proxy-ca-key.pem -config=ca-config.json -profile=kubernetes front-proxy-client-csr.json | cfssljson -bare /etc/kubernetes/pki/front-proxy-client # 生成controller-manager证书 cfssl gencert \\ -ca=/etc/kubernetes/pki/ca.pem \\ -ca-key=/etc/kubernetes/pki/ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ manager-csr.json | cfssljson -bare /etc/kubernetes/pki/controller-manager # 设置集群信息，集群名：kubernetes server：https://192.168.43.200:8443 # 如果不是高可用集群，192.168.43.200:8443改为master01的地址，8443改为apiserver的端口，默认是6443 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/pki/ca.pem \\ --embed-certs=true \\ --server=https://192.168.43.200:8443 \\ --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig # 设置上下文信息，context为system:kube-controller-manager@kubernetes kubectl config set-context system:kube-controller-manager@kubernetes \\ --cluster=kubernetes \\ --user=system:kube-controller-manager \\ --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig # 设置用户认证信息，用户为system:kube-controller-manager kubectl config set-credentials system:kube-controller-manager \\ --client-certificate=/etc/kubernetes/pki/controller-manager.pem \\ --client-key=/etc/kubernetes/pki/controller-manager-key.pem \\ --embed-certs=true \\ --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig # 设置默认集群环境 kubectl config use-context system:kube-controller-manager@kubernetes \\ --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig # 生成scheduler证书 cfssl gencert \\ -ca=/etc/kubernetes/pki/ca.pem \\ -ca-key=/etc/kubernetes/pki/ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ scheduler-csr.json | cfssljson -bare /etc/kubernetes/pki/scheduler # 同样的，scheduler需要和controller-manager设置相同的集群上下文等信息 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/pki/ca.pem \\ --embed-certs=true \\ --server=https://192.168.43.200:8443 \\ --kubeconfig=/etc/kubernetes/scheduler.kubeconfig kubectl config set-credentials system:kube-scheduler \\ --client-certificate=/etc/kubernetes/pki/scheduler.pem \\ --client-key=/etc/kubernetes/pki/scheduler-key.pem \\ --embed-certs=true \\ --kubeconfig=/etc/kubernetes/scheduler.kubeconfig kubectl config set-context system:kube-scheduler@kubernetes \\ --cluster=kubernetes \\ --user=system:kube-scheduler \\ --kubeconfig=/etc/kubernetes/scheduler.kubeconfig kubectl config use-context system:kube-scheduler@kubernetes \\ --kubeconfig=/etc/kubernetes/scheduler.kubeconfig # 配置admin证书，以及集群上下文等信息 cfssl gencert \\ -ca=/etc/kubernetes/pki/ca.pem \\ -ca-key=/etc/kubernetes/pki/ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ admin-csr.json | cfssljson -bare /etc/kubernetes/pki/admin kubectl config set-cluster kubernetes --certificate-authority=/etc/kubernetes/pki/ca.pem --embed-certs=true --server=https://192.168.43.200:8443 --kubeconfig=/etc/kubernetes/admin.kubeconfig kubectl config set-credentials kubernetes-admin --client-certificate=/etc/kubernetes/pki/admin.pem --client-key=/etc/kubernetes/pki/admin-key.pem --embed-certs=true --kubeconfig=/etc/kubernetes/admin.kubeconfig kubectl config set-context kubernetes-admin@kubernetes --cluster=kubernetes --user=kubernetes-admin --kubeconfig=/etc/kubernetes/admin.kubeconfig kubectl config use-context kubernetes-admin@kubernetes --kubeconfig=/etc/kubernetes/admin.kubeconfig # 创建ServiceAccount密钥对 openssl genrsa -out /etc/kubernetes/pki/sa.key 2048 openssl rsa -in /etc/kubernetes/pki/sa.key -pubout -out /etc/kubernetes/pki/sa.pub # 拷贝证书到其他master节点 for i in k8s-master02 k8s-master03; do for FILE in $(ls /etc/kubernetes/pki | grep -v etcd); do scp /etc/kubernetes/pki/${FILE} $i:/etc/kubernetes/pki/${FILE}; done; for FILE in admin.kubeconfig controller-manager.kubeconfig scheduler.kubeconfig; do scp /etc/kubernetes/${FILE} $i:/etc/kubernetes/${FILE}; done; done # 查看证书数量是否为23 ls /etc/kubernetes/pki/ | wc -l 23 ETCD集群(Master) 如果Etcd集群服务器和Kubernetes集群服务器不重合（即独立的Etcd集群），需要根据实际情况配置集群IP。\n# master01 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/etcd/etcd.config.yml name: \u0026#39;k8s-master01\u0026#39; data-dir: /var/lib/etcd wal-dir: /var/lib/etcd/wal snapshot-count: 5000 heartbeat-interval: 100 election-timeout: 1000 quota-backend-bytes: 0 listen-peer-urls: \u0026#39;https://192.168.43.201:2380\u0026#39; listen-client-urls: \u0026#39;https://192.168.43.201:2379,http://127.0.0.1:2379\u0026#39; max-snapshots: 3 max-wals: 5 cors: initial-advertise-peer-urls: \u0026#39;https://192.168.43.201:2380\u0026#39; advertise-client-urls: \u0026#39;https://192.168.43.201:2379\u0026#39; discovery: discovery-fallback: \u0026#39;proxy\u0026#39; discovery-proxy: discovery-srv: initial-cluster: \u0026#39;k8s-master01=https://192.168.43.201:2380,k8s-master02=https://192.168.43.202:2380,k8s-master03=https://192.168.43.203:2380\u0026#39; initial-cluster-token: \u0026#39;etcd-k8s-cluster\u0026#39; initial-cluster-state: \u0026#39;new\u0026#39; strict-reconfig-check: false enable-v2: true enable-pprof: true proxy: \u0026#39;off\u0026#39; proxy-failure-wait: 5000 proxy-refresh-interval: 30000 proxy-dial-timeout: 1000 proxy-write-timeout: 5000 proxy-read-timeout: 0 client-transport-security: cert-file: \u0026#39;/etc/kubernetes/pki/etcd/etcd.pem\u0026#39; key-file: \u0026#39;/etc/kubernetes/pki/etcd/etcd-key.pem\u0026#39; client-cert-auth: true trusted-ca-file: \u0026#39;/etc/kubernetes/pki/etcd/etcd-ca.pem\u0026#39; auto-tls: true peer-transport-security: cert-file: \u0026#39;/etc/kubernetes/pki/etcd/etcd.pem\u0026#39; key-file: \u0026#39;/etc/kubernetes/pki/etcd/etcd-key.pem\u0026#39; peer-client-cert-auth: true trusted-ca-file: \u0026#39;/etc/kubernetes/pki/etcd/etcd-ca.pem\u0026#39; auto-tls: true debug: false log-package-levels: log-outputs: [default] force-new-cluster: false EOF # master02 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/etcd/etcd.config.yml name: \u0026#39;k8s-master02\u0026#39; data-dir: /var/lib/etcd wal-dir: /var/lib/etcd/wal snapshot-count: 5000 heartbeat-interval: 100 election-timeout: 1000 quota-backend-bytes: 0 listen-peer-urls: \u0026#39;https://192.168.43.202:2380\u0026#39; listen-client-urls: \u0026#39;https://192.168.43.202:2379,http://127.0.0.1:2379\u0026#39; max-snapshots: 3 max-wals: 5 cors: initial-advertise-peer-urls: \u0026#39;https://192.168.43.202:2380\u0026#39; advertise-client-urls: \u0026#39;https://192.168.43.202:2379\u0026#39; discovery: discovery-fallback: \u0026#39;proxy\u0026#39; discovery-proxy: discovery-srv: initial-cluster: \u0026#39;k8s-master01=https://192.168.43.201:2380,k8s-master02=https://192.168.43.202:2380,k8s-master03=https://192.168.43.203:2380\u0026#39; initial-cluster-token: \u0026#39;etcd-k8s-cluster\u0026#39; initial-cluster-state: \u0026#39;new\u0026#39; strict-reconfig-check: false enable-v2: true enable-pprof: true proxy: \u0026#39;off\u0026#39; proxy-failure-wait: 5000 proxy-refresh-interval: 30000 proxy-dial-timeout: 1000 proxy-write-timeout: 5000 proxy-read-timeout: 0 client-transport-security: cert-file: \u0026#39;/etc/kubernetes/pki/etcd/etcd.pem\u0026#39; key-file: \u0026#39;/etc/kubernetes/pki/etcd/etcd-key.pem\u0026#39; client-cert-auth: true trusted-ca-file: \u0026#39;/etc/kubernetes/pki/etcd/etcd-ca.pem\u0026#39; auto-tls: true peer-transport-security: cert-file: \u0026#39;/etc/kubernetes/pki/etcd/etcd.pem\u0026#39; key-file: \u0026#39;/etc/kubernetes/pki/etcd/etcd-key.pem\u0026#39; peer-client-cert-auth: true trusted-ca-file: \u0026#39;/etc/kubernetes/pki/etcd/etcd-ca.pem\u0026#39; auto-tls: true debug: false log-package-levels: log-outputs: [default] force-new-cluster: false EOF # master03 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/etcd/etcd.config.yml name: \u0026#39;k8s-master03\u0026#39; data-dir: /var/lib/etcd wal-dir: /var/lib/etcd/wal snapshot-count: 5000 heartbeat-interval: 100 election-timeout: 1000 quota-backend-bytes: 0 listen-peer-urls: \u0026#39;https://192.168.43.203:2380\u0026#39; listen-client-urls: \u0026#39;https://192.168.43.203:2379,http://127.0.0.1:2379\u0026#39; max-snapshots: 3 max-wals: 5 cors: initial-advertise-peer-urls: \u0026#39;https://192.168.43.203:2380\u0026#39; advertise-client-urls: \u0026#39;https://192.168.43.203:2379\u0026#39; discovery: discovery-fallback: \u0026#39;proxy\u0026#39; discovery-proxy: discovery-srv: initial-cluster: \u0026#39;k8s-master01=https://192.168.43.201:2380,k8s-master02=https://192.168.43.202:2380,k8s-master03=https://192.168.43.203:2380\u0026#39; initial-cluster-token: \u0026#39;etcd-k8s-cluster\u0026#39; initial-cluster-state: \u0026#39;new\u0026#39; strict-reconfig-check: false enable-v2: true enable-pprof: true proxy: \u0026#39;off\u0026#39; proxy-failure-wait: 5000 proxy-refresh-interval: 30000 proxy-dial-timeout: 1000 proxy-write-timeout: 5000 proxy-read-timeout: 0 client-transport-security: cert-file: \u0026#39;/etc/kubernetes/pki/etcd/etcd.pem\u0026#39; key-file: \u0026#39;/etc/kubernetes/pki/etcd/etcd-key.pem\u0026#39; client-cert-auth: true trusted-ca-file: \u0026#39;/etc/kubernetes/pki/etcd/etcd-ca.pem\u0026#39; auto-tls: true peer-transport-security: cert-file: \u0026#39;/etc/kubernetes/pki/etcd/etcd.pem\u0026#39; key-file: \u0026#39;/etc/kubernetes/pki/etcd/etcd-key.pem\u0026#39; peer-client-cert-auth: true trusted-ca-file: \u0026#39;/etc/kubernetes/pki/etcd/etcd-ca.pem\u0026#39; auto-tls: true debug: false log-package-levels: log-outputs: [default] force-new-cluster: false EOF # 在所有master节点创建etcd服务并启动 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /usr/lib/systemd/system/etcd.service [Unit] Description=Etcd Service Documentation=https://coreos.com/etcd/docs/latest/ After=network.target [Service] Type=notify ExecStart=/usr/local/bin/etcd --config-file=/etc/etcd/etcd.config.yml Restart=on-failure RestartSec=10 LimitNOFILE=65536 [Install] WantedBy=multi-user.target Alias=etcd3.service EOF # 所有Master节点创建etcd证书目录并链接证书，否则启动会失败 mkdir /etc/kubernetes/pki/etcd ln -s /etc/etcd/ssl/* /etc/kubernetes/pki/etcd/ # 启动 systemctl daemon-reload \u0026amp;\u0026amp; systemctl enable --now etcd # 查看etcd集群状态如下即可 ETCDCTL_API=3 etcdctl --endpoints=\u0026#34;192.168.43.203:2379,192.168.43.202:2379,192.168.43.201:2379\u0026#34; --cacert=/etc/kubernetes/pki/etcd/etcd-ca.pem --cert=/etc/kubernetes/pki/etcd/etcd.pem --key=/etc/kubernetes/pki/etcd/etcd-key.pem endpoint status --write-out=table +---------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS | +---------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ | 192.168.43.203:2379 | fd1372a073304e | 3.5.1 | 20 kB | false | false | 2 | 9 | 9 | | | 192.168.43.202:2379 | 837ce9c47e0719eb | 3.5.1 | 20 kB | false | false | 2 | 9 | 9 | | | 192.168.43.201:2379 | e9bf8d99824c9061 | 3.5.1 | 20 kB | true | false | 2 | 9 | 9 | | +---------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ 高可用组件(Master) # 所有master节点安装Keepalived和haproxy，并创建配置文件目录 yum install keepalived haproxy -y mkdir /etc/haproxy mkdir /etc/keepalived # 为所有master节点添加haproxy配置，配置都一样，检查最后三行主机名和IP地址对应上就行 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/haproxy/haproxy.cfg global maxconn 2000 ulimit-n 16384 log 127.0.0.1 local0 err stats timeout 30s defaults log global mode http option httplog timeout connect 5000 timeout client 50000 timeout server 50000 timeout http-request 15s timeout http-keep-alive 15s frontend monitor-in bind *:33305 mode http option httplog monitor-uri /monitor frontend k8s-master bind 0.0.0.0:8443 bind 127.0.0.1:8443 mode tcp option tcplog tcp-request inspect-delay 5s default_backend k8s-master backend k8s-master mode tcp option tcplog option tcp-check balance roundrobin default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100 server k8s-master01 192.168.43.201:6443 check server k8s-master02 192.168.43.202:6443 check server k8s-master03 192.168.43.203:6443 check EOF # keepalived配置不一样，注意区分网卡名、IP地址和虚拟IP地址 # 检查服务器网卡名 ip a 或 ifconfig # k8s-master01 Keepalived配置 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/keepalived/keepalived.conf ! Configuration File for keepalived global_defs { router_id LVS_DEVEL script_user root enable_script_security } vrrp_script chk_apiserver { script \u0026#34;/etc/keepalived/check_apiserver.sh\u0026#34; interval 5 weight -5 fall 2 rise 1 } vrrp_instance VI_1 { state MASTER interface ens33 mcast_src_ip 192.168.43.201 virtual_router_id 51 priority 101 advert_int 2 authentication { auth_type PASS auth_pass K8SHA_KA_AUTH } virtual_ipaddress { 192.168.43.200 } track_script { chk_apiserver } } EOF # k8s-master02 Keepalived配置 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/keepalived/keepalived.conf ! Configuration File for keepalived global_defs { router_id LVS_DEVEL script_user root enable_script_security } vrrp_script chk_apiserver { script \u0026#34;/etc/keepalived/check_apiserver.sh\u0026#34; interval 5 weight -5 fall 2 rise 1 } vrrp_instance VI_1 { state BACKUP interface ens33 mcast_src_ip 192.168.43.202 virtual_router_id 51 priority 100 advert_int 2 authentication { auth_type PASS auth_pass K8SHA_KA_AUTH } virtual_ipaddress { 192.168.43.200 } track_script { chk_apiserver } } EOF # k8s-master03 Keepalived配置 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/keepalived/keepalived.conf ! Configuration File for keepalived global_defs { router_id LVS_DEVEL script_user root enable_script_security } vrrp_script chk_apiserver { script \u0026#34;/etc/keepalived/check_apiserver.sh\u0026#34; interval 5 weight -5 fall 2 rise 1 } vrrp_instance VI_1 { state BACKUP interface ens33 mcast_src_ip 192.168.43.203 virtual_router_id 51 priority 100 advert_int 2 authentication { auth_type PASS auth_pass K8SHA_KA_AUTH } virtual_ipaddress { 192.168.43.200 } track_script { chk_apiserver } } EOF # 所有master节点配置Keepalived健康检查脚本 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/keepalived/check_apiserver.sh #!/bin/bash err=0 for k in $(seq 1 3) do check_code=$(pgrep haproxy) if [[ $check_code == \u0026#34;\u0026#34; ]]; then err=$(expr $err + 1) sleep 1 continue else err=0 break fi done if [[ $err != \u0026#34;0\u0026#34; ]]; then echo \u0026#34;systemctl stop keepalived\u0026#34; /usr/bin/systemctl stop keepalived exit 1 else exit 0 fi EOF # 赋予可执行权限 chmod +x /etc/keepalived/check_apiserver.sh # 启动haproxy和Keepalived并加入开机启动 systemctl daemon-reload \u0026amp;\u0026amp; systemctl enable --now haproxy \u0026amp;\u0026amp; systemctl enable --now keepalived # 检查服务是否正常 # 这种告警可以忽略：Mar 06 14:03:35 k8s-master01 haproxy-systemd-wrapper[1981]: [WARNING] 064/140335 (1982) : config : frontend \u0026#39;GLOBAL\u0026#39; has no \u0026#39;bind\u0026#39; systemctl status haproxy systemctl status keepalived # 测试一波 telnet k8s-master-vip 8443 ping k8s-master-vip 配置集群组件 # 所有节点创建以下集群资源目录 mkdir -p /etc/kubernetes/manifests/ /etc/systemd/system/kubelet.service.d /var/lib/kubelet /var/log/kubernetes Apiserver(Master)\n所有master节点配置kube-apiserver服务 # service网段为10.96.0.0/12，可自行设置，其他参数按需配置即可 # master01配置 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /usr/lib/systemd/system/kube-apiserver.service [Unit] Description=Kubernetes API Server Documentation=https://github.com/kubernetes/kubernetes After=network.target [Service] ExecStart=/usr/local/bin/kube-apiserver \\ --v=2 \\ --allow-privileged=true \\ --bind-address=0.0.0.0 \\ --secure-port=6443 \\ --advertise-address=192.168.43.201 \\ --service-cluster-ip-range=10.96.0.0/12 \\ --service-node-port-range=30000-32767 \\ --etcd-servers=https://192.168.43.201:2379,https://192.168.43.202:2379,https://192.168.43.203:2379 \\ --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem \\ --etcd-certfile=/etc/etcd/ssl/etcd.pem \\ --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \\ --client-ca-file=/etc/kubernetes/pki/ca.pem \\ --tls-cert-file=/etc/kubernetes/pki/apiserver.pem \\ --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem \\ --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem \\ --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem \\ --service-account-key-file=/etc/kubernetes/pki/sa.pub \\ --service-account-signing-key-file=/etc/kubernetes/pki/sa.key \\ --service-account-issuer=https://kubernetes.default.svc.cluster.local \\ --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname \\ --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota \\ --authorization-mode=Node,RBAC \\ --enable-bootstrap-token-auth=true \\ --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem \\ --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem \\ --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem \\ --requestheader-allowed-names=aggregator \\ --requestheader-group-headers=X-Remote-Group \\ --requestheader-extra-headers-prefix=X-Remote-Extra- \\ --requestheader-username-headers=X-Remote-User # --token-auth-file=/etc/kubernetes/token.csv Restart=on-failure RestartSec=10s LimitNOFILE=65535 [Install] WantedBy=multi-user.target EOF # master02配置 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /usr/lib/systemd/system/kube-apiserver.service [Unit] Description=Kubernetes API Server Documentation=https://github.com/kubernetes/kubernetes After=network.target [Service] ExecStart=/usr/local/bin/kube-apiserver \\ --v=2 \\ --allow-privileged=true \\ --bind-address=0.0.0.0 \\ --secure-port=6443 \\ --advertise-address=192.168.43.202 \\ --service-cluster-ip-range=10.96.0.0/12 \\ --service-node-port-range=30000-32767 \\ --etcd-servers=https://192.168.43.201:2379,https://192.168.43.202:2379,https://192.168.43.203:2379 \\ --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem \\ --etcd-certfile=/etc/etcd/ssl/etcd.pem \\ --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \\ --client-ca-file=/etc/kubernetes/pki/ca.pem \\ --tls-cert-file=/etc/kubernetes/pki/apiserver.pem \\ --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem \\ --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem \\ --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem \\ --service-account-key-file=/etc/kubernetes/pki/sa.pub \\ --service-account-signing-key-file=/etc/kubernetes/pki/sa.key \\ --service-account-issuer=https://kubernetes.default.svc.cluster.local \\ --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname \\ --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota \\ --authorization-mode=Node,RBAC \\ --enable-bootstrap-token-auth=true \\ --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem \\ --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem \\ --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem \\ --requestheader-allowed-names=aggregator \\ --requestheader-group-headers=X-Remote-Group \\ --requestheader-extra-headers-prefix=X-Remote-Extra- \\ --requestheader-username-headers=X-Remote-User # --token-auth-file=/etc/kubernetes/token.csv Restart=on-failure RestartSec=10s LimitNOFILE=65535 [Install] WantedBy=multi-user.target EOF # master03配置 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /usr/lib/systemd/system/kube-apiserver.service [Unit] Description=Kubernetes API Server Documentation=https://github.com/kubernetes/kubernetes After=network.target [Service] ExecStart=/usr/local/bin/kube-apiserver \\ --v=2 \\ --allow-privileged=true \\ --bind-address=0.0.0.0 \\ --secure-port=6443 \\ --advertise-address=192.168.43.203 \\ --service-cluster-ip-range=10.96.0.0/12 \\ --service-node-port-range=30000-32767 \\ --etcd-servers=https://192.168.43.201:2379,https://192.168.43.202:2379,https://192.168.43.203:2379 \\ --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem \\ --etcd-certfile=/etc/etcd/ssl/etcd.pem \\ --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \\ --client-ca-file=/etc/kubernetes/pki/ca.pem \\ --tls-cert-file=/etc/kubernetes/pki/apiserver.pem \\ --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem \\ --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem \\ --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem \\ --service-account-key-file=/etc/kubernetes/pki/sa.pub \\ --service-account-signing-key-file=/etc/kubernetes/pki/sa.key \\ --service-account-issuer=https://kubernetes.default.svc.cluster.local \\ --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname \\ --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota \\ --authorization-mode=Node,RBAC \\ --enable-bootstrap-token-auth=true \\ --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem \\ --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem \\ --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem \\ --requestheader-allowed-names=aggregator \\ --requestheader-group-headers=X-Remote-Group \\ --requestheader-extra-headers-prefix=X-Remote-Extra- \\ --requestheader-username-headers=X-Remote-User # --token-auth-file=/etc/kubernetes/token.csv Restart=on-failure RestartSec=10s LimitNOFILE=65535 [Install] WantedBy=multi-user.target EOF # 启动kube-apiserver systemctl daemon-reload \u0026amp;\u0026amp; systemctl enable --now kube-apiserver ControllerManager(Master)\n所有Master节点配置kube-controller-manager服务，配置都一样\n# Pod网段是172.16.0.0/12，可按需更改，不要和其他在用网段冲突即可 # 给所有master节点配置服务 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /usr/lib/systemd/system/kube-controller-manager.service [Unit] Description=Kubernetes Controller Manager Documentation=https://github.com/kubernetes/kubernetes After=network.target [Service] ExecStart=/usr/local/bin/kube-controller-manager \\ --v=2 \\ --root-ca-file=/etc/kubernetes/pki/ca.pem \\ --cluster-signing-cert-file=/etc/kubernetes/pki/ca.pem \\ --cluster-signing-key-file=/etc/kubernetes/pki/ca-key.pem \\ --service-account-private-key-file=/etc/kubernetes/pki/sa.key \\ --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig \\ --feature-gates=LegacyServiceAccountTokenNoAutoGeneration=false \\ --leader-elect=true \\ --use-service-account-credentials=true \\ --node-monitor-grace-period=40s \\ --node-monitor-period=5s \\ --pod-eviction-timeout=2m0s \\ --controllers=*,bootstrapsigner,tokencleaner \\ --allocate-node-cidrs=true \\ --cluster-cidr=172.16.0.0/12 \\ --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem \\ --node-cidr-mask-size=24 Restart=always RestartSec=10s [Install] WantedBy=multi-user.target EOF # 启动 systemctl daemon-reload \u0026amp;\u0026amp; systemctl enable --now kube-controller-manager Scheduler(Master)\n所有Master节点配置kube-scheduler服务，配置都一样\ncat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /usr/lib/systemd/system/kube-scheduler.service [Unit] Description=Kubernetes Scheduler Documentation=https://github.com/kubernetes/kubernetes After=network.target [Service] ExecStart=/usr/local/bin/kube-scheduler \\ --v=2 \\ --leader-elect=true \\ --authentication-kubeconfig=/etc/kubernetes/scheduler.kubeconfig \\ --authorization-kubeconfig=/etc/kubernetes/scheduler.kubeconfig \\ --kubeconfig=/etc/kubernetes/scheduler.kubeconfig Restart=always RestartSec=10s [Install] WantedBy=multi-user.target EOF # 启动 systemctl daemon-reload \u0026amp;\u0026amp; systemctl enable --now kube-scheduler 确认集群组件状态\n# 所有Master节点均检查，-l参数输出完整信息 # 如果有E开头的报错，需要排查解决一下，常见问题是IP冲突、证书错误 # W告警可暂时忽略 systemctl status kube-apiserver -l systemctl status kube-controller-manager -l systemctl status kube-scheduler -l 配置TLS Bootstrapping 只需在master01节点配置，TLS Bootstrapping的官方说明请见：TLS Bootstrapping\ncd /root/k8s-ha-install/bootstrap # 查看一下secret，name后要和token-id一致，token-id.token-secret和集群设置--token=对应上 [root@k8s-master01 bootstrap]# cat bootstrap.secret.yaml apiVersion: v1 kind: Secret metadata: name: bootstrap-token-c8ad9c namespace: kube-system type: bootstrap.kubernetes.io/token stringData: description: \u0026#34;The default bootstrap token generated by \u0026#39;kubelet \u0026#39;.\u0026#34; token-id: c8ad9c token-secret: 2e4d610cf3e7426e ... kubectl config set-cluster kubernetes --certificate-authority=/etc/kubernetes/pki/ca.pem --embed-certs=true --server=https://192.168.43.200:8443 --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig kubectl config set-credentials tls-bootstrap-token-user --token=c8ad9c.2e4d610cf3e7426e --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig kubectl config set-context tls-bootstrap-token-user@kubernetes --cluster=kubernetes --user=tls-bootstrap-token-user --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig kubectl config use-context tls-bootstrap-token-user@kubernetes --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig # 创建配置目录，拷贝admin.kubeconfig文件为config，授权kubectl，使得当前用户可以使用kubectl创建资源 # 其他master节点如果需要使用kubectl命令创建资源，也可以拷贝文件过去 # 下面是没有授权的情况 # [root@k8s-master01 bootstrap]# kubectl get cs # The connection to the server localhost:8080 was refused - did you specify the right host or port? mkdir -p /root/.kube;cp /etc/kubernetes/admin.kubeconfig /root/.kube/config # 授权后查看集群状态 [root@k8s-master01 bootstrap]# kubectl get cs Warning: v1 ComponentStatus is deprecated in v1.19+ NAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy {\u0026#34;health\u0026#34;:\u0026#34;true\u0026#34;,\u0026#34;reason\u0026#34;:\u0026#34;\u0026#34;} etcd-1 Healthy {\u0026#34;health\u0026#34;:\u0026#34;true\u0026#34;,\u0026#34;reason\u0026#34;:\u0026#34;\u0026#34;} etcd-2 Healthy {\u0026#34;health\u0026#34;:\u0026#34;true\u0026#34;,\u0026#34;reason\u0026#34;:\u0026#34;\u0026#34;} # 创建bootstrap-secret [root@k8s-master01 bootstrap]# kubectl apply -f bootstrap.secret.yaml secret/bootstrap-token-c8ad9c created clusterrolebinding.rbac.authorization.k8s.io/kubelet-bootstrap created clusterrolebinding.rbac.authorization.k8s.io/node-autoapprove-bootstrap created clusterrolebinding.rbac.authorization.k8s.io/node-autoapprove-certificate-rotation created clusterrole.rbac.authorization.k8s.io/system:kube-apiserver-to-kubelet created clusterrolebinding.rbac.authorization.k8s.io/system:kube-apiserver created 配置Kubelet # 从master01拷贝证书文件到其他节点 cd /etc/kubernetes/ for i in k8s-master02 k8s-master03 k8s-node01 k8s-node02; do ssh $i mkdir -p /etc/kubernetes/pki for FILE in pki/ca.pem pki/ca-key.pem pki/front-proxy-ca.pem bootstrap-kubelet.kubeconfig; do scp /etc/kubernetes/$FILE $i:/etc/kubernetes/${FILE} done done # 所有节点配置kubelet服务 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /usr/lib/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kubelet Restart=always StartLimitInterval=0 RestartSec=10 [Install] WantedBy=multi-user.target EOF # Runtime为Containerd，kubelet服务的配置文件 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/systemd/system/kubelet.service.d/10-kubelet.conf [Service] Environment=\u0026#34;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig --kubeconfig=/etc/kubernetes/kubelet.kubeconfig\u0026#34; Environment=\u0026#34;KUBELET_SYSTEM_ARGS=--container-runtime=remote --runtime-request-timeout=15m --container-runtime-endpoint=unix:///run/containerd/containerd.sock\u0026#34; Environment=\u0026#34;KUBELET_CONFIG_ARGS=--config=/etc/kubernetes/kubelet-conf.yml\u0026#34; Environment=\u0026#34;KUBELET_EXTRA_ARGS=--node-labels=node.kubernetes.io/node=\u0026#39;\u0026#39; \u0026#34; ExecStart= ExecStart=/usr/local/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_SYSTEM_ARGS $KUBELET_EXTRA_ARGS EOF # 创建kubelet配置文件，对应上面Environment配置KUBELET_CONFIG_ARGS # clusterDNS配置为service网段的第十个地址 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/kubernetes/kubelet-conf.yml apiVersion: kubelet.config.k8s.io/v1beta1 kind: KubeletConfiguration address: 0.0.0.0 port: 10250 readOnlyPort: 10255 authentication: anonymous: enabled: false webhook: cacheTTL: 2m0s enabled: true x509: clientCAFile: /etc/kubernetes/pki/ca.pem authorization: mode: Webhook webhook: cacheAuthorizedTTL: 5m0s cacheUnauthorizedTTL: 30s cgroupDriver: systemd cgroupsPerQOS: true clusterDNS: - 10.96.0.10 clusterDomain: cluster.local containerLogMaxFiles: 5 containerLogMaxSize: 10Mi contentType: application/vnd.kubernetes.protobuf cpuCFSQuota: true cpuManagerPolicy: none cpuManagerReconcilePeriod: 10s enableControllerAttachDetach: true enableDebuggingHandlers: true enforceNodeAllocatable: - pods eventBurst: 10 eventRecordQPS: 5 evictionHard: imagefs.available: 15% memory.available: 100Mi nodefs.available: 10% nodefs.inodesFree: 5% evictionPressureTransitionPeriod: 5m0s failSwapOn: true fileCheckFrequency: 20s hairpinMode: promiscuous-bridge healthzBindAddress: 127.0.0.1 healthzPort: 10248 httpCheckFrequency: 20s imageGCHighThresholdPercent: 85 imageGCLowThresholdPercent: 80 imageMinimumGCAge: 2m0s iptablesDropBit: 15 iptablesMasqueradeBit: 14 kubeAPIBurst: 10 kubeAPIQPS: 5 makeIPTablesUtilChains: true maxOpenFiles: 1000000 maxPods: 110 nodeStatusUpdateFrequency: 10s oomScoreAdj: -999 podPidsLimit: -1 registryBurst: 10 registryPullQPS: 5 resolvConf: /etc/resolv.conf rotateCertificates: true runtimeRequestTimeout: 2m0s serializeImagePulls: true staticPodPath: /etc/kubernetes/manifests streamingConnectionIdleTimeout: 4h0m0s syncFrequency: 1m0s volumeStatsAggPeriod: 1m0s EOF # 启动服务 systemctl daemon-reload \u0026amp;\u0026amp; systemctl enable --now kubelet # 以k8s-master01为例查看运行日志和状态，只有CNI一处报错表示配置正确，后面CNI配置好后就会正常 tail -f /var/log/messages ... Apr 21 16:15:56 k8s-master01 kubelet: E0421 16:15:56.608747 7169 kubelet.go:2475] \u0026#34;Container runtime network not ready\u0026#34; networkReady=\u0026#34;NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\u0026#34; ... [root@k8s-master01 ~]# systemctl status kubelet ● kubelet.service - Kubernetes Kubelet Loaded: loaded (/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled) Drop-In: /etc/systemd/system/kubelet.service.d └─10-kubelet.conf Active: active (running) since 五 2023-04-21 16:09:00 CST; 6min ago Docs: https://github.com/kubernetes/kubernetes Main PID: 7169 (kubelet) Tasks: 11 Memory: 40.6M CGroup: /system.slice/kubelet.service └─7169 /usr/local/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig --kubeconfig=/etc/kuberne... 4月 21 16:14:26 k8s-master01 kubelet[7169]: E0421 16:14:26.578931 7169 kubelet.go:2475] \u0026#34;Container runtime network not read...alized\u0026#34; 4月 21 16:14:31 k8s-master01 kubelet[7169]: E0421 16:14:31.580077 7169 kubelet.go:2475] \u0026#34;Container runtime network not read...alized\u0026#34; .... # 并且此时节点状态应该是可查询且处于NotReady，CNI配置后就会ready [root@k8s-master01 ~]# kubectl get node NAME STATUS ROLES AGE VERSION k8s-master01 NotReady \u0026lt;none\u0026gt; 7m12s v1.26.4 k8s-master02 NotReady \u0026lt;none\u0026gt; 7m11s v1.26.4 k8s-master03 NotReady \u0026lt;none\u0026gt; 7m10s v1.26.4 k8s-node01 NotReady \u0026lt;none\u0026gt; 7m24s v1.26.4 k8s-node02 NotReady \u0026lt;none\u0026gt; 7m12s v1.26.4 配置Kube-proxy # 只需在k8s-master01上执行 cd /root/k8s-ha-install kubectl -n kube-system create serviceaccount kube-proxy kubectl create clusterrolebinding system:kube-proxy --clusterrole system:node-proxier --serviceaccount kube-system:kube-proxy SECRET=$(kubectl -n kube-system get sa/kube-proxy \\ --output=jsonpath=\u0026#39;{.secrets[0].name}\u0026#39;) JWT_TOKEN=$(kubectl -n kube-system get secret/$SECRET \\ --output=jsonpath=\u0026#39;{.data.token}\u0026#39; | base64 -d) PKI_DIR=/etc/kubernetes/pki K8S_DIR=/etc/kubernetes kubectl config set-cluster kubernetes --certificate-authority=/etc/kubernetes/pki/ca.pem --embed-certs=true --server=https://192.168.43.200:8443 --kubeconfig=${K8S_DIR}/kube-proxy.kubeconfig kubectl config set-credentials kubernetes --token=${JWT_TOKEN} --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig kubectl config set-context kubernetes --cluster=kubernetes --user=kubernetes --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig kubectl config use-context kubernetes --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig # 从k8s-master01将kubeconfig拷贝到其他节点 for i in k8s-master02 k8s-master03 k8s-node01 k8s-node02; do scp /etc/kubernetes/kube-proxy.kubeconfig $i:/etc/kubernetes/kube-proxy.kubeconfig done # 所有节点配置kube-proxy服务 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /usr/lib/systemd/system/kube-proxy.service [Unit] Description=Kubernetes Kube Proxy Documentation=https://github.com/kubernetes/kubernetes After=network.target [Service] ExecStart=/usr/local/bin/kube-proxy \\ --config=/etc/kubernetes/kube-proxy.yaml \\ --v=2 Restart=always RestartSec=10s [Install] WantedBy=multi-user.target EOF # 所有节点配置kube-proxy.yaml,clusterCIDR为pod网段 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/kubernetes/kube-proxy.yaml apiVersion: kubeproxy.config.k8s.io/v1alpha1 bindAddress: 0.0.0.0 clientConnection: acceptContentTypes: \u0026#34;\u0026#34; burst: 10 contentType: application/vnd.kubernetes.protobuf kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig qps: 5 clusterCIDR: 172.16.0.0/12 configSyncPeriod: 15m0s conntrack: max: null maxPerCore: 32768 min: 131072 tcpCloseWaitTimeout: 1h0m0s tcpEstablishedTimeout: 24h0m0s enableProfiling: false healthzBindAddress: 0.0.0.0:10256 hostnameOverride: \u0026#34;\u0026#34; iptables: masqueradeAll: false masqueradeBit: 14 minSyncPeriod: 0s syncPeriod: 30s ipvs: masqueradeAll: true minSyncPeriod: 5s scheduler: \u0026#34;rr\u0026#34; syncPeriod: 30s kind: KubeProxyConfiguration metricsBindAddress: 127.0.0.1:10249 mode: \u0026#34;ipvs\u0026#34; nodePortAddresses: null oomScoreAdj: -999 portRange: \u0026#34;\u0026#34; udpIdleTimeout: 250ms EOF # 启动 systemctl daemon-reload \u0026amp;\u0026amp; systemctl enable --now kube-proxy 配置Calico # k8s-master01上执行，更改calico中Pod网段为自己的 cd /root/k8s-ha-install/calico/ sed -i \u0026#34;s#POD_CIDR#172.16.0.0/12#g\u0026#34; calico.yaml # 检查是否更改成功 [root@k8s-master01 calico]# grep \u0026#34;IPV4POOL_CIDR\u0026#34; calico.yaml -A 1 - name: CALICO_IPV4POOL_CIDR value: \u0026#34;172.16.0.0/12\u0026#34; # 应用calico [root@k8s-master01 calico]# kubectl apply -f calico.yaml poddisruptionbudget.policy/calico-kube-controllers created poddisruptionbudget.policy/calico-typha created serviceaccount/calico-kube-controllers created serviceaccount/calico-node created configmap/calico-config created customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created clusterrole.rbac.authorization.k8s.io/calico-node created clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created clusterrolebinding.rbac.authorization.k8s.io/calico-node created service/calico-typha created daemonset.apps/calico-node created deployment.apps/calico-kube-controllers created deployment.apps/calico-typha created # （中间状态）在calico生效过程中查看各节点污点状态，发现均有不可调度污点，不必管他，等待calico生效即可，这种污点是kubernetes集群保护机制，在节点处于not ready状态时，节点不可调度 [root@k8s-master01 calico]# kubectl describe node| grep Taints: Taints: node.kubernetes.io/not-ready:NoSchedule Taints: node.kubernetes.io/not-ready:NoSchedule Taints: node.kubernetes.io/not-ready:NoSchedule Taints: node.kubernetes.io/not-ready:NoSchedule Taints: node.kubernetes.io/not-ready:NoSchedule # calico Pod处于Running后集群网络将建立，node将处于Ready状态，calico建立网络取决于电脑性能（硬件和网络环境），一般几分钟即可完成，性能差的可能会花费更长时间 [root@k8s-master01 calico]# kubectl get po -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-6bd6b69df9-5nwkl 1/1 Running 0 10m kube-system calico-node-8cvcd 1/1 Running 0 10m kube-system calico-node-96mx8 1/1 Running 1 (36s ago) 10m kube-system calico-node-f49rb 1/1 Running 0 10m kube-system calico-node-rgs7f 1/1 Running 0 10m kube-system calico-node-vzrls 1/1 Running 0 10m kube-system calico-typha-77fc8866f5-h9bsj 1/1 Running 0 10m # 查看集群状态和资源 [root@k8s-master01 calico]# kubectl get node NAME STATUS ROLES AGE VERSION k8s-master01 Ready \u0026lt;none\u0026gt; 23m v1.26.4 k8s-master02 Ready \u0026lt;none\u0026gt; 23m v1.26.4 k8s-master03 Ready \u0026lt;none\u0026gt; 23m v1.26.4 k8s-node01 Ready \u0026lt;none\u0026gt; 23m v1.26.4 k8s-node02 Ready \u0026lt;none\u0026gt; 23m v1.26.4 配置CoreDNS 在k8s-master01操作 # 如果更改了k8s service的网段需要将coredns的serviceIP改成k8s service网段的第十个IP cd /root/k8s-ha-install/ COREDNS_SERVICE_IP=`kubectl get svc | grep kubernetes | awk \u0026#39;{print $3}\u0026#39;`0 sed -i \u0026#34;s#KUBEDNS_SERVICE_IP#${COREDNS_SERVICE_IP}#g\u0026#34; CoreDNS/coredns.yaml kubectl apply -f CoreDNS/coredns.yaml [root@k8s-master01 k8s-ha-install]# kubectl get pod -A | grep coredns kube-system coredns-5db5696c7-tsqts 1/1 Running 0 80s 配置Metrics 在k8s-master01操作 在新版的Kubernetes中系统资源的采集均使用Metrics-server，可以通过Metrics采集节点和Pod的内存、磁盘、CPU和网络的使用率。\ncd /root/k8s-ha-install/metrics-server kubectl create -f . # 等待metrics-server部署好后，便可使用 [root@k8s-master01 metrics-server]# kubectl top nodes NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% k8s-master01 186m 9% 1094Mi 58% k8s-master02 192m 9% 1176Mi 62% k8s-master03 176m 8% 1123Mi 60% k8s-node01 72m 3% 463Mi 24% k8s-node02 66m 3% 472Mi 25% 配置Dashboard Dashboard是一个展示Kubernetes集群资源和Pod日志，甚至可以执行容器命令的web控制台。\n# 直接部署即可 cd /root/k8s-ha-install/dashboard/ kubectl apply -f . # 查看dashboard端口，默认是NodePort模式，访问集群内任意节点的32486端口即可 [root@k8s-master01 ~]# kubectl get svc -A | grep dash kubernetes-dashboard dashboard-metrics-scraper ClusterIP 10.111.39.8 \u0026lt;none\u0026gt; 8000/TCP 12m kubernetes-dashboard kubernetes-dashboard NodePort 10.98.143.126 \u0026lt;none\u0026gt; 443:32486/TCP 12m 访问dashboard：https://集群内任意节点IP:32486\n发现提示隐私设置错误的问题，解决方法是在Chrome浏览器启动参数加入--test-type --ignore-certificate-errors，再访问就没有这个提示\n# 获取登陆令牌（token） [root@k8s-master01 dashboard]# kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk \u0026#39;{print $1}\u0026#39;) Name: admin-user-token-cj2kt Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: admin-user kubernetes.io/service-account.uid: c86fbde2-36ea-4dd3-94fd-8ce8012fdf22 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1411 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6ImFPeklobHBkNVRzZzZYVF9nbG5BMTgwOHdvMUNkV2FGbW1wdmUzZzdJRXcifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLWNqMmt0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJjODZmYmRlMi0zNmVhLTRkZDMtOTRmZC04Y2U4MDEyZmRmMjIiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.YgxIsaxR-hfyT9YLGdszggQ0Rvoc4SvyqswgvHz2ySc27q8lAQ7EJxhze3bhrdTL79z3J30T6vmuA5Be3kq_c2r42r2Iy-pC92t8xTISlPWEl7JfSg8GSbX2-UxUM_wqCmbMO3RWGYW5FpzrJ2pSVaeIGlu2JmYTugtS50LCFi87DmP2tDAKLQfh1NRylpEPI1AJPbl41E2wyDBUlS86YF_glUnQxyDCyrf2wJ2Akjqe7If2b9tAXHbSZBcQJFGHENymYhdBW6QObmTRxUsaOX9wdTToFcoHr-FaE4LcP9KuXhxP-gNNyVN7HN0k0WbhAp6CBIoypFCVLIN96EvNIg 选择ALL namespace，可以查看如下图 集群优化(可选) Docker可在/etc/docker/daemon.json自定义优化配置，所有配置可见：官方docker configuration，docker常用优化配置见下方注释说明。\n# （！！！如果使用docker作为Runtime的话）优化docker配置 # /etc/docker/daemon.json文件，按需配置，不需要全部都照抄，使用时删除注释，因为JSON文件不支持注释 { \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;], # cgroups驱动 \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://ynirk4k5.mirror.aliyuncs.com\u0026#34;], # 镜像加速器地址 \u0026#34;allow-nondistributable-artifacts\u0026#34;: [], \u0026#34;api-cors-header\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;authorization-plugins\u0026#34;: [], \u0026#34;bip\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;bridge\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;cgroup-parent\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;cluster-advertise\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;cluster-store\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;cluster-store-opts\u0026#34;: {}, \u0026#34;containerd\u0026#34;: \u0026#34;/run/containerd/containerd.sock\u0026#34;, \u0026#34;containerd-namespace\u0026#34;: \u0026#34;docker\u0026#34;, \u0026#34;data-root\u0026#34;: \u0026#34;\u0026#34;, # 数据根目录，大量docker镜像可能会占用较大存储，可以设置系统盘外的挂载盘 \u0026#34;debug\u0026#34;: true, \u0026#34;default-address-pools\u0026#34;: [ { \u0026#34;base\u0026#34;: \u0026#34;172.30.0.0/16\u0026#34;, \u0026#34;size\u0026#34;: 24 }, { \u0026#34;base\u0026#34;: \u0026#34;172.31.0.0/16\u0026#34;, \u0026#34;size\u0026#34;: 24 } ], \u0026#34;default-cgroupns-mode\u0026#34;: \u0026#34;private\u0026#34;, \u0026#34;default-gateway\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;default-gateway-v6\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;default-runtime\u0026#34;: \u0026#34;runc\u0026#34;, \u0026#34;default-shm-size\u0026#34;: \u0026#34;64M\u0026#34;, \u0026#34;default-ulimits\u0026#34;: { \u0026#34;nofile\u0026#34;: { \u0026#34;Hard\u0026#34;: 64000, \u0026#34;Name\u0026#34;: \u0026#34;nofile\u0026#34;, \u0026#34;Soft\u0026#34;: 64000 } }, \u0026#34;dns\u0026#34;: [], \u0026#34;dns-opts\u0026#34;: [], \u0026#34;dns-search\u0026#34;: [], \u0026#34;exec-root\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;experimental\u0026#34;: false, \u0026#34;features\u0026#34;: {}, \u0026#34;fixed-cidr\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;fixed-cidr-v6\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;group\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;hosts\u0026#34;: [], \u0026#34;icc\u0026#34;: false, \u0026#34;init\u0026#34;: false, \u0026#34;init-path\u0026#34;: \u0026#34;/usr/libexec/docker-init\u0026#34;, \u0026#34;insecure-registries\u0026#34;: [], \u0026#34;ip\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;ip-forward\u0026#34;: false, \u0026#34;ip-masq\u0026#34;: false, \u0026#34;iptables\u0026#34;: false, \u0026#34;ip6tables\u0026#34;: false, \u0026#34;ipv6\u0026#34;: false, \u0026#34;labels\u0026#34;: [], \u0026#34;live-restore\u0026#34;: true, # docker进程宕机时容器依然保持存活 \u0026#34;log-driver\u0026#34;: \u0026#34;json-file\u0026#34;, # 日志格式 \u0026#34;log-level\u0026#34;: \u0026#34;\u0026#34;, # 日志级别 \u0026#34;log-opts\u0026#34;: { # 日志优化 \u0026#34;cache-disabled\u0026#34;: \u0026#34;false\u0026#34;, \u0026#34;cache-max-file\u0026#34;: \u0026#34;5\u0026#34;, \u0026#34;cache-max-size\u0026#34;: \u0026#34;20m\u0026#34;, \u0026#34;cache-compress\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;env\u0026#34;: \u0026#34;os,customer\u0026#34;, \u0026#34;labels\u0026#34;: \u0026#34;somelabel\u0026#34;, \u0026#34;max-file\u0026#34;: \u0026#34;5\u0026#34;, # 最大日志数量 \u0026#34;max-size\u0026#34;: \u0026#34;10m\u0026#34; # 保存的最大日志大小 }, \u0026#34;max-concurrent-downloads\u0026#34;: 3, # pull下载并发数 \u0026#34;max-concurrent-uploads\u0026#34;: 5, # push上传并发数 \u0026#34;max-download-attempts\u0026#34;: 5, \u0026#34;mtu\u0026#34;: 0, \u0026#34;no-new-privileges\u0026#34;: false, \u0026#34;node-generic-resources\u0026#34;: [ \u0026#34;NVIDIA-GPU=UUID1\u0026#34;, \u0026#34;NVIDIA-GPU=UUID2\u0026#34; ], \u0026#34;oom-score-adjust\u0026#34;: -500, \u0026#34;pidfile\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;raw-logs\u0026#34;: false, \u0026#34;runtimes\u0026#34;: { \u0026#34;cc-runtime\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/usr/bin/cc-runtime\u0026#34; }, \u0026#34;custom\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/usr/local/bin/my-runc-replacement\u0026#34;, \u0026#34;runtimeArgs\u0026#34;: [ \u0026#34;--debug\u0026#34; ] } }, \u0026#34;seccomp-profile\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;selinux-enabled\u0026#34;: false, \u0026#34;shutdown-timeout\u0026#34;: 15, \u0026#34;storage-driver\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;storage-opts\u0026#34;: [], \u0026#34;swarm-default-advertise-addr\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;tls\u0026#34;: true, \u0026#34;tlscacert\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;tlscert\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;tlskey\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;tlsverify\u0026#34;: true, \u0026#34;userland-proxy\u0026#34;: false, \u0026#34;userland-proxy-path\u0026#34;: \u0026#34;/usr/libexec/docker-proxy\u0026#34;, \u0026#34;userns-remap\u0026#34;: \u0026#34;\u0026#34; } # 无注释版 { \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;], \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://ynirk4k5.mirror.aliyuncs.com\u0026#34;], \u0026#34;containerd-namespace\u0026#34;: \u0026#34;docker\u0026#34;, \u0026#34;data-root\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;debug\u0026#34;: true, \u0026#34;default-cgroupns-mode\u0026#34;: \u0026#34;private\u0026#34;, \u0026#34;default-gateway\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;default-gateway-v6\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;default-runtime\u0026#34;: \u0026#34;runc\u0026#34;, \u0026#34;default-shm-size\u0026#34;: \u0026#34;64M\u0026#34;, \u0026#34;default-ulimits\u0026#34;: { \u0026#34;nofile\u0026#34;: { \u0026#34;Hard\u0026#34;: 64000, \u0026#34;Name\u0026#34;: \u0026#34;nofile\u0026#34;, \u0026#34;Soft\u0026#34;: 64000 } }, \u0026#34;init-path\u0026#34;: \u0026#34;/usr/libexec/docker-init\u0026#34;, \u0026#34;live-restore\u0026#34;: true, \u0026#34;log-driver\u0026#34;: \u0026#34;json-file\u0026#34;, \u0026#34;log-level\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;log-opts\u0026#34;: { \u0026#34;cache-disabled\u0026#34;: \u0026#34;false\u0026#34;, \u0026#34;cache-max-file\u0026#34;: \u0026#34;5\u0026#34;, \u0026#34;cache-max-size\u0026#34;: \u0026#34;20m\u0026#34;, \u0026#34;cache-compress\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;env\u0026#34;: \u0026#34;os,customer\u0026#34;, \u0026#34;labels\u0026#34;: \u0026#34;somelabel\u0026#34;, \u0026#34;max-file\u0026#34;: \u0026#34;5\u0026#34;, \u0026#34;max-size\u0026#34;: \u0026#34;10m\u0026#34; }, \u0026#34;max-concurrent-downloads\u0026#34;: 3, \u0026#34;max-concurrent-uploads\u0026#34;: 5, \u0026#34;max-download-attempts\u0026#34;: 5, \u0026#34;mtu\u0026#34;: 0, \u0026#34;no-new-privileges\u0026#34;: false, \u0026#34;oom-score-adjust\u0026#34;: -500, \u0026#34;pidfile\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;raw-logs\u0026#34;: false, \u0026#34;runtimes\u0026#34;: { \u0026#34;cc-runtime\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/usr/bin/cc-runtime\u0026#34; }, \u0026#34;custom\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/usr/local/bin/my-runc-replacement\u0026#34;, \u0026#34;runtimeArgs\u0026#34;: [ \u0026#34;--debug\u0026#34; ] } }, \u0026#34;seccomp-profile\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;selinux-enabled\u0026#34;: false, \u0026#34;shutdown-timeout\u0026#34;: 15, \u0026#34;storage-driver\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;storage-opts\u0026#34;: [], \u0026#34;swarm-default-advertise-addr\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;userland-proxy-path\u0026#34;: \u0026#34;/usr/libexec/docker-proxy\u0026#34;, \u0026#34;userns-remap\u0026#34;: \u0026#34;\u0026#34; } # 设置证书有效期 [root@k8s-master01 ~]# vim /usr/lib/systemd/system/kube-controller-manager.service ... # 加入下面配置 --experimental-cluster-signing-duration=876000h0m0s ... [root@k8s-master01 ~]# systemctl daemon-reload [root@k8s-master01 ~]# systemctl restart kube-controller-manager # kubelet优化加密算法，默认的算法容易被漏洞扫描；增长镜像下载周期，避免有些大镜像未下载完成就被动死亡退出 # --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 # --image-pull-progress-deadline=30m [root@k8s-master01 ~]# vim /etc/systemd/system/kubelet.service.d/10-kubelet.conf ... # 下面这行中KUBELET_EXTRA_ARGS=后加入配置 Environment=\u0026#34;KUBELET_EXTRA_ARGS=--tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 --image-pull-progress-deadline=30m\u0026#34; ... # 集群配置优化，详见https://kubernetes.io/zh/docs/tasks/administer-cluster/reserve-compute-resources/ [root@k8s-master01 ~]# vim /etc/kubernetes/kubelet-conf.yml # 文件中添加如下配置 rotateServerCertificates: true allowedUnsafeSysctls: # 允许在修改内核参数，此操作按情况选择，用不到就不用设置 - \u0026#34;net.core*\u0026#34; - \u0026#34;net.ipv4.*\u0026#34; kubeReserved: # 为Kubernetes集群守护进程组件预留资源，例如：kubelet、Runtime等 cpu: \u0026#34;100m\u0026#34; memory: 100Mi ephemeral-storage: 1Gi systemReserved: # 为系统守护进程预留资源，例如：sshd、cron等 cpu: \u0026#34;100m\u0026#34; memory: 100Mi ephemeral-storage: 1Gi # 为集群节点打标签，删除标签把 = 换成 - 即可 kubectl label nodes k8s-node01 node-role.kubernetes.io/node= kubectl label nodes k8s-node02 node-role.kubernetes.io/node= kubectl label nodes k8s-master01 node-role.kubernetes.io/master= kubectl label nodes k8s-master02 node-role.kubernetes.io/master= kubectl label nodes k8s-master03 node-role.kubernetes.io/master= # 添加标签后查看集群状态 [root@k8s-master01 ~]# kubectl get node NAME STATUS ROLES AGE VERSION k8s-master01 Ready master 35m v1.26.4 k8s-master02 Ready master 35m v1.26.4 k8s-master03 Ready master 35m v1.26.4 k8s-node01 Ready node 35m v1.26.4 k8s-node02 Ready node 35m v1.26.4 生产环境建议ETCD集群和Kubernetes集群分离，而且使用高性能数据盘存储数据，根据情况决定是否将Master节点也作为Pod调度节点。\n测试集群 # 测试namespace kubectl get namespace kubectl create namespace test kubectl get namespace kubectl delete namespace test # 创建nginx实例并开放端口 kubectl create deployment nginx --image=nginx kubectl expose deployment nginx --port=80 --type=NodePort # 查看调度状态和端口号 [root@k8s-master01 ~]# kubectl get po,svc -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/nginx-748c667d99-dmtn6 1/1 Running 0 9m55s 172.25.244.194 k8s-master01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 69m \u0026lt;none\u0026gt; service/nginx NodePort 10.110.18.105 \u0026lt;none\u0026gt; 80:31687/TCP 9s app=nginx 在浏览器输入http://任意节点IP:31687/ 访问nginx，访问结果如图\n至此，基于二进制方式的Kubernetes高可用集群部署并验证成功。\n","permalink":"https://deemoprobe.github.io/posts/tech/kubernetes/kubernetesbinaryinstallation-ha/","summary":"环境说明 宿主机系统：Windows 10 虚拟机版本：VMware® Workstation 16 Pro IOS镜像版本：CentOS Linux release 7.9.2009 Kubernetes版本：1.26.4 Runtime：Containerd v1.6.20 Etcd版本：3.5.6 集群操作用户：root 更新时间：2023-04-21 CentOS7安装请参考博","title":"KubernetesBinaryInstallation HA"},{"content":"环境说明 宿主机系统：Windows 10 虚拟机版本：VMware® Workstation 16 Pro IOS镜像版本：CentOS Linux release 7.9.2009 集群操作用户：root Kubernetes版本：1.23.0 Etcd版本：3.5.1 Runtime：Docker 20.10 CentOS7安装请参考博客文章：LINUX之VMWARE WORKSTATION安装CENTOS-7\n资源分配 网段划分 Kubernetes集群需要规划三个网段：\n宿主机网段：Kubernetes集群节点的网段 Pod网段：集群内Pod的网段，相当于容器的IP Service网段：集群内服务发现使用的网段，service用于集群容器通信 生产环境根据申请到的IP资源进行分配即可，原则是三个网段不允许有重合IP。IP网段计算可以参考：在线IP地址计算。本文虚拟机练习环境IP地址网段分配如下：\n宿主机网段：192.168.43.1/24 Pod网段：172.16.0.0/12 Service：10.96.0.0/12 节点分配 采用3管理节点2工作节点的高可用Kubernetes集群模式：\nk8s-master01/k8s-master02/k8s-master03 集群的Master节点 三个master节点同时做etcd集群 k8s-node01/k8s-node02 集群的Node节点 k8s-master-vip做高可用k8s-master01~03的VIP，不占用物理资源 主机节点名称 IP CPU核心数 内存大小 磁盘大小 k8s-master-vip 192.168.43.182 / / / k8s-master01 192.168.43.183 2 2G 40G k8s-master02 192.168.43.184 2 2G 40G k8s-master03 192.168.43.185 2 2G 40G k8s-node01 192.168.43.186 2 2G 40G k8s-node02 192.168.43.187 2 2G 40G 操作步骤 标题后小括号注释表明操作范围：\nALL 所有节点（k8s-master01/k8s-master02/k8s-master03/k8s-node01/k9s-node02）执行 Master 只需要在master节点（k8s-master01/k8s-master02/k8s-master03）执行 Node 只需要在node节点（k8s-node01/k8s-node02）执行 已标注的个别命令只需要在某一台机器执行，会在操作前说明 未标注的会在操作时说明 使用cat \u0026lt;\u0026lt; \u0026quot;EOF\u0026quot; \u0026gt;\u0026gt; file或cat \u0026gt;\u0026gt; file \u0026lt;\u0026lt; \u0026quot;EOF\u0026quot;添加文件内容注意cat后面的EOF一定要加上双引号（标准输入的），否则不会保留输入时的缩进格式而且会直接解析输入时的变量，进而造成文件可读性差甚至不可用；同时注意文件的\u0026gt;重写与\u0026gt;\u0026gt;追加。虽然单独转义输入时的变量也能避免变量被解析，但是不推荐，漏转义会造成不必要的麻烦。\n准备工作(ALL) 添加主机信息、关闭防火墙、关闭swap、关闭SELinux、dnsmasq、NetworkManager\n# 添加主机信息 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt;\u0026gt; /etc/hosts 192.168.43.182 k8s-master-vip 192.168.43.183 k8s-master01 192.168.43.184 k8s-master02 192.168.43.185 k8s-master03 192.168.43.186 k8s-node01 192.168.43.187 k8s-node02 EOF # 关闭防火墙、dnsmasq、NetworkManager，--now参数表示关闭服务并移除开机自启 # 这些服务是否可以关闭视情况而定，本文是虚拟机实践，没有用到这些服务 systemctl disable --now firewalld systemctl disable --now dnsmasq systemctl disable --now NetworkManager # 关闭swap，并注释fstab文件swap所在行 swapoff -a sed -i \u0026#39;/swap/s/^\\(.*\\)$/#\\1/g\u0026#39; /etc/fstab # 关闭SELinux，并更改selinux配置文件 setenforce 0 sed -i \u0026#34;s/=enforcing/=disabled/g\u0026#34; /etc/selinux/config 值得注意的是/etc/sysconfig/selinux文件是/etc/selinux/config文件的软连接，用sed -i命令修改软连接文件会破坏软连接属性，将/etc/sysconfig/selinux变为一个独立的文件，即使该文件被修改了，但源文件/etc/selinux/config配置是没变的。此外，使用vim等编辑器编辑源文件或链接文件（编辑模式不会修改文件属性）修改也可以。软链接原理可参考博客：LINUX之INODE详解\n必要操作(ALL) # 默认的yum源太慢，更新为阿里源，同时用sed命令删除文件中不需要的两个URL的行 curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo sed -i -e \u0026#39;/mirrors.cloud.aliyuncs.com/d\u0026#39; -e \u0026#39;/mirrors.aliyuncs.com/d\u0026#39; /etc/yum.repos.d/CentOS-Base.repo # 安装常用工具包 yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git -y # 配置阿里Kubernetes源，如果提示gpg文件有问题，可以改为gpgcheck=0，并把后三行删除（仅限测试环境使用） cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF # 配置ntpdate，同步服务器时间 rpm -ivh http://mirrors.wlnmp.com/centos/wlnmp-release-centos.noarch.rpm yum install ntpdate -y # 同步时区和时间 ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime echo \u0026#39;Asia/Shanghai\u0026#39; \u0026gt;/etc/timezone ntpdate time2.aliyun.com # 可以加入计划任务，保证集群时钟是一致的 # /var/spool/cron/root文件也是crontab -e写入的文件 # crontab执行日志查看可用：tail -f /var/log/cron cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt;\u0026gt; /var/spool/cron/root */5 * * * * /usr/sbin/ntpdate time2.aliyun.com EOF # 保证文件句柄不会限制集群的可持续发展，配置limits ulimit -SHn 65500 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt;\u0026gt; /etc/security/limits.conf * soft nofile 65500 * hard nofile 65500 * soft nproc 65500 * hard nproc 65500 * soft memlock unlimited * hard memlock unlimited EOF # 配置免密登录，k8s-master01到其他节点 # 生成密钥对（在k8s-master01节点配置即可） ssh-keygen -t rsa # 拷贝公钥到其他节点，首次需要认证一下各个节点的root密码，以后就可以免密ssh到其他节点 for i in k8s-master02 k8s-master03 k8s-node01 k8s-node02;do ssh-copy-id -i .ssh/id_rsa.pub $i;done # 克隆二进制仓库文件（k8s-master01上操作即可） cd root;git clone https://gitee.com/deemoprobe/k8s-ha-install.git cd k8s-ha-install;git branch -a * master remotes/origin/HEAD -\u0026gt; origin/master remotes/origin/manual-installation remotes/origin/manual-installation-v1.16.x remotes/origin/manual-installation-v1.17.x remotes/origin/manual-installation-v1.18.x remotes/origin/manual-installation-v1.19.x remotes/origin/manual-installation-v1.20.x remotes/origin/manual-installation-v1.20.x-csi-hostpath remotes/origin/manual-installation-v1.21.x remotes/origin/manual-installation-v1.22.x remotes/origin/manual-installation-v1.23.x remotes/origin/master # 可以切换到需要版本的分支中获取配置文件 git checkout manual-installation-v1.22.x # 所有节点系统升级 yum update --exclude=kernel* -y 升级内核，4.17以下的内核cgroup存在内存泄漏的BUG，具体分析过程浏览器搜Kubernetes集群为什么要升级内核会有很多文章讲解\n内核备用下载（建议下载到本地后上传到服务器，尽量不用wget）：\nkernel-ml-devel-4.19.12-1.el7.elrepo.x86_64.rpm kernel-ml-4.19.12-1.el7.elrepo.x86_64.rpm # 下载4.19版本内核，如果无法下载，可以用上面提供的备用下载 cd /root wget http://193.49.22.109/elrepo/kernel/el7/x86_64/RPMS/kernel-ml-devel-4.19.12-1.el7.elrepo.x86_64.rpm wget http://193.49.22.109/elrepo/kernel/el7/x86_64/RPMS/kernel-ml-4.19.12-1.el7.elrepo.x86_64.rpm # 可以在k8s-master01节点下载后，免密传到其他节点 for i in k8s-master02 k8s-master03 k8s-node01 k8s-node02;do scp kernel-ml-* $i:/root;done # 所有节点安装内核 cd /root \u0026amp;\u0026amp; yum localinstall -y kernel-ml* # 所有节点更改内核启动顺序 grub2-set-default 0 \u0026amp;\u0026amp; grub2-mkconfig -o /etc/grub2.cfg grubby --args=\u0026#34;user_namespace.enable=1\u0026#34; --update-kernel=\u0026#34;$(grubby --default-kernel)\u0026#34; # 查看默认内核，并重启节点 grubby --default-kernel reboot # 确认内核版本 uname -a # （可选）删除老版本的内核，避免以后被升级取代默认的开机4.19内核 rpm -qa | grep kernel yum remove -y kernel-3* # 升级系统软件包（如果跳过内核升级加参数 --exclude=kernel*） yum update -y # 安装IPVS相关工具，由于IPVS在资源消耗和性能上均已明显优于iptables，所以推荐开启 # 具体原因可参考官网介绍 https://kubernetes.io/zh/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/ yum install ipvsadm ipset sysstat conntrack libseccomp -y # 加载模块，最后一条4.18及以下内核使用nf_conntrack_ipv4，4.19已改为nf_conntrack modprobe -- ip_vs modprobe -- ip_vs_rr modprobe -- ip_vs_wrr modprobe -- ip_vs_sh modprobe -- nf_conntrack # 编写参数文件 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/modules-load.d/ipvs.conf ip_vs ip_vs_lc ip_vs_wlc ip_vs_rr ip_vs_wrr ip_vs_lblc ip_vs_lblcr ip_vs_dh ip_vs_sh ip_vs_fo ip_vs_nq ip_vs_sed ip_vs_ftp ip_vs_sh nf_conntrack ip_tables ip_set xt_set ipt_set ipt_rpfilter ipt_REJECT ipip EOF # systemd-modules-load加入开机自启 systemctl enable --now systemd-modules-load # 自定义内核参数优化配置文件 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/sysctl.d/kubernetes.conf net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 fs.may_detach_mounts = 1 net.ipv4.conf.all.route_localnet = 1 vm.overcommit_memory=1 vm.panic_on_oom=0 fs.inotify.max_user_watches=89100 fs.file-max=52706963 fs.nr_open=52706963 net.netfilter.nf_conntrack_max=2310720 net.ipv4.tcp_keepalive_time = 600 net.ipv4.tcp_keepalive_probes = 3 net.ipv4.tcp_keepalive_intvl =15 net.ipv4.tcp_max_tw_buckets = 36000 net.ipv4.tcp_tw_reuse = 1 net.ipv4.tcp_max_orphans = 327680 net.ipv4.tcp_orphan_retries = 3 net.ipv4.tcp_syncookies = 1 net.ipv4.tcp_max_syn_backlog = 16384 net.ipv4.ip_conntrack_max = 65536 net.ipv4.tcp_max_syn_backlog = 16384 net.ipv4.tcp_timestamps = 0 net.core.somaxconn = 16384 EOF # 加载 sysctl --system # 重启查看IPVS模块是否依旧加载 reboot lsmod | grep -e ip_vs -e nf_conntrack 保证每台服务器中IPVS加载成功，以k8s-master01为例，如图： 部署Docker(ALL) # 卸载已存在docker，新机器的话这步可以忽略 yum remove -y docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine yum remove -y docker-ce docker-ce-cli containerd.io # 配置阿里docker源 yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo # 安装最新版本docker yum install docker-ce docker-ce-cli containerd.io -y # （可选）如果想要安装指定版本docker，先查询一下。安装指定版本，例如20.10.9-3.el7 yum list docker-ce docker-ce-cli --showduplicates | grep \u0026#34;^doc\u0026#34; | sort -r yum install docker-ce-20.10.9-3.el7 docker-ce-cli-20.10.9-3.el7 containerd.io -y # 加入开机启动并启动 systemctl enable docker systemctl start docker # 测试运行hello-world镜像并查看docker版本信息 docker run hello-world docker version # 配置阿里docker镜像加速器，阿里云(登录账号--\u0026gt;点击管理控制台--\u0026gt;搜索容器镜像服务--\u0026gt;镜像工具--\u0026gt;镜像加速器--\u0026gt;复制加速器地址) # docker文件驱动改成 systemd cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/docker/daemon.json { \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;], \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://ynirk4k5.mirror.aliyuncs.com\u0026#34;] } EOF # 重启docker systemctl restart docker # 如果启动失败,强制加载再启动试试 systemctl reset-failed docker systemctl restart docker # 查看docker配置信息 docker info docker info | grep Driver 安装kubernetes(ALL) 一般kubectl在master节点安装即可,node节点装不装均可\n# 查看可以安装的版本号 yum list kubeadm --showduplicates | sort -r # 不指定版本的话默认安装最新版本安装 yum install -y kubelet kubeadm kubectl # （可选）指定版本进行安装，如1.23.0 yum install -y kubelet-1.23.0 kubeadm-1.23.0 kubectl-1.23.0 # 配置pause镜像仓库，默认的gcr.io国内无法访问，可以使用阿里仓库 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/sysconfig/kubelet KUBELET_EXTRA_ARGS=\u0026#34;--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.5\u0026#34; EOF # 加入开机启动并启动，此处启动失败不必排查，待初始化完成kubelet会自动恢复正常 systemctl enable --now kubelet 高可用组件安装(Master) # 所有master节点安装Keepalived和haproxy yum install keepalived haproxy -y # 为所有master节点添加haproxy配置，配置都一样，检查最后三行主机名和IP地址对应上就行 mkdir /etc/haproxy cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/haproxy/haproxy.cfg global maxconn 2000 ulimit-n 16384 log 127.0.0.1 local0 err stats timeout 30s defaults log global mode http option httplog timeout connect 5000 timeout client 50000 timeout server 50000 timeout http-request 15s timeout http-keep-alive 15s frontend monitor-in bind *:33305 mode http option httplog monitor-uri /monitor frontend k8s-master bind 0.0.0.0:16443 bind 127.0.0.1:16443 mode tcp option tcplog tcp-request inspect-delay 5s default_backend k8s-master backend k8s-master mode tcp option tcplog option tcp-check balance roundrobin default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100 server k8s-master01 192.168.43.183:6443 check server k8s-master02 192.168.43.184:6443 check server k8s-master03 192.168.43.185:6443 check EOF # keepalived配置不一样，注意区分网卡名、IP地址和虚拟IP地址 # 检查服务器网卡名 ip a 或 ifconfig # k8s-master01 Keepalived配置 mkdir /etc/keepalived cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/keepalived/keepalived.conf ! Configuration File for keepalived global_defs { router_id LVS_DEVEL script_user root enable_script_security } vrrp_script chk_apiserver { script \u0026#34;/etc/keepalived/check_apiserver.sh\u0026#34; interval 5 weight -5 fall 2 rise 1 } vrrp_instance VI_1 { state MASTER interface ens33 mcast_src_ip 192.168.43.183 virtual_router_id 51 priority 101 advert_int 2 authentication { auth_type PASS auth_pass K8SHA_KA_AUTH } virtual_ipaddress { 192.168.43.182 } track_script { chk_apiserver } } EOF # k8s-master02 Keepalived配置 mkdir /etc/keepalived cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/keepalived/keepalived.conf ! Configuration File for keepalived global_defs { router_id LVS_DEVEL script_user root enable_script_security } vrrp_script chk_apiserver { script \u0026#34;/etc/keepalived/check_apiserver.sh\u0026#34; interval 5 weight -5 fall 2 rise 1 } vrrp_instance VI_1 { state BACKUP interface ens33 mcast_src_ip 192.168.43.184 virtual_router_id 51 priority 100 advert_int 2 authentication { auth_type PASS auth_pass K8SHA_KA_AUTH } virtual_ipaddress { 192.168.43.182 } track_script { chk_apiserver } } EOF # k8s-master03 Keepalived配置 mkdir /etc/keepalived cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/keepalived/keepalived.conf ! Configuration File for keepalived global_defs { router_id LVS_DEVEL script_user root enable_script_security } vrrp_script chk_apiserver { script \u0026#34;/etc/keepalived/check_apiserver.sh\u0026#34; interval 5 weight -5 fall 2 rise 1 } vrrp_instance VI_1 { state BACKUP interface ens33 mcast_src_ip 192.168.43.185 virtual_router_id 51 priority 100 advert_int 2 authentication { auth_type PASS auth_pass K8SHA_KA_AUTH } virtual_ipaddress { 192.168.43.182 } track_script { chk_apiserver } } EOF # 所有master节点配置Keepalived健康检查脚本 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/keepalived/check_apiserver.sh #!/bin/bash err=0 for k in $(seq 1 3) do check_code=$(pgrep haproxy) if [[ $check_code == \u0026#34;\u0026#34; ]]; then err=$(expr $err + 1) sleep 1 continue else err=0 break fi done if [[ $err != \u0026#34;0\u0026#34; ]]; then echo \u0026#34;systemctl stop keepalived\u0026#34; /usr/bin/systemctl stop keepalived exit 1 else exit 0 fi EOF # 赋予可执行权限 chmod +x /etc/keepalived/check_apiserver.sh # 启动haproxy和Keepalived并加入开机启动 systemctl start haproxy systemctl start keepalived systemctl enable haproxy systemctl enable keepalived # 测试一波 telnet k8s-master-vip 16443 ping k8s-master-vip 部署k8s-master(k8s-master01) 在k8s-master01节点上执行，个别步骤在所有master节点执行，已另行说明，没说明的均是在k8s-master01执行。\n# 创建初始化文件，注意版本号和IP对应上 cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; kubeadm-config.yaml apiVersion: kubeadm.k8s.io/v1beta2 bootstrapTokens: - groups: - system:bootstrappers:kubeadm:default-node-token token: 7t2weq.bjbawausm0jaxury ttl: 24h0m0s usages: - signing - authentication kind: InitConfiguration localAPIEndpoint: advertiseAddress: 192.168.43.183 # API通知地址，设置为初始化IP即可 bindPort: 6443 nodeRegistration: criSocket: /var/run/dockershim.sock # docker作为Container Runtime # criSocket: /run/containerd/containerd.sock # containerd作为Container Runtime name: k8s-master01 # 初始化节点名 taints: - effect: NoSchedule key: node-role.kubernetes.io/master --- apiServer: certSANs: - 192.168.43.182 timeoutForControlPlane: 4m0s # 初始化超时时间 apiVersion: kubeadm.k8s.io/v1beta2 certificatesDir: /etc/kubernetes/pki clusterName: kubernetes controlPlaneEndpoint: 192.168.43.182:16443 # 如果不是高可用集群，则为6443 controllerManager: {} dns: type: CoreDNS etcd: local: dataDir: /var/lib/etcd imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers # 国内地址 kind: ClusterConfiguration kubernetesVersion: v1.23.0 # 需与kubeadm version版本号保持一致 networking: dnsDomain: cluster.local podSubnet: 172.16.0.0/12 # 不可与其他IP段冲突 serviceSubnet: 10.96.0.0/12 # 不可与其他IP段冲突 scheduler: {} EOF # 更新初始化文件 kubeadm config migrate --old-config kubeadm-config.yaml --new-config new.yaml # 将new.yaml复制到其他master节点上 for i in k8s-master02 k8s-master03;do scp new.yaml $i:/root/;done # 镜像预下载，节省集群初始化的时间（这一步在所有master节点上执行） kubeadm config images pull --config /root/new.yaml # 执行结果如下 [config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.23.0 [config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.23.0 [config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.23.0 [config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.23.0 [config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.6 [config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.1-0 [config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.8.6 # master01初始化，初始化完成后，加入其他节点即可 kubeadm init --config /root/new.yaml --upload-certs # 初始化如果失败，可用下面命令清除初始化信息，然后再次尝试初始化 kubeadm reset -f ; ipvsadm --clear ; rm -rf ~/.kube # 初始化成功类似于下面输出，保存好这些信息 ... Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \u0026#34;kubectl apply -f [podnetwork].yaml\u0026#34; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of the control-plane node running the following command on each as root: kubeadm join 192.168.43.182:16443 --token 7t2weq.bjbawausm0jaxury \\ --discovery-token-ca-cert-hash sha256:5257d44118ab035adc5af89dd7d5a24ca4c31c33e1918b3453ea9aa32597121b \\ --control-plane --certificate-key 9a2e86718fceba001c96e503e9df47db3a645d4917bf783decaea9c5d0a726ed Please note that the certificate-key gives access to cluster sensitive data, keep it secret! As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use \u0026#34;kubeadm init phase upload-certs --upload-certs\u0026#34; to reload certs afterward. Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192.168.43.182:16443 --token 7t2weq.bjbawausm0jaxury \\ --discovery-token-ca-cert-hash sha256:5257d44118ab035adc5af89dd7d5a24ca4c31c33e1918b3453ea9aa32597121b # 按照提示，如果你是普通用户在操作，执行一下下面几条 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config # 如果是root用户在操作初始化，执行下面一条即可 export KUBECONFIG=/etc/kubernetes/admin.conf # 查看docker镜像,可以看到kube..和etcd等镜像 docker images # Token过期后生成新的token（没提示过期下面两步就不用管了） kubeadm token create --print-join-command # Master需要生成--certificate-key kubeadm init phase upload-certs --upload-certs 其他节点加入集群 其他master节点加入k8s-master01\n# 根据kubeadm init提示的token kubeadm join 192.168.43.182:16443 --token 7t2weq.bjbawausm0jaxury \\ --discovery-token-ca-cert-hash sha256:5257d44118ab035adc5af89dd7d5a24ca4c31c33e1918b3453ea9aa32597121b \\ --control-plane --certificate-key 9a2e86718fceba001c96e503e9df47db3a645d4917bf783decaea9c5d0a726ed node节点加入k8s-master01\n# 根据kubeadm init提示的token kubeadm join 192.168.43.182:16443 --token 7t2weq.bjbawausm0jaxury \\ --discovery-token-ca-cert-hash sha256:5257d44118ab035adc5af89dd7d5a24ca4c31c33e1918b3453ea9aa32597121b # master01上查看加入后的节点信息，因为还未配置CNI插件，所以node之间通信还未打通 [root@k8s-master01 ~]# kubectl get no NAME STATUS ROLES AGE VERSION k8s-master01 NotReady control-plane,master 19m v1.23.0 k8s-master02 NotReady control-plane,master 3m39s v1.23.0 k8s-master03 NotReady control-plane,master 3m35s v1.23.0 k8s-node01 NotReady \u0026lt;none\u0026gt; 2m21s v1.23.0 k8s-node02 NotReady \u0026lt;none\u0026gt; 2m21s v1.23.0 配置calico网络 在k8s-master01节点上执行\n网络方案也可以选择其他(例如：flannel等)\n# 先把所需的配置文件从GitHub拉下来 git clone https://github.com/deemoprobe/k8s-ha-install.git # 切换到1.23分支并进入calico文件夹 cd /root/k8s-ha-install \u0026amp;\u0026amp; git checkout manual-installation-v1.23.x \u0026amp;\u0026amp; cd calico/ # 替换一下POD网段 POD_SUBNET=`cat /etc/kubernetes/manifests/kube-controller-manager.yaml | grep cluster-cidr= | awk -F= \u0026#39;{print $NF}\u0026#39;` sed -i \u0026#34;s#POD_CIDR#${POD_SUBNET}#g\u0026#34; calico.yaml # 应用calico插件 kubectl apply -f calico.yaml # 集群节点均已处于Ready状态 [root@k8s-master01 calico]# kubectl get no NAME STATUS ROLES AGE VERSION k8s-master01 Ready control-plane,master 29m v1.23.0 k8s-master02 Ready control-plane,master 13m v1.23.0 k8s-master03 Ready control-plane,master 13m v1.23.0 k8s-node01 Ready \u0026lt;none\u0026gt; 11m v1.23.0 k8s-node02 Ready \u0026lt;none\u0026gt; 11m v1.23.0 # 查看calico Pod是否都正常 kubectl get pod -A # 如果不正常，可以排查一下，一般是镜像拉取问题，多等待几分钟即可，也可以根据报错简单处理一下 kubectl describe pod XXX -n kube-system # 比如我这里有个pod处于pending状态，查看一下原因 [root@k8s-master01 calico]# kubectl get pod -A NAMESPACE NAME READY STATUS RESTARTS AGE ... kube-system calico-typha-8445487f56-hx8w9 1/1 Running 0 11m kube-system calico-typha-8445487f56-mh6tp 0/1 Pending 0 11m kube-system calico-typha-8445487f56-pxthb 1/1 Running 0 11m ... # 可以看到提示说2个node节点无法提供足量的pod端口分配需求，而且提示master节点设置了污点 [root@k8s-master01 calico]# kubectl describe pod calico-typha-8445487f56-mh6tp -n kube-system ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 11m default-scheduler 0/5 nodes are available: 2 node(s) had taint {node.kubernetes.io/not-ready: }, that the pod didn\u0026#39;t tolerate, 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn\u0026#39;t tolerate. Warning FailedScheduling 10m default-scheduler 0/5 nodes are available: 1 node(s) didn\u0026#39;t have free ports for the requested pod ports, 1 node(s) had taint {node.kubernetes.io/not-ready: }, that the pod didn\u0026#39;t tolerate, 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn\u0026#39;t tolerate. Warning FailedScheduling 10m default-scheduler 0/5 nodes are available: 2 node(s) didn\u0026#39;t have free ports for the requested pod ports, 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn\u0026#39;t tolerate. Warning FailedScheduling 8m24s (x1 over 9m24s) default-scheduler 0/5 nodes are available: 2 node(s) didn\u0026#39;t have free ports for the requested pod ports, 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn\u0026#39;t tolerate.\u0026#39; # 确认一下，可以看到三个master节点打上了不可调度pod的污点 [root@k8s-master01 calico]# for i in k8s-master01 k8s-master02 k8s-master03;do kubectl describe node $i | grep -i taint;done Taints: node-role.kubernetes.io/master:NoSchedule Taints: node-role.kubernetes.io/master:NoSchedule Taints: node-role.kubernetes.io/master:NoSchedule # 由于是练习环境，我就把不可调度的污点取消了，如果是生产环境，建议扩容node工作节点来实现足量的端口分配 [root@k8s-master01 calico]# for i in k8s-master01 k8s-master02 k8s-master03;do kubectl taint node $i node-role.kubernetes.io/master:NoSchedule-;done node/k8s-master01 untainted node/k8s-master02 untainted node/k8s-master03 untainted # 污点成功取消 [root@k8s-master01 calico]# for i in k8s-master01 k8s-master02 k8s-master03;do kubectl describe node $i | grep -i taint;done Taints: \u0026lt;none\u0026gt; Taints: \u0026lt;none\u0026gt; Taints: \u0026lt;none\u0026gt; # 再查看刚才处于pending的pod发现已经处于running状态了 [root@k8s-master01 calico]# kubectl get po -A | grep calico-typha-8445487f56-mh6tp kube-system calico-typha-8445487f56-mh6tp 1/1 Running 0 23m 部署Metrics 在新版的Kubernetes中系统资源的采集均使用Metrics-server，可以通过Metrics采集节点和Pod的内存、磁盘、CPU和网络的使用率。\n# 将Master01节点的front-proxy-ca.crt复制到所有Node节点 [root@k8s-master01 calico]# for i in k8s-node01 k8s-node02;do scp /etc/kubernetes/pki/front-proxy-ca.crt $i:/etc/kubernetes/pki/front-proxy-ca.crt;done front-proxy-ca.crt 100% 1115 593.4KB/s 00:00 front-proxy-ca.crt 100% 1115 1.4MB/s 00:00 # 安装metrics server [root@k8s-master01 calico]# cd /root/k8s-ha-install/kubeadm-metrics-server [root@k8s-master01 kubeadm-metrics-server]# kubectl apply -f comp.yaml serviceaccount/metrics-server created clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created clusterrole.rbac.authorization.k8s.io/system:metrics-server created rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created service/metrics-server created deployment.apps/metrics-server created apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created # 查看运行状态 [root@k8s-master01 kubeadm-metrics-server]# kubectl get po -A | grep metrics kube-system metrics-server-5cf8885b66-2nnb6 1/1 Running 0 68s # 部署后便可以查看指标了 [root@k8s-master01 kubeadm-metrics-server]# kubectl top node NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% k8s-master01 177m 8% 1196Mi 64% k8s-master02 153m 7% 1101Mi 58% k8s-master03 163m 8% 1102Mi 58% k8s-node01 88m 4% 848Mi 45% k8s-node02 85m 4% 842Mi 45% [root@k8s-master01 kubeadm-metrics-server]# kubectl top po NAME CPU(cores) MEMORY(bytes) nginx-85b98978db-7mn6r 0m 3Mi 部署Dashboard Dashboard是一个展示Kubernetes集群资源和Pod日志，甚至可以执行容器命令的web控制台。\n# 直接部署即可 [root@k8s-master01 kubeadm-metrics-server]# cd /root/k8s-ha-install/dashboard/ [root@k8s-master01 dashboard]# ls dashboard-user.yaml dashboard.yaml [root@k8s-master01 dashboard]# kubectl apply -f . serviceaccount/admin-user created clusterrolebinding.rbac.authorization.k8s.io/admin-user created namespace/kubernetes-dashboard created serviceaccount/kubernetes-dashboard created service/kubernetes-dashboard created secret/kubernetes-dashboard-certs created secret/kubernetes-dashboard-csrf created secret/kubernetes-dashboard-key-holder created configmap/kubernetes-dashboard-settings created role.rbac.authorization.k8s.io/kubernetes-dashboard created clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created deployment.apps/kubernetes-dashboard created service/dashboard-metrics-scraper created deployment.apps/dashboard-metrics-scraper created # 查看dashboard端口，默认是NodePort模式，访问集群内任意节点的31073端口即可 [root@k8s-master01 dashboard]# kubectl get svc -owide -A | grep dash kubernetes-dashboard dashboard-metrics-scraper ClusterIP 10.105.172.8 \u0026lt;none\u0026gt; 8000/TCP 19m k8s-app=dashboard-metrics-scraper kubernetes-dashboard kubernetes-dashboard NodePort 10.99.148.159 \u0026lt;none\u0026gt; 443:31073/TCP 19m k8s-app=kubernetes-dashboard 访问dashboard：https://集群内任意节点IP:31073\n发现提示隐私设置错误的问题，如图：\n在Chrome浏览器启动参数加入--test-type --ignore-certificate-errors，然后再访问就没有这个安全提示了\n# 获取登陆令牌（token） [root@k8s-master01 dashboard]# kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk \u0026#39;{print $1}\u0026#39;) Name: admin-user-token-mwnfs Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: admin-user kubernetes.io/service-account.uid: 29584392-1cbd-4d5c-91af-9dd4703008aa Type: kubernetes.io/service-account-token Data ==== ca.crt: 1099 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IjRyZlh6Ukxta0FlajlHREF5ei1mdl8tZmR6ekwteV9fVEIwalQtejRwUk0ifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLW13bmZzIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIyOTU4NDM5Mi0xY2JkLTRkNWMtOTFhZi05ZGQ0NzAzMDA4YWEiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.pMBMkLAP2AoymIXJC7H47IPu3avdBWPYSZfvjRME7lEQAnbe-SM-yrTFGPzcsJQC3O9gPDvXgIZ1x1tQUtQhc_333GtDMj_VL9oEZxYiOdd578CnBiFmF0BWVX06pAzONgKbguamMD8XEPAvKt4mnlDUr7WCeQJZf_juXKdl7ZOBtrM5Zae0UQHFG6juKLmFP-XxIgoDVIPhcxeAH1ktOHM9Fk1M831hywL1SL2OLHiN52wGLT4WuYrP2iUbJkNpt2PYitSp3iNuh7rESL4Ur7lmFQkLZa9e5vNMCc1wTwOAWvaW4P5TbxtfI_ng4NK_avquiXJY-67D77G-8WKzWg 集群优化(可选) Docker可在/etc/docker/daemon.json自定义优化配置，所有配置可见：官方docker configuration，docker常用优化配置见下方注释说明。\n# 优化docker配置 # /etc/docker/daemon.json文件，按需配置，不需要全部都照抄，使用时删除注释，因为JSON文件不支持注释 { \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;], # cgroups驱动 \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://ynirk4k5.mirror.aliyuncs.com\u0026#34;], # 镜像加速器地址 \u0026#34;allow-nondistributable-artifacts\u0026#34;: [], \u0026#34;api-cors-header\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;authorization-plugins\u0026#34;: [], \u0026#34;bip\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;bridge\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;cgroup-parent\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;cluster-advertise\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;cluster-store\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;cluster-store-opts\u0026#34;: {}, \u0026#34;containerd\u0026#34;: \u0026#34;/run/containerd/containerd.sock\u0026#34;, \u0026#34;containerd-namespace\u0026#34;: \u0026#34;docker\u0026#34;, \u0026#34;data-root\u0026#34;: \u0026#34;\u0026#34;, # 数据根目录，大量docker镜像可能会占用较大存储，可以设置系统盘外的挂载盘 \u0026#34;debug\u0026#34;: true, \u0026#34;default-address-pools\u0026#34;: [ { \u0026#34;base\u0026#34;: \u0026#34;172.30.0.0/16\u0026#34;, \u0026#34;size\u0026#34;: 24 }, { \u0026#34;base\u0026#34;: \u0026#34;172.31.0.0/16\u0026#34;, \u0026#34;size\u0026#34;: 24 } ], \u0026#34;default-cgroupns-mode\u0026#34;: \u0026#34;private\u0026#34;, \u0026#34;default-gateway\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;default-gateway-v6\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;default-runtime\u0026#34;: \u0026#34;runc\u0026#34;, \u0026#34;default-shm-size\u0026#34;: \u0026#34;64M\u0026#34;, \u0026#34;default-ulimits\u0026#34;: { \u0026#34;nofile\u0026#34;: { \u0026#34;Hard\u0026#34;: 64000, \u0026#34;Name\u0026#34;: \u0026#34;nofile\u0026#34;, \u0026#34;Soft\u0026#34;: 64000 } }, \u0026#34;dns\u0026#34;: [], \u0026#34;dns-opts\u0026#34;: [], \u0026#34;dns-search\u0026#34;: [], \u0026#34;exec-root\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;experimental\u0026#34;: false, \u0026#34;features\u0026#34;: {}, \u0026#34;fixed-cidr\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;fixed-cidr-v6\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;group\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;hosts\u0026#34;: [], \u0026#34;icc\u0026#34;: false, \u0026#34;init\u0026#34;: false, \u0026#34;init-path\u0026#34;: \u0026#34;/usr/libexec/docker-init\u0026#34;, \u0026#34;insecure-registries\u0026#34;: [], \u0026#34;ip\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;ip-forward\u0026#34;: false, \u0026#34;ip-masq\u0026#34;: false, \u0026#34;iptables\u0026#34;: false, \u0026#34;ip6tables\u0026#34;: false, \u0026#34;ipv6\u0026#34;: false, \u0026#34;labels\u0026#34;: [], \u0026#34;live-restore\u0026#34;: true, # docker进程宕机时容器依然保持存活 \u0026#34;log-driver\u0026#34;: \u0026#34;json-file\u0026#34;, # 日志格式 \u0026#34;log-level\u0026#34;: \u0026#34;\u0026#34;, # 日志级别 \u0026#34;log-opts\u0026#34;: { # 日志优化 \u0026#34;cache-disabled\u0026#34;: \u0026#34;false\u0026#34;, \u0026#34;cache-max-file\u0026#34;: \u0026#34;5\u0026#34;, \u0026#34;cache-max-size\u0026#34;: \u0026#34;20m\u0026#34;, \u0026#34;cache-compress\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;env\u0026#34;: \u0026#34;os,customer\u0026#34;, \u0026#34;labels\u0026#34;: \u0026#34;somelabel\u0026#34;, \u0026#34;max-file\u0026#34;: \u0026#34;5\u0026#34;, # 最大日志数量 \u0026#34;max-size\u0026#34;: \u0026#34;10m\u0026#34; # 保存的最大日志大小 }, \u0026#34;max-concurrent-downloads\u0026#34;: 3, # pull下载并发数 \u0026#34;max-concurrent-uploads\u0026#34;: 5, # push上传并发数 \u0026#34;max-download-attempts\u0026#34;: 5, \u0026#34;mtu\u0026#34;: 0, \u0026#34;no-new-privileges\u0026#34;: false, \u0026#34;node-generic-resources\u0026#34;: [ \u0026#34;NVIDIA-GPU=UUID1\u0026#34;, \u0026#34;NVIDIA-GPU=UUID2\u0026#34; ], \u0026#34;oom-score-adjust\u0026#34;: -500, \u0026#34;pidfile\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;raw-logs\u0026#34;: false, \u0026#34;runtimes\u0026#34;: { \u0026#34;cc-runtime\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/usr/bin/cc-runtime\u0026#34; }, \u0026#34;custom\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/usr/local/bin/my-runc-replacement\u0026#34;, \u0026#34;runtimeArgs\u0026#34;: [ \u0026#34;--debug\u0026#34; ] } }, \u0026#34;seccomp-profile\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;selinux-enabled\u0026#34;: false, \u0026#34;shutdown-timeout\u0026#34;: 15, \u0026#34;storage-driver\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;storage-opts\u0026#34;: [], \u0026#34;swarm-default-advertise-addr\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;tls\u0026#34;: true, \u0026#34;tlscacert\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;tlscert\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;tlskey\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;tlsverify\u0026#34;: true, \u0026#34;userland-proxy\u0026#34;: false, \u0026#34;userland-proxy-path\u0026#34;: \u0026#34;/usr/libexec/docker-proxy\u0026#34;, \u0026#34;userns-remap\u0026#34;: \u0026#34;\u0026#34; } # 无注释版 { \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;], \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://ynirk4k5.mirror.aliyuncs.com\u0026#34;], \u0026#34;containerd-namespace\u0026#34;: \u0026#34;docker\u0026#34;, \u0026#34;data-root\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;debug\u0026#34;: true, \u0026#34;default-cgroupns-mode\u0026#34;: \u0026#34;private\u0026#34;, \u0026#34;default-gateway\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;default-gateway-v6\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;default-runtime\u0026#34;: \u0026#34;runc\u0026#34;, \u0026#34;default-shm-size\u0026#34;: \u0026#34;64M\u0026#34;, \u0026#34;default-ulimits\u0026#34;: { \u0026#34;nofile\u0026#34;: { \u0026#34;Hard\u0026#34;: 64000, \u0026#34;Name\u0026#34;: \u0026#34;nofile\u0026#34;, \u0026#34;Soft\u0026#34;: 64000 } }, \u0026#34;init-path\u0026#34;: \u0026#34;/usr/libexec/docker-init\u0026#34;, \u0026#34;live-restore\u0026#34;: true, \u0026#34;log-driver\u0026#34;: \u0026#34;json-file\u0026#34;, \u0026#34;log-level\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;log-opts\u0026#34;: { \u0026#34;cache-disabled\u0026#34;: \u0026#34;false\u0026#34;, \u0026#34;cache-max-file\u0026#34;: \u0026#34;5\u0026#34;, \u0026#34;cache-max-size\u0026#34;: \u0026#34;20m\u0026#34;, \u0026#34;cache-compress\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;env\u0026#34;: \u0026#34;os,customer\u0026#34;, \u0026#34;labels\u0026#34;: \u0026#34;somelabel\u0026#34;, \u0026#34;max-file\u0026#34;: \u0026#34;5\u0026#34;, \u0026#34;max-size\u0026#34;: \u0026#34;10m\u0026#34; }, \u0026#34;max-concurrent-downloads\u0026#34;: 3, \u0026#34;max-concurrent-uploads\u0026#34;: 5, \u0026#34;max-download-attempts\u0026#34;: 5, \u0026#34;mtu\u0026#34;: 0, \u0026#34;no-new-privileges\u0026#34;: false, \u0026#34;oom-score-adjust\u0026#34;: -500, \u0026#34;pidfile\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;raw-logs\u0026#34;: false, \u0026#34;runtimes\u0026#34;: { \u0026#34;cc-runtime\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/usr/bin/cc-runtime\u0026#34; }, \u0026#34;custom\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/usr/local/bin/my-runc-replacement\u0026#34;, \u0026#34;runtimeArgs\u0026#34;: [ \u0026#34;--debug\u0026#34; ] } }, \u0026#34;seccomp-profile\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;selinux-enabled\u0026#34;: false, \u0026#34;shutdown-timeout\u0026#34;: 15, \u0026#34;storage-driver\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;storage-opts\u0026#34;: [], \u0026#34;swarm-default-advertise-addr\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;userland-proxy-path\u0026#34;: \u0026#34;/usr/libexec/docker-proxy\u0026#34;, \u0026#34;userns-remap\u0026#34;: \u0026#34;\u0026#34; } # 设置证书有效期 [root@k8s-master01 ~]# vim /usr/lib/systemd/system/kube-controller-manager.service ... # 加入下面配置 --experimental-cluster-signing-duration=876000h0m0s ... [root@k8s-master01 ~]# systemctl daemon-reload [root@k8s-master01 ~]# systemctl restart kube-controller-manager # kubelet优化加密算法，默认的算法容易被漏洞扫描；增长镜像下载周期，避免有些大镜像未下载完成就被动死亡退出 # --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 # --image-pull-progress-deadline=30m [root@k8s-master01 ~]# vim /etc/systemd/system/kubelet.service.d/10-kubelet.conf ... # 下面这行中KUBELET_EXTRA_ARGS=后加入配置 Environment=\u0026#34;KUBELET_EXTRA_ARGS=--tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 --image-pull-progress-deadline=30m\u0026#34; ... # 集群配置优化，详见https://kubernetes.io/zh/docs/tasks/administer-cluster/reserve-compute-resources/ [root@k8s-master01 ~]# vim /etc/kubernetes/kubelet-conf.yml # 文件中添加如下配置 rotateServerCertificates: true allowedUnsafeSysctls: # 允许在修改内核参数，此操作按情况选择，用不到就不用设置 - \u0026#34;net.core*\u0026#34; - \u0026#34;net.ipv4.*\u0026#34; kubeReserved: # 为Kubernetes集群守护进程组件预留资源，例如：kubelet、Runtime等 cpu: \u0026#34;100m\u0026#34; memory: 100Mi ephemeral-storage: 1Gi systemReserved: # 为系统守护进程预留资源，例如：sshd、cron等 cpu: \u0026#34;100m\u0026#34; memory: 100Mi ephemeral-storage: 1Gi # 更改kube-proxy模式为ipvs [root@k8s-master01 dashboard]# kubectl edit cm kube-proxy -n kube-system mode: \u0026#34;ipvs\u0026#34; # 更新kube-proxy的pod [root@k8s-master01 dashboard]# kubectl patch daemonset kube-proxy -p \u0026#34;{\\\u0026#34;spec\\\u0026#34;:{\\\u0026#34;template\\\u0026#34;:{\\\u0026#34;metadata\\\u0026#34;:{\\\u0026#34;annotations\\\u0026#34;:{\\\u0026#34;date\\\u0026#34;:\\\u0026#34;`date +\u0026#39;%s\u0026#39;`\\\u0026#34;}}}}}\u0026#34; -n kube-system daemonset.apps/kube-proxy patched # 验证 [root@k8s-master01 dashboard]# curl 127.0.0.1:10249/proxyMode ipvs # 为集群节点打标签，删除标签把 = 换成 - 即可 kubectl label nodes k8s-node01 node-role.kubernetes.io/node= kubectl label nodes k8s-node02 node-role.kubernetes.io/node= kubectl label nodes k8s-master01 node-role.kubernetes.io/master= kubectl label nodes k8s-master02 node-role.kubernetes.io/master= kubectl label nodes k8s-master03 node-role.kubernetes.io/master= # 添加标签后查看集群状态 [root@k8s-master01 ~]# kubectl get node NAME STATUS ROLES AGE VERSION k8s-master01 Ready master 100m v1.23.0 k8s-master02 Ready master 100m v1.23.0 k8s-master03 Ready master 100m v1.23.0 k8s-node01 Ready node 100m v1.23.0 k8s-node02 Ready node 100m v1.23.0 生产环境建议ETCD集群和Kubernetes集群分离，而且使用高性能数据盘存储数据，根据情况决定是否将Master节点也作为Pod调度节点。\n测试集群 # 测试namespace kubectl get namespace kubectl create namespace test kubectl get namespace kubectl delete namespace test # 创建nginx实例并开放端口 kubectl create deployment nginx --image=nginx kubectl expose deployment nginx --port=80 --type=NodePort # 查看调度状态和端口号 [root@k8s-master01 calico]# kubectl get pod,svc -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/nginx-85b98978db-7mn6r 1/1 Running 0 2m16s 172.27.14.193 k8s-master02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 59m \u0026lt;none\u0026gt; service/nginx NodePort 10.104.33.99 \u0026lt;none\u0026gt; 80:31720/TCP 2m6s app=nginx 可见调度到了k8s-master02（IP地址是192.168.43.184）上，对应的NodePort为31720\n在浏览器输入http://192.168.43.184:31720/ 访问nginx，访问结果如图\n至此，基于kubeadm的Kubernetes高可用集群部署并验证成功。\n","permalink":"https://deemoprobe.github.io/posts/tech/kubernetes/kuberneteskubeadminstallation-ha/","summary":"环境说明 宿主机系统：Windows 10 虚拟机版本：VMware® Workstation 16 Pro IOS镜像版本：CentOS Linux release 7.9.2009 集群操作用户：root Kubernetes版本：1.23.0 Etcd版本：3.5.1 Runtime：Docker 20.10 CentOS7安装请参考博客文章：LINUX之VMWARE WOR","title":"KubernetesKubeadmInstallation HA"},{"content":"概念 Kubernetes 是谷歌开源的容器集群管理系统，是 Google 多年大规模容器管理技术 Borg 的开源版本。Borg作业调度系统详见论文：Google使用Borg进行大规模集群管理\nKubernetes以计算节点为基础管理集群，在容器技术的基础上，抽象出Pod，Pod暴露的服务抽象为Service，负载均衡抽象为Ingress，密码和配置抽象为Secret和ConfigMap。存储资源方面抽象为Volume、PV、PVC。为了实现滚动更新等功能在Pod基础上进一步抽象了ReplicaSet、Deployment、DaemonSet、StatefulSet、CronJob等高级资源。拥有强大的作业调度和管理功能，诸如滚动更新、服务发现、健康检查、资源配额和QoS、密码和配置管理、认证授权和准入控制、支持多种调度机制和存储类型。不依赖具体语言技术栈，为微服务提供全套的解决方案。插件化的架构和云原生标准化的推动使Kubernetes具备更强大的生态和发展前景。\nKubernetes理念：\n集群管理：以计算节点为基础，管理集群彼此通信的节点 作业调度和管理 支持多种存储类型：如本地存储和网络存储 滚动更新和回退 高利用率调度机制 资源配额和QoS 自愈机制：健康检查和故障恢复 密码和配置管理 服务发现与治理 声明式API 控制器模式 插件化架构 标准化 Kubernetes高可用架构图如图所示\n基于Kubeadm或二进制搭建高可用集群，最为重要的是数据持久化，通过ETCD集群的高可用实现，ETCD集群可以内建于Kubernetes集群节点中，也可以独立于Kubernetes集群外，不管是什么方式，ETCD存储必须采用高性能的磁盘。\n模型设计 TypeMeta定义对象类型 Group：对象分组，如apps、node.k8s.io Kind：对象类型，如Node、Pod、Deployment Version：对象版本，如v1、v1beta1 Metadata定义对象身份 Namespace：对象所在命名空间 Name：对象名称 Label：对象标签 Annotation：对象注释信息 Finalizer：资源锁，当对象接收删除请求时，如果该字段不为空，则会等待列表中资源释放 ResourceVersion：资源版本，保证对象多线程操作时的一致性，是一种乐观锁 时间戳、UID Spec定义对象的状态 ETCD Etcd是轻量型分布式键值数据存储组件，与APIServer交互实现资源配置数据的持久化。诸如集群中Node、Pod、Service等对象的状态和元数据，以及配置数据等。ETCD高可用实现的两种方式：\nKubernetes集群内建高可用etcd集群：在每个Master节点中部署etcd实例，Master节点高可用的同时部署etcd的高可用。这种方式将etcd与Kubernetes主控节点耦合在一起。 Kubernetes集群外部高可用etcd集群：即etcd集群是独立于Kubernetes集群存在的，这种方式将etcd与Kubernetes集群解耦，使得二者故障影响系数降低，更专注于各自本身的集群管理工作。缺点是架构中需要的独立主机数量增加。 任何etcd实例都可以处理读请求，只有领导者可以处理写请求；当etcd实例接收到Kubernetes集群apiserver的写请求时候，如果该实例不是领导者，则请求会转交到领导者处理；领导者将请求复制到其他etcd成员节点进行仲裁，当仲裁过半数实例同意后，领导者才会对请求进行操作。每个集群仅有一名领导者，当领导者不再响应时，其余etcd节点会在选举倒计时结束后开始新领导者的选举，将自己标记为候选者，集群内投票选举。\nETCD集群实例之间通过Raft一致性共识算法确保数据的一致性，一般是3或5个etcd实例组成高可用集群：奇数个是为了保证etcd leader选举的合理性；不使用更多的etcd实例原因是一方面为了节省计算资源，另一方面是Raft算法机制导致如果实例太多，集群写仲裁时间边长，性能会一定程度上变低。此外，etcd集群所能容忍的故障节点数最多为(N-1)/2，N为etcd集群实例数。\n一个三节点etcd高可用集群的状态：\nRaft一致性共识算法机制：\n候选者（Candidate）、领导者（Leader）、追随者（Follower）和任期（Term）； 最初所有节点都是follower，每个节点分配一个不同的随机election-timeout（150ms~300ms），term=0 当最短election-timeout节点超时后，term+1，标记自己为candidate，发起投票并投自己一票 follower收到term比自己大节点的选举请求后，选举该节点为leader，并更新自己的term与leader节点相同，重置election-timeout 运行期间，领导者间歇性发送heatbeat-timeout，证明自己还活着，不需要选举新领导 leader宕机后，follower监视到leader心跳消失，出现新的election-timeout超时节点，标记自己为candidate，开启新一轮选举 如果出现争抢则重新开始选举保证不会发生脑裂现象 leader处理写操作时要得到该区域半数以上节点的仲裁确认信息才能向数据库提交数据 如果网络原因导致脑裂，则节点较少的区域因为无法得到该区域半数以上节点确认而停止数据提交操作并取消该区域leader等待网络恢复 网络恢复后，小区域节点回退到故障操作前状态，加入新leader区并同步数据 至多容忍(N-1)/2节点故障 Master节点 集群的控制节点，核心组件如下：\nAPIServer：集群控制平面的前端，承担API网关的职责。是用户请求和系统组件与集群交互的唯一入口，所有资源的创建、删除和更新等操作都是通过调用apiserver的API接口进行。集群内，apiserver是各个模块之间的通信枢纽，提供etcd API接口，这些API能够让集群内其他组件监听到资源对象的增删改的变化；集群外，apiserver充当网关的作用，拥有完整的安全机制，完成客户端身份的认证（Authentication）和授权（Authorization），并对资源进行准入控制（Addmission Control）。 Controller Manager：集群自动化管理和控制中心，包含多种控制器。诸如Pod控制器（rs/deploy/ds/sts/cj/hpa）、网络管理方面（ep/svc/ingress等）和存储方面（pv/pvc等），还有其他几十种控制器。控制器采用主备模式和领导选举机制实现故障转移，允许多个副本同时运行，但只有领导者在工作，其他副本在领导者无法工作时选举新领导者提供服务。 Scheduler 集群Pod调度器，监听apiserver处的Pod变化，根据预定的条件对Pod进行调度。调度程序会综合考虑Pod的资源需求（Quota：CPU或内存）、服务质量（QoS）、亲和力反亲和力、策略约束和集群的运行状况等，将Pod调度到合适的计算节点，实现资源的高利用率。调度器也采用主备模式和领导选举机制。 master节点资源一定要尽量给够，以至于后期不会拖累集群的整体性能。并且允许的情况下，etcd最好也要和master节点区分开来，单独创建集群进行数据的存储，etcd-cluster必须使用高性能ssd硬盘，否则后期将大大影响集群的性能。\n# 集群一个master节点中核心组件守护进程状态和参数 [root@k8s-master01 ~]# systemctl status -l kube-apiserver.service ● kube-apiserver.service - Kubernetes API Server Loaded: loaded (/usr/lib/systemd/system/kube-apiserver.service; enabled; vendor preset: disabled) Active: active (running) since Sun 2022-03-13 19:59:48 CST; 33min ago Docs: https://github.com/kubernetes/kubernetes Main PID: 1782 (kube-apiserver) Tasks: 15 Memory: 375.2M CGroup: /system.slice/kube-apiserver.service └─1782 /usr/local/bin/kube-apiserver --v=2 --logtostderr=true --allow-privileged=true --bind-address=0.0.0.0 --secure-port=6443 --insecure-port=0 --advertise-address=192.168.43.183 --service-cluster-ip-range=10.96.0.0/12 --service-node-port-range=30000-32767 --etcd-servers=https://192.168.43.183:2379,https://192.168.43.184:2379,https://192.168.43.185:2379 --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem --etcd-certfile=/etc/etcd/ssl/etcd.pem --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem --client-ca-file=/etc/kubernetes/pki/ca.pem --tls-cert-file=/etc/kubernetes/pki/apiserver.pem --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-account-issuer=https://kubernetes.default.svc.cluster.local --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota --authorization-mode=Node,RBAC --enable-bootstrap-token-auth=true --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem --requestheader-allowed-names=aggregator --requestheader-group-headers=X-Remote-Group --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-username-headers=X-Remote-User --feature-gates=EphemeralContainers=true ... [root@k8s-master01 ~]# systemctl status -l kube-controller-manager.service ● kube-controller-manager.service - Kubernetes Controller Manager Loaded: loaded (/usr/lib/systemd/system/kube-controller-manager.service; enabled; vendor preset: disabled) Active: active (running) since Sun 2022-03-13 19:58:12 CST; 34min ago Docs: https://github.com/kubernetes/kubernetes Main PID: 1002 (kube-controller) Tasks: 6 Memory: 66.7M CGroup: /system.slice/kube-controller-manager.service └─1002 /usr/local/bin/kube-controller-manager --v=2 --logtostderr=true --address=127.0.0.1 --root-ca-file=/etc/kubernetes/pki/ca.pem --cluster-signing-cert-file=/etc/kubernetes/pki/ca.pem --cluster-signing-key-file=/etc/kubernetes/pki/ca-key.pem --service-account-private-key-file=/etc/kubernetes/pki/sa.key --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig --leader-elect=true --use-service-account-credentials=true --node-monitor-grace-period=40s --node-monitor-period=5s --pod-eviction-timeout=2m0s --controllers=*,bootstrapsigner,tokencleaner --allocate-node-cidrs=true --cluster-cidr=172.16.0.0/12 --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem --node-cidr-mask-size=24 --feature-gates=EphemeralContainers=true ... [root@k8s-master01 ~]# systemctl status -l kube-scheduler.service ● kube-scheduler.service - Kubernetes Scheduler Loaded: loaded (/usr/lib/systemd/system/kube-scheduler.service; enabled; vendor preset: disabled) Active: active (running) since Sun 2022-03-13 19:58:12 CST; 35min ago Docs: https://github.com/kubernetes/kubernetes Main PID: 998 (kube-scheduler) Tasks: 8 Memory: 50.6M CGroup: /system.slice/kube-scheduler.service └─998 /usr/local/bin/kube-scheduler --v=2 --logtostderr=true --address=127.0.0.1 --leader-elect=true --kubeconfig=/etc/kubernetes/scheduler.kubeconfig --feature-gates=EphemeralContainers=true ... Node节点 应用部署的节点，工作节点，资源调度的对象，核心组件如下：\nkubelet：运行在节点上负责启动容器的守护进程，监听节点上pod的状态，与master节点apiserver进行通信将状态上报到主控节点。kubelet需定时（nodeStatusUpdateFrequency: 10s 默认10s）向apiserver汇报自身的情况（磁盘空间状态、CPU和Memory是否有压力和自身服务是否Ready等），如果Node上的kubelet停止了汇报，NodeLifecycle控制器将标记对应node状态为NotReady，过一段时间后驱逐该node节点上的pod。pod被调度到kubelet所在节点时，kubelet会首先将pod中申请的volume挂载到当前节点，之后调用容器运行时创建pause沙箱（PodSandBox）容器（该容器用以维护pod网络协议栈）和pod容器。kubelet周期性地查询容器的状态，并定期汇报容器状态，通过cAdvisor监控容器资源的使用情况。 kube-proxy：负责pod之间的通信和负载均衡，将指定的流量分发到后端正确的机器上，一般工作模式默认为ipvs。具体工作行为是从apiserver监听service和endpoint对象，根据endpoint信息设置service到后端pod的路由，维护网络规则。 上面两个守护进程并不是工作节点特有，一般master控制节点也会启动kubelet和kube-proxy进程提高集群的可用性。\nIPVS：监听master节点增加和删除service以及endpoint的消息，调用netlink接口创建相应的ipvs规则。通过ipvs规则将流量转到相应的pod上。ipvs是内核级的转发，速度很快。 Iptables：监听master节点增加和删除service以及endpoint的消息，对于每一个service，都会创建一个iptables规则，将service的clusterIP代理到后端对应的pod上。iptables由于线性查找匹配、全量更新等特点，当规则很多时，性能会比ipvs差，所以一般选择ipvs即可。\niptables工具基于Linux内核的Netfilter模块，默认定义了多张表。每张表里包含若干内置链（chain），也可能包含用户自定义的链。每条链是一套规则（rule）列表，用于匹配一组数据包。每条规则都指定如何处理匹配的数据包。\ntable：包含一组chain的表\nchain：包含一组rule的链\nrule：匹配数据包的规则，例如：协议，端口号\ntarget：规则中的具体行为，例如：ACCEPT，DROP，INPUT，FORWARD，OUTPUT\n表\nfilter：默认表，通用数据包过滤表 mangle：为特定的数据包设计 nat：针对创建新连接的数据包 raw：主要用于结合 NOTRACK target 配置连接跟踪的豁免 security：用于MAC（Mandatory Access Control，强制访问控制）规则 规则链\nINPUT：以本机为目标的入口数据包规则链 OUTPUT：本机产生，向外转发的数据包规则链 FORWARD：路由经过本机的数据包规则链 PREROUTING：数据包进入路由之前的规则链 POSTROUTING：数据包发送到目标前（出路由）的规则链 IPtables处理链接的算法复杂度为O(n)，IPVS为O(1)，Service规模在1000内二者差距并不是很大。在Service规模超过1000后，IPtables规则链达到2000以上，性能开始下降，响应时间成倍数增加，IPVS则几乎不受Service规模的影响。Calico虽然也采用IPtables技术，但对规则链进行了优化，算法复杂度也达到了了O(1)的水平。除了大规模服务性能的差距，IPVS还具备复杂均衡算法全（轮询、最小连接数、哈希值、最小延迟等）、支持健康检查等优点。\n# 集群一个node节点核心组件运行状态和参数 [root@k8s-node01 ~]# systemctl status -l kubelet.service ● kubelet.service - Kubernetes Kubelet Loaded: loaded (/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled) Drop-In: /etc/systemd/system/kubelet.service.d └─10-kubelet.conf Active: active (running) since Sun 2022-03-13 20:03:34 CST; 33min ago Docs: https://github.com/kubernetes/kubernetes Main PID: 1572 (kubelet) Tasks: 14 Memory: 164.2M CGroup: /system.slice/kubelet.service └─1572 /usr/local/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig --kubeconfig=/etc/kubernetes/kubelet.kubeconfig --config=/etc/kubernetes/kubelet-conf.yml --network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin --container-runtime=remote --runtime-request-timeout=15m --container-runtime-endpoint=unix:///run/containerd/containerd.sock --cgroup-driver=systemd --node-labels=node.kubernetes.io/node= ... [root@k8s-node01 ~]# systemctl status -l kube-proxy.service ● kube-proxy.service - Kubernetes Kube Proxy Loaded: loaded (/usr/lib/systemd/system/kube-proxy.service; enabled; vendor preset: disabled) Active: active (running) since Sun 2022-03-13 20:03:32 CST; 33min ago Docs: https://github.com/kubernetes/kubernetes Main PID: 996 (kube-proxy) Tasks: 6 Memory: 57.9M CGroup: /system.slice/kube-proxy.service └─996 /usr/local/bin/kube-proxy --config=/etc/kubernetes/kube-proxy.yaml --v=2 --feature-gates=EphemeralContainers=true ... # kubelet进程对应的配置文件--config=/etc/kubernetes/kubelet-conf.yml [root@k8s-node01 ~]# cat /etc/kubernetes/kubelet-conf.yml apiVersion: kubelet.config.k8s.io/v1beta1 kind: KubeletConfiguration address: 0.0.0.0 port: 10250 readOnlyPort: 10255 authentication: anonymous: enabled: false webhook: cacheTTL: 2m0s enabled: true x509: clientCAFile: /etc/kubernetes/pki/ca.pem authorization: mode: Webhook webhook: cacheAuthorizedTTL: 5m0s cacheUnauthorizedTTL: 30s cgroupDriver: systemd cgroupsPerQOS: true clusterDNS: - 10.96.0.10 clusterDomain: cluster.local containerLogMaxFiles: 5 containerLogMaxSize: 10Mi contentType: application/vnd.kubernetes.protobuf cpuCFSQuota: true cpuManagerPolicy: none cpuManagerReconcilePeriod: 10s enableControllerAttachDetach: true enableDebuggingHandlers: true enforceNodeAllocatable: - pods eventBurst: 10 eventRecordQPS: 5 evictionHard: imagefs.available: 15% memory.available: 100Mi nodefs.available: 10% nodefs.inodesFree: 5% evictionPressureTransitionPeriod: 5m0s failSwapOn: true fileCheckFrequency: 20s hairpinMode: promiscuous-bridge healthzBindAddress: 127.0.0.1 healthzPort: 10248 httpCheckFrequency: 20s imageGCHighThresholdPercent: 85 imageGCLowThresholdPercent: 80 imageMinimumGCAge: 2m0s iptablesDropBit: 15 iptablesMasqueradeBit: 14 kubeAPIBurst: 10 kubeAPIQPS: 5 makeIPTablesUtilChains: true maxOpenFiles: 1000000 maxPods: 110 nodeStatusUpdateFrequency: 10s oomScoreAdj: -999 podPidsLimit: -1 registryBurst: 10 registryPullQPS: 5 resolvConf: /etc/resolv.conf rotateCertificates: true runtimeRequestTimeout: 2m0s serializeImagePulls: true staticPodPath: /etc/kubernetes/manifests streamingConnectionIdleTimeout: 4h0m0s syncFrequency: 1m0s volumeStatsAggPeriod: 1m0s featureGates: EphemeralContainers: true # kube-proxy对应的配置文件--config=/etc/kubernetes/kube-proxy.yaml [root@k8s-node01 ~]# cat /etc/kubernetes/kube-proxy.yaml apiVersion: kubeproxy.config.k8s.io/v1alpha1 bindAddress: 0.0.0.0 clientConnection: acceptContentTypes: \u0026#34;\u0026#34; burst: 10 contentType: application/vnd.kubernetes.protobuf kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig qps: 5 clusterCIDR: 172.16.0.0/12 configSyncPeriod: 15m0s conntrack: max: null maxPerCore: 32768 min: 131072 tcpCloseWaitTimeout: 1h0m0s tcpEstablishedTimeout: 24h0m0s enableProfiling: false healthzBindAddress: 0.0.0.0:10256 hostnameOverride: \u0026#34;\u0026#34; iptables: masqueradeAll: false masqueradeBit: 14 minSyncPeriod: 0s syncPeriod: 30s ipvs: masqueradeAll: true minSyncPeriod: 5s scheduler: \u0026#34;rr\u0026#34; syncPeriod: 30s kind: KubeProxyConfiguration metricsBindAddress: 127.0.0.1:10249 mode: \u0026#34;ipvs\u0026#34; nodePortAddresses: null oomScoreAdj: -999 portRange: \u0026#34;\u0026#34; udpIdleTimeout: 250ms 容器运行时 符合CRI（Container Runtime Interface）标准的容器运行时（Container Runtime）是实际上管理容器的组件，容器运行时可分为高层和底层运行时。高层运行时诸如：Docker、containerd、CRI-O，官方介绍:容器运行时 底层运行时诸如：runc、kata、gVisor，kata和gVisor相对不是很成熟，目前底层运行时一般默认选择runc。\n# containerd容器运行时服务状态 [root@k8s-master01 ~]# systemctl status -l containerd.service ● containerd.service - containerd container runtime Loaded: loaded (/usr/lib/systemd/system/containerd.service; enabled; vendor preset: disabled) Active: active (running) since Sun 2022-03-13 19:58:14 CST; 1h 13min ago Docs: https://containerd.io Process: 996 ExecStartPre=/sbin/modprobe overlay (code=exited, status=0/SUCCESS) Main PID: 1011 (containerd) Tasks: 48 Memory: 99.9M CGroup: /system.slice/containerd.service ├─1011 /usr/bin/containerd ├─1857 /usr/bin/containerd-shim-runc-v2 -namespace k8s.io -id 83f9705395fb4a165bd52894f6545a58b8040a5fe6774b03cdcd02ca90900708 -address /run/containerd/containerd.sock ├─2394 /usr/bin/containerd-shim-runc-v2 -namespace k8s.io -id ae7a1e3a28a331bf11762c3b049e8237c322cddabd842dca398911a45dc656db -address /run/containerd/containerd.sock └─2802 /usr/bin/containerd-shim-runc-v2 -namespace k8s.io -id 30e5560350fdcb76efbbb10f7c8e9a41fa4fdc10e4483f62830fff749a448fb1 -address /run/containerd/containerd.sock 网络插件 符合CNI（Container Network Interface）标准的网络插件，诸如：Calico、Cilium、Flannel等。会为每个pod生成唯一的IP地址，并且把每个节点当做一个路由器。Cilium官方有一个CNI性能测试报告，采用eBPF技术的Cilium和Calico性能优越，具备TCP吞吐量大、CPU和网络开销低等特点。eBPF是一种内核级包过滤技术，诸如tcpdump、Netfilter均采用该技术，性能比iptables要好很多。可以使用route -n查看\n# 使用calico插件的路由 [root@k8s-master01 ~]# route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 192.168.43.1 0.0.0.0 UG 0 0 0 ens33 169.254.0.0 0.0.0.0 255.255.0.0 U 1002 0 0 ens33 172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0 172.17.125.0 192.168.43.186 255.255.255.192 UG 0 0 0 tunl0 172.18.195.0 192.168.43.185 255.255.255.192 UG 0 0 0 tunl0 172.25.92.64 192.168.43.184 255.255.255.192 UG 0 0 0 tunl0 172.25.244.192 0.0.0.0 255.255.255.192 U 0 0 0 * 172.25.244.213 0.0.0.0 255.255.255.255 UH 0 0 0 cali183ffe150c7 172.25.244.214 0.0.0.0 255.255.255.255 UH 0 0 0 calicec0b1cfb68 172.27.14.192 192.168.43.187 255.255.255.192 UG 0 0 0 tunl0 192.168.43.0 0.0.0.0 255.255.255.0 U 0 0 0 ens33 CoreDNS 用于集群内部service的域名解析系统，可以让pod把service名称解析成ip地址，然后通过service的IP地址连接到对应的应用上。\nPod Pod是Kubernetes中最小的单元，是由一个或多个容器组成的。每个pod还包含一个pause容器，pause容器是pod的父容器，负责僵尸进程的回收管理，通过pause容器可以使同一个pod内多个容器共享存储、网络、PID、IPC等。详细可以查看博客：Pod定义及零宕机部署\n本文相关概念参考书籍：《Kubernetes生产化实践之路》\n","permalink":"https://deemoprobe.github.io/posts/tech/kubernetes/kubernetesarchitecture-ha/","summary":"概念 Kubernetes 是谷歌开源的容器集群管理系统，是 Google 多年大规模容器管理技术 Borg 的开源版本。Borg作业调度系统详见论文：Google使用Borg进行大规模集群管理 Kubernetes以计算节点为基础管理集群，在容器技术的基础上，抽象出Pod，Pod暴露的服务抽象为Service，负载均衡抽象为","title":"KubernetesArchitecture HA"},{"content":"优化原理 docker镜像层有以下特点（和镜像大小息息相关）：\nRUN、COPY 和 ADD 指令会在已有镜像层的基础上创建一个新的镜像层，执行指令产生的所有文件系统变更会在指令结束后作为一个镜像层整体提交 镜像层具有copy-on-write的特性，如果去更新其他镜像层中已存在的文件，会先将其复制到新的镜像层中再修改，造成双倍的文件空间占用 如果去删除其他镜像层的一个文件，只会在当前镜像层生成一个该文件的删除标记，并不会减少整个镜像的实际体积 [root@centos7 tmp]# cat Dockerfile FROM alpine COPY resource.tar / RUN touch /resource.tar RUN rm -f /resource.tar [root@centos7 tmp]# ls -lh resource.tar -rw-r--r--. 1 root root 9.9M Apr 26 15:46 resource.tar [root@centos7 tmp]# docker build -t demo:v1 . [+] Building 20.3s (9/9) FINISHED =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; [internal] load build definition from Dockerfile 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 177B 0.0s =\u0026gt; [internal] load metadata for docker.io/library/alpine:latest 17.5s =\u0026gt; [internal] load build context 0.2s =\u0026gt; =\u0026gt; transferring context: 10.30MB 0.2s =\u0026gt; [1/4] FROM docker.io/library/alpine@sha256:21a3deaa0d32a8057914f36584b5288d2e5ecc984380bc0118285c70fa8c9300 1.6s =\u0026gt; =\u0026gt; resolve docker.io/library/alpine@sha256:21a3deaa0d32a8057914f36584b5288d2e5ecc984380bc0118285c70fa8c9300 0.0s =\u0026gt; =\u0026gt; sha256:21a3deaa0d32a8057914f36584b5288d2e5ecc984380bc0118285c70fa8c9300 1.64kB / 1.64kB 0.0s =\u0026gt; =\u0026gt; sha256:e7d88de73db3d3fd9b2d63aa7f447a10fd0220b7cbf39803c803f2af9ba256b3 528B / 528B 0.0s =\u0026gt; =\u0026gt; sha256:c059bfaa849c4d8e4aecaeb3a10c2d9b3d85f5165c66ad3a4d937758128c4d18 1.47kB / 1.47kB 0.0s =\u0026gt; =\u0026gt; sha256:59bf1c3509f33515622619af21ed55bbe26d24913cedbca106468a5fb37a50c3 2.82MB / 2.82MB 1.2s =\u0026gt; =\u0026gt; extracting sha256:59bf1c3509f33515622619af21ed55bbe26d24913cedbca106468a5fb37a50c3 0.3s =\u0026gt; [2/4] COPY resource.tar / 0.1s =\u0026gt; [3/4] RUN touch /resource.tar 0.4s =\u0026gt; [4/4] RUN rm -f /resource.tar 0.5s =\u0026gt; exporting to image 0.2s =\u0026gt; =\u0026gt; exporting layers 0.2s =\u0026gt; =\u0026gt; writing image sha256:1113b0fb22dabb56b6926960eafd2a6aaf183d1bd2373f1be10df67c73714f37 0.0s =\u0026gt; =\u0026gt; naming to docker.io/library/demo:v1 0.0s [root@centos7 tmp]# docker history demo:v1 IMAGE CREATED CREATED BY SIZE COMMENT 1113b0fb22da 13 seconds ago RUN /bin/sh -c rm -f /resource.tar # buildkit 0B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 13 seconds ago RUN /bin/sh -c touch /resource.tar # buildkit 10.3MB buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 14 seconds ago COPY resource.tar / # buildkit 10.3MB buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 17 months ago /bin/sh -c #(nop) CMD [\u0026#34;/bin/sh\u0026#34;] 0B \u0026lt;missing\u0026gt; 17 months ago /bin/sh -c #(nop) ADD file:9233f6f2237d79659… 5.59MB [root@centos7 tmp]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE demo v1 1113b0fb22da 24 seconds ago 26.2MB 上面例子可以分析出：alpine基础镜像大小+5.59MB，resource.tar COPY解压后+10.3MB，touch命令执行时复制了一份+10.3MB，rm虽然显示文件大小为0，但实际上前几层的镜像依然存在。最终镜像大小26.2MB。\n镜像分析 docker history IMAGE：查看IMAGE构建过程，展示镜像层信息 dive IMAGE：第三方分析工具，分析镜像层结构，每层镜像所包含的文件以及体积，比history更加详细 dive安装参考官方文档\n[root@centos7 go]# dive demo:v1 ┃ ● Layers ┣━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ │ Current Layer Contents ├────────────────────────────────────────── Cmp Size Command Permission UID:GID Size Filetree 7.0 MB FROM 580fdc606fb93e5 drwxr-xr-x 0:0 841 kB ├── bin 10 MB COPY resource.tar / # buildkit -rwxrwxrwx 0:0 0 B │ ├── arch → /bin/busybox 10 MB RUN /bin/sh -c touch /resource.tar # buildkit -rwxrwxrwx 0:0 0 B │ ├── ash → /bin/busybox 0 B RUN /bin/sh -c rm -f /resource.tar # buildkit -rwxrwxrwx 0:0 0 B │ ├── base64 → /bin/busybox -rwxrwxrwx 0:0 0 B │ ├── bbconfig → /bin/busybox │ Layer Details ├─────────────────────────────────────────────────── -rwxr-xr-x 0:0 841 kB │ ├── busybox -rwxrwxrwx 0:0 0 B │ ├── cat → /bin/busybox Tags: (unavailable) -rwxrwxrwx 0:0 0 B │ ├── chattr → /bin/busybox Id: 580fdc606fb93e5fc291e8df154398e0585362a58b8e5b23bfd9d47ca369 -rwxrwxrwx 0:0 0 B │ ├── chgrp → /bin/busybox 8a79 -rwxrwxrwx 0:0 0 B │ ├── chmod → /bin/busybox Digest: sha256:f1417ff83b319fbdae6dd9cd6d8c9c88002dcd75ecf6ec201c8c6 -rwxrwxrwx 0:0 0 B │ ├── chown → /bin/busybox 894681cf2b5 -rwxrwxrwx 0:0 0 B │ ├── cp → /bin/busybox Command: -rwxrwxrwx 0:0 0 B │ ├── date → /bin/busybox #(nop) ADD file:9a4f77dfaba7fd2aa78186e4ef0e7486ad55101cefc1fabbc1b3 -rwxrwxrwx 0:0 0 B │ ├── dd → /bin/busybox 85601bb38920 in / -rwxrwxrwx 0:0 0 B │ ├── df → /bin/busybox -rwxrwxrwx 0:0 0 B │ ├── dmesg → /bin/busybox │ Image Details ├─────────────────────────────────────────────────── -rwxrwxrwx 0:0 0 B │ ├── dnsdomainname → /bin/busy -rwxrwxrwx 0:0 0 B │ ├── dumpkmap → /bin/busybox -rwxrwxrwx 0:0 0 B │ ├── echo → /bin/busybox Total Image size: 28 MB -rwxrwxrwx 0:0 0 B │ ├── ed → /bin/busybox Potential wasted space: 21 MB -rwxrwxrwx 0:0 0 B │ ├── egrep → /bin/busybox Image efficiency score: 25 % -rwxrwxrwx 0:0 0 B │ ├── false → /bin/busybox -rwxrwxrwx 0:0 0 B │ ├── fatattr → /bin/busybox Count Total Space Path -rwxrwxrwx 0:0 0 B │ ├── fdflush → /bin/busybox 3 21 MB /resource.tar -rwxrwxrwx 0:0 0 B │ ├── fgrep → /bin/busybox 2 0 B /etc -rwxrwxrwx 0:0 0 B │ ├── fsync → /bin/busybox ... 镜像优化 镜像（体积）优化最主要的目的是方便镜像更快速拉取和分发，同时节省硬盘空间，提升部署效率，优化方式可以从下面几个方式入手：\n优化基础镜像，在不影响业务维护的情况下，尽可能使用更小的基础镜像，如：alpine或image:alpine 优化Dockerfile中指令，如多条RUN指令\u0026amp;\u0026amp;串联写在同一行，使之仅生成一层镜像，防止分层过多 使用--squash参数压缩镜像层，官方提示已弃用（本人目前版本是23.0.4） 多阶段构建，更多信息可以查看官网介绍 根据dive分析结果优化 对于基础镜像（最底层）大于500MB的，建议选择更小的基础镜像，如基于alpine的镜像 对于分层大于10层的镜像，获取各层的命令汇总展示，建议调整指令，是否已使用串连的形式 避免产生无用的缓存，如pip应使用--no-cache-dir禁用缓存，yum makecache也会产生缓存 避免无用文件（.md .pdf .txt .doc等一些文档文件不应该出现在镜像中） 避免安装多余的软件 # 先拉取镜像到本地，加快后面的构建速度 docker pull centos:latest docker pull alpine:latest docker pull golang:latest docker pull ubuntu:latest docker pull fedora:28 优化基础镜像 [root@centos7 base]# pwd /root/dockerfile/base [root@centos7 base]# cat Dockerfile FROM centos CMD [\u0026#34;echo\u0026#34;, \u0026#34;hi\u0026#34;] [root@centos7 base]# docker build -t base:v1 . # 修改基础镜像 [root@centos7 base]# cat Dockerfile FROM alpine CMD [\u0026#34;echo\u0026#34;, \u0026#34;hi\u0026#34;] [root@centos7 base]# docker build -t base:v2 . [root@centos7 base]# docker images | grep base base v2 025106dd85eb 4 weeks ago 7.05MB base v1 108460ab34b0 19 months ago 231MB 串联命令 [root@centos7 multirun]# pwd /root/dockerfile/multirun [root@centos7 multirun]# cat Dockerfile FROM fedora:28 RUN dnf install -y nginx RUN dnf clean all RUN rm -rf /var/cache/yum [root@centos7 multirun]# docker build -t multirun:v1 . # 分析镜像 [root@centos7 multirun]# docker images| grep mul multirun v1 2666d7aa527a 2 minutes ago 488MB [root@centos7 multirun]# docker history multirun:v1 IMAGE CREATED CREATED BY SIZE COMMENT 2666d7aa527a 17 seconds ago RUN /bin/sh -c rm -rf /var/cache/yum # build… 0B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 18 seconds ago RUN /bin/sh -c dnf clean all # buildkit 1.79MB buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 19 seconds ago RUN /bin/sh -c dnf install -y nginx # buildk… 226MB buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 4 years ago /bin/sh -c #(nop) CMD [\u0026#34;/bin/bash\u0026#34;] 0B \u0026lt;missing\u0026gt; 4 years ago /bin/sh -c #(nop) ADD file:42ddab590052fda98… 261MB \u0026lt;missing\u0026gt; 4 years ago /bin/sh -c #(nop) ENV DISTTAG=f28container … 0B \u0026lt;missing\u0026gt; 4 years ago /bin/sh -c #(nop) LABEL maintainer=Clement … 0B [root@centos7 multirun]# dive multirun:v1 ┃ ● Layers ┣━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ │ Current Layer Contents ├────────────────────────────────────────── Cmp Size Command Permission UID:GID Size Filetree 260 MB FROM 9bfbc900bde2f55 -rwxrwxrwx 0:0 0 B ├── bin → usr/bin 226 MB RUN /bin/sh -c dnf install -y nginx # buildkit dr-xr-xr-x 0:0 0 B ├── boot 1.8 MB RUN /bin/sh -c dnf clean all # buildkit drwxr-xr-x 0:0 0 B ├── dev 0 B RUN /bin/sh -c rm -rf /var/cache/yum # buildkit drwxr-xr-x 0:0 1.8 MB ├── etc -rw------- 0:0 0 B │ ├── .pwd.lock │ Layer Details ├─────────────────────────────────────────────────── -rw-r--r-- 0:0 4.5 kB │ ├── DIR_COLORS -rw-r--r-- 0:0 5.2 kB │ ├── DIR_COLORS.256color Tags: (unavailable) -rw-r--r-- 0:0 4.6 kB │ ├── DIR_COLORS.lightbgcolor Id: 9bfbc900bde2f55fafbeeed2be52e80c9b06244dee226dc54a354f9d09ea -rw-r--r-- 0:0 94 B │ ├── GREP_COLORS 1910 drwxr-xr-x 0:0 203 B │ ├── X11 Digest: sha256:17beab58d693a5e55591675c3dcc5b575a44e476d125408102ce7 drwxr-xr-x 0:0 0 B │ │ ├── applnk 5348011f807 drwxr-xr-x 0:0 0 B │ │ ├── fontpath.d Command: drwxr-xr-x 0:0 203 B │ │ ├── xinit #(nop) ADD file:42ddab590052fda98865a9ecd9dc28c8d3ba7922a6d21b1f182a drwxr-xr-x 0:0 203 B │ │ │ └── xinitrc.d 0a3769419677 in / -rwxr-xr-x 0:0 203 B │ │ │ └── 50-systemd-us drwxr-xr-x 0:0 0 B │ │ └── xorg.conf.d │ Image Details ├─────────────────────────────────────────────────── -rw-r--r-- 0:0 16 B │ ├── adjtime -rw-r--r-- 0:0 1.5 kB │ ├── aliases drwxr-xr-x 0:0 0 B │ ├── alternatives Total Image size: 488 MB -rwxrwxrwx 0:0 0 B │ │ ├── cifs-idmap-plugin → / Potential wasted space: 234 MB -rwxrwxrwx 0:0 0 B │ │ └── libnssckbi.so.x86_64 Image efficiency score: 54 % drwxr-xr-x 0:0 0 B │ ├── bash_completion.d -rw-r--r-- 0:0 3.0 kB │ ├── bashrc Count Total Space Path drwxr-xr-x 0:0 0 B │ ├── binfmt.d 2 48 MB /var/cache/dnf/fedora-filenames.solvx drwxr-xr-x 0:0 0 B │ ├── chkconfig.d 2 47 MB /var/cache/dnf/fedora-f21308f6293b3270/repodata drwxr-xr-x 0:0 0 B │ ├── cifs-utils /a53009478e29c551710df570a5dc726ce5160300c7c1ecde852895ab8a6fcf72-fi -rwxrwxrwx 0:0 0 B │ │ └── idmap-plugin → /etc/a lelists.xml.gz drwxr-xr-x 0:0 614 B │ ├── crypto-policies 2 24 MB /var/cache/dnf/updates-filenames.solvx drwxr-xr-x 0:0 0 B │ │ ├── back-ends 2 23 MB /var/cache/dnf/updates-8bd9ef368505a5fd/repodat -rwxrwxrwx 0:0 0 B │ │ │ ├── bind.config → /us a/15dc5106b1b18f200a2f520b561e59fd8a75d921acfd7edf34e8d88db3b9220a-f -rwxrwxrwx 0:0 0 B │ │ │ ├── gnutls.config → / ilelists.xml.gz -rwxrwxrwx 0:0 0 B │ │ │ ├── java.config → /us 2 21 MB /var/cache/dnf/fedora.solv -rwxrwxrwx 0:0 0 B │ │ │ ├── krb5.config → /us 2 18 MB /var/lib/rpm/Packages -rwxrwxrwx 0:0 0 B │ │ │ ├── libreswan.config 2 16 MB /var/cache/dnf/fedora-f21308f6293b3270/repodata -rwxrwxrwx 0:0 0 B │ │ │ ├── nss.config → /usr /9659dfc44de50563682658fd41f2ab0da60e5657e1cc4f598b50d39fb3436e0c-pr -rwxrwxrwx 0:0 0 B │ │ │ ├── openssh.config → ... # 从dive分析结果可以看出不但每个RUN都产生了一层镜像，而且实际上缓存并没有清理掉，提示Potential wasted space: 234 MB，潜在浪费了234MB空间 # 优化RUN命令 [root@centos7 multirun]# cat Dockerfile FROM fedora:28 RUN dnf install -y nginx\\ \u0026amp;\u0026amp; dnf clean all\\ \u0026amp;\u0026amp; rm -rf /var/cache/yum [root@centos7 multirun]# docker build -t multirun:v2 . [root@centos7 multirun]# dive multirun:v2 ┃ ● Layers ┣━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ │ Current Layer Contents ├────────────────────────────────────────── Cmp Size Command Permission UID:GID Size Filetree 260 MB FROM 9bfbc900bde2f55 -rwxrwxrwx 0:0 0 B ├── bin → usr/bin 18 MB RUN /bin/sh -c dnf install -y nginx \u0026amp;\u0026amp; dnf clean all \u0026amp;\u0026amp; dr-xr-xr-x 0:0 0 B ├── boot drwxr-xr-x 0:0 0 B ├── dev │ Layer Details ├─────────────────────────────────────────────────── drwxr-xr-x 0:0 1.8 MB ├── etc -rw------- 0:0 0 B │ ├── .pwd.lock Tags: (unavailable) -rw-r--r-- 0:0 4.5 kB │ ├── DIR_COLORS Id: 9bfbc900bde2f55fafbeeed2be52e80c9b06244dee226dc54a354f9d09ea -rw-r--r-- 0:0 5.2 kB │ ├── DIR_COLORS.256color 1910 -rw-r--r-- 0:0 4.6 kB │ ├── DIR_COLORS.lightbgcolor Digest: sha256:17beab58d693a5e55591675c3dcc5b575a44e476d125408102ce7 -rw-r--r-- 0:0 94 B │ ├── GREP_COLORS 5348011f807 drwxr-xr-x 0:0 203 B │ ├── X11 Command: drwxr-xr-x 0:0 0 B │ │ ├── applnk #(nop) ADD file:42ddab590052fda98865a9ecd9dc28c8d3ba7922a6d21b1f182a drwxr-xr-x 0:0 0 B │ │ ├── fontpath.d 0a3769419677 in / drwxr-xr-x 0:0 203 B │ │ ├── xinit drwxr-xr-x 0:0 203 B │ │ │ └── xinitrc.d │ Image Details ├─────────────────────────────────────────────────── -rwxr-xr-x 0:0 203 B │ │ │ └── 50-systemd-us drwxr-xr-x 0:0 0 B │ │ └── xorg.conf.d -rw-r--r-- 0:0 16 B │ ├── adjtime Total Image size: 278 MB -rw-r--r-- 0:0 1.5 kB │ ├── aliases Potential wasted space: 24 MB drwxr-xr-x 0:0 0 B │ ├── alternatives Image efficiency score: 95 % -rwxrwxrwx 0:0 0 B │ │ ├── cifs-idmap-plugin → / -rwxrwxrwx 0:0 0 B │ │ └── libnssckbi.so.x86_64 Count Total Space Path drwxr-xr-x 0:0 0 B │ ├── bash_completion.d 2 18 MB /var/lib/rpm/Packages -rw-r--r-- 0:0 3.0 kB │ ├── bashr ... # 缓存已清理干净，有效空间占比95%，Image efficiency score: 95 % 压缩镜像层（弃用） docker1.13版本后可以直接使用--squash参数压缩镜像层，之前需要单独安装docker-squash工具才能实现。但目前已移除了这个参数，使用也无效。\n# 依旧以上面RUN为例 [root@centos7 squash]# pwd /root/dockerfile/squash [root@centos7 squash]# cat Dockerfile FROM fedora:28 RUN dnf install -y nginx RUN dnf clean all RUN rm -rf /var/cache/yum [root@centos7 squash]# docker build -t squash:v1 . # 压缩前488MB [root@centos7 squash]# docker images | grep squ squash v1 2666d7aa527a 45 minutes ago 488MB # 发现提示已移除该参数，提示使用多阶段构建 [root@centos7 squash]# docker build -t squash:v2 --squash . WARNING: experimental flag squash is removed with BuildKit. You should squash inside build using a multi-stage Dockerfile for efficiency. [root@centos7 squash]# docker images | grep squ squash v1 2666d7aa527a 46 minutes ago 488MB squash v2 2666d7aa527a 46 minutes ago 488MB [root@centos7 squash]# docker version Client: Docker Engine - Community Version: 23.0.4 ... 既然官方已弃用，那就不详细研究了。建议使用多阶段构建。原本镜像层压缩就会造成其他缓存层无法共用，失去了镜像的优势，在镜像较多的情况下无法公用，存储开销是很大的。\n多阶段构建 scratch是一个空镜像，不能拉取也不能运行，里面什么都没有，但可以作为基础镜像构建其他镜像。\n[root@centos7 go]# pwd /root/dockerfile/go [root@centos7 go]# cat hello.go package main import \u0026#34;fmt\u0026#34; func main() { fmt.Println(\u0026#34;hello world.\u0026#34;) } [root@centos7 go]# cat Dockerfile FROM golang COPY hello.go . RUN go build hello.go CMD [\u0026#34;./hello\u0026#34;] [root@centos7 go]# docker run -it --rm go:v1 hello world. # 分阶段构建，这里指定WORKDIR，方便复制可执行文件 [root@centos7 go]# cat Dockerfile FROM golang AS first WORKDIR /app COPY hello.go . RUN go build hello.go FROM scratch COPY --from=first /app/hello . CMD [\u0026#34;./hello\u0026#34;] [root@centos7 go]# docker build -t go:v2 . [root@centos7 go]# docker run -it --rm go:v2 hello world. [root@centos7 go]# docker images | grep go go v2 d9e76072de38 29 seconds ago 1.85MB go v1 c5bb11851550 2 minutes ago 804MB # 使用非scratch镜像 [root@centos7 go]# cat Dockerfile FROM golang WORKDIR /app COPY hello.go . RUN go build hello.go FROM ubuntu COPY --from=0 /app/hello . CMD [\u0026#34;./hello\u0026#34;] [root@centos7 go]# docker build -t go:v3 . [root@centos7 go]# docker images | grep v3 go v3 d1429ea70f10 13 seconds ago 74.6MB 注意到分阶段构建时用了--from=first和--from=0。--from=0表示FROM按序号算（从0开始）的阶段，可以不必指定AS build_name；--from=first这里的first就是构建阶段的别名，用AS指定别名，以供调用。\n优化注意事项 scratch镜像：该镜像什么都没有，没有shell，没有程序链接库（库文件），没有调试工具（ps/top/ping等），所以如果需要这些功能，不推荐使用scratch镜像，可以选用合适的镜像构建，如：busybox、alpine、ubuntu等\nalpine镜像：该镜像标准库是musl libc而不是glibc，程序依赖glibc库时，在alpine中编译时会因为库缺失而报错。除了标准库，alpine对文件系统也进行了精简，Python程序在alpine中运行效率非常低，所以谨慎使用，可以选择其他合适镜像构建，如：busybox:glibc、ubuntu、debian-slim和openjdk:8-jre-alpine，很多镜像发布了自己得alpine版本，也可以选择使用\n生产中是否使用alpine镜像可以参考文章：alpine镜像分析\n镜像精简固然重要，但如果以提升程序复杂度为代价是不妥的。\n针对不同开发语言的镜像精简策略可以参考文章：Docker 镜像制作教程：针对不同语言的精简策略\n","permalink":"https://deemoprobe.github.io/posts/tech/docker/dockerimagesoptimization/","summary":"优化原理 docker镜像层有以下特点（和镜像大小息息相关）： RUN、COPY 和 ADD 指令会在已有镜像层的基础上创建一个新的镜像层，执行指令产生的所有文件系统变更会在指令结束后作为一个镜像层整体提交 镜像层具有copy-on-write的特性，如果去更新其他镜像层中已存在的文件，会先将其","title":"DockerImagesOptimization"},{"content":"Docker架构 Docker是一种C/S架构，Docker Client通过Docker提供的REST API与守护进程Docker Daemon（dockerd）进行通信，Docker客户端和守护进程可以在同一个系统上运行，也可以将Docker客户端连接到远程Docker守护进程。守护进程负责镜像的构建以及docker容器的运行和分发，守护进程在构建镜像过程中获取本地文件时就需要根据指定的上下文路径来查找（也就是本文的重点：context，后面有实例进行分析）。\nDocker Daemon Docker守护进程dockerd，用来监听Docker API的请求和管理Docker对象：镜像、容器、网络和Volume Docker Client Docker客户端，执行docker命令与Docker Daemon（dockerd）进行交互 Docker Registry Docker镜像仓库，Docker默认从公共仓库Docker Hub上查找镜像，当然也可按需搭建私有仓库（如Harbor），客户端使用docker pull或docker run命令时，如果本地未曾拉取相关镜像，就会从配置的Docker镜像仓库中去拉取镜像，使用docker push命令时，会将构建的镜像推送到对应的镜像仓库中 Images Docker镜像，镜像是一个制度模板，带有Docker容器的说明 Containers Docker容器，容器是镜像的可运行实例，可以使用Docker REST API或者CLI来操作容器，容器的实质是进程，容器进程拥有独立的命名空间，如rootfs、网络配置、进程空间、用户ID。容器运行在一个隔离的环境里，这种特性使得容器封装的应用比传统进程更加安全 docker build构建方式 docker build [选项] [上下文路径|URL|-]\n常用选项：-t test:v1为构建的镜像打标签 构建方式（以-t test:v1标签为例）：\ndocker build -t test:v1 . 此时镜像构建文件就在当前目录且名称为Dockerfile docker build -t test:v1 -f /Path/To/Dockerfile [context] 此时Dockerfile可为任意名称，只要内容格式符合Dockerfile标准即可 docker build -t test:v1 [Git Repository] 此时根据git仓库写好的逻辑进行构建 docker build -t test:v1 [URL/test.tar.gz] 根据URL中压缩包进行构建，dockerd自动解压压缩包并以压缩包内容为上下文构建镜像 docker build -t test:v1 - \u0026lt; Dockerfile 不推荐，从标准输入构建，无法指定上下文，无法进行COPY之类操作 docker build -t test:v1 - \u0026lt; [test.tar.gz] 从标准输入的压缩包中构建镜像，dockerd自动解压压缩包并以压缩包内容为上下文构建镜像 # 先把基础镜像拉下来 [root@centos7 tmp]# docker pull alpine # 示例1 [root@centos7 tmp]# cat Dockerfile FROM alpine COPY ./resource.tar / RUN echo \u0026#34;test\u0026#34; [root@centos7 tmp]# docker build -t test:v1 . [+] Building 0.8s (8/8) FINISHED =\u0026gt; [internal] load build definition from Dockerfile 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 147B 0.0s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; [internal] load metadata for docker.io/library/alpine:latest 0.0s =\u0026gt; [internal] load build context 0.2s =\u0026gt; =\u0026gt; transferring context: 10.30MB 0.2s =\u0026gt; [1/3] FROM docker.io/library/alpine 0.0s =\u0026gt; [2/3] COPY ./resource.tar / 0.1s =\u0026gt; [3/3] RUN echo \u0026#34;test\u0026#34; 0.3s =\u0026gt; exporting to image 0.2s =\u0026gt; =\u0026gt; exporting layers 0.2s =\u0026gt; =\u0026gt; writing image sha256:db32566b6ed4e3864f6229709631b8c7eb9b8a29d7c71de7dc7d2fb9046524f2 0.0s =\u0026gt; =\u0026gt; naming to docker.io/library/test:v1 0.0s [root@centos7 tmp]# docker images | grep v1 test v1 db32566b6ed4 36 seconds ago 15.9MB # 示例2 [root@centos7 tmp]# docker build -t test:v2 -f /root/tmp/Dockerfile . [+] Building 0.1s (8/8) FINISHED =\u0026gt; [internal] load build definition from Dockerfile 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 147B 0.0s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; [internal] load metadata for docker.io/library/alpine:latest 0.0s =\u0026gt; [internal] load build context 0.0s =\u0026gt; =\u0026gt; transferring context: 96B 0.0s =\u0026gt; [1/3] FROM docker.io/library/alpine 0.0s =\u0026gt; CACHED [2/3] COPY ./resource.tar / 0.0s =\u0026gt; CACHED [3/3] RUN echo \u0026#34;test\u0026#34; 0.0s =\u0026gt; exporting to image 0.0s =\u0026gt; =\u0026gt; exporting layers 0.0s =\u0026gt; =\u0026gt; writing image sha256:db32566b6ed4e3864f6229709631b8c7eb9b8a29d7c71de7dc7d2fb9046524f2 0.0s =\u0026gt; =\u0026gt; naming to docker.io/library/test:v2 0.0s [root@centos7 tmp]# docker images | grep v2 test v2 db32566b6ed4 2 minutes ago 15.9MB # 示例3，类似下方，但似乎无法拉取，不是重点，略过 [root@centos7 tmp]# docker build -t hello-world https://github.com/docker-library/hello-world.git#master:amd64/hello-world # 示例4 [root@centos7 tmp]# tar -zcvf test.tar.gz resource.tar Dockerfile resource.tar Dockerfile # 上传到我的阿里OSS存储后，使用带压缩包的URL构建 [root@centos7 tmp]# docker build -t test:v4 https://deemoprobe.oss-cn-shanghai.aliyuncs.com/repo/test.tar.gz [+] Building 5.4s (7/7) FINISHED =\u0026gt; [internal] load remote build context 5.0s =\u0026gt; copy /context / 0.2s =\u0026gt; [internal] load metadata for docker.io/library/alpine:latest 0.0s =\u0026gt; [1/3] FROM docker.io/library/alpine 0.0s =\u0026gt; CACHED [2/3] COPY ./resource.tar / 0.0s =\u0026gt; CACHED [3/3] RUN echo \u0026#34;test\u0026#34; 0.0s =\u0026gt; exporting to image 0.0s =\u0026gt; =\u0026gt; exporting layers 0.0s =\u0026gt; =\u0026gt; writing image sha256:8f64891c181eb4171bfcc54ea3d9dd5342a51830aee3d68653e93e14d3cefdca 0.0s =\u0026gt; =\u0026gt; naming to docker.io/library/test:v4 0.0s [root@centos7 tmp]# docker images | grep v4 test v4 8f64891c181e 32 minutes ago 15.9MB # 示例5，由于无法指定上下文，造成COPY失败，无法构建 [root@centos7 tmp]# docker build -t test:v5 - \u0026lt; Dockerfile [+] Building 0.1s (6/7) =\u0026gt; [internal] load build definition from Dockerfile 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 158B 0.0s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; [internal] load metadata for docker.io/library/alpine:latest 0.0s =\u0026gt; [internal] load build context 0.0s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; [1/3] FROM docker.io/library/alpine 0.0s =\u0026gt; ERROR [2/3] COPY ./resource.tar / 0.0s ------ \u0026gt; [2/3] COPY ./resource.tar /: ------ Dockerfile:2 -------------------- 1 | FROM alpine 2 | \u0026gt;\u0026gt;\u0026gt; COPY ./resource.tar / 3 | RUN echo \u0026#34;test\u0026#34; 4 | -------------------- ERROR: failed to solve: failed to compute cache key: failed to calculate checksum of ref moby::3y6r7qr9xy0ob7osrv9tmgc5i: \u0026#34;/resource.tar\u0026#34;: not found # 删除COPY后即可构建成功 [root@centos7 tmp]# cat Dockerfile FROM alpine RUN echo \u0026#34;test\u0026#34; [root@centos7 tmp]# docker build -t test:v5 - \u0026lt; Dockerfile [+] Building 0.6s (6/6) FINISHED =\u0026gt; [internal] load build definition from Dockerfile 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 136B 0.0s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; [internal] load metadata for docker.io/library/alpine:latest 0.0s =\u0026gt; CACHED [1/2] FROM docker.io/library/alpine 0.0s =\u0026gt; [2/2] RUN echo \u0026#34;test\u0026#34; 0.6s =\u0026gt; exporting to image 0.0s =\u0026gt; =\u0026gt; exporting layers 0.0s =\u0026gt; =\u0026gt; writing image sha256:8bb31e67b5d765d567aeb1607e57225b26b447f4e5883f564307dd5e77ff9cc3 0.0s =\u0026gt; =\u0026gt; naming to docker.io/library/test:v5 0.0s [root@centos7 tmp]# docker images | grep v5 test v5 8bb31e67b5d7 38 seconds ago 5.59MB # 示例6 [root@centos7 tmp]# docker build -t test:v6 - \u0026lt; test.tar.gz [+] Building 0.5s (7/7) FINISHED =\u0026gt; [internal] load remote build context 0.3s =\u0026gt; CACHED copy /context / 0.0s =\u0026gt; [internal] load metadata for docker.io/library/alpine:latest 0.0s =\u0026gt; [1/3] FROM docker.io/library/alpine 0.0s =\u0026gt; CACHED [2/3] COPY ./resource.tar / 0.0s =\u0026gt; CACHED [3/3] RUN echo \u0026#34;test\u0026#34; 0.0s =\u0026gt; exporting to image 0.0s =\u0026gt; =\u0026gt; exporting layers 0.0s =\u0026gt; =\u0026gt; writing image sha256:d74fefe3ea1f8761835a8201235571cfad3564d4b2b51d26c3662bebe3957938 0.0s =\u0026gt; =\u0026gt; naming to docker.io/library/test:v6 0.0s [root@centos7 tmp]# docker images | grep v6 test v6 d74fefe3ea1f 43 minutes ago 15.9MB 注意到v6最后构建，时间并不是最新（而是43 minutes ago），因为v1/v2/v4/v6均采用同一个Dockerfile构建，系统已经有缓存，所以直接用了v1的镜像，然后改了个标签。\ncontext上下文 上面构建过程中注意.表示上下文（context）：在镜像构建过程中COPY、ADD和RUN均会产生镜像层，.表示在当前路径，构建过程中文件均在当前目录寻找，指定其他上下文，会在其他上下文寻找，比如：../file/，Dockerfile构建时会在上级目录的file文件夹下找文件。\n[root@centos7 tmp]# tree ./ ./ ├── Dockerfile ├── resource.tar ├── test │ └── Dockerfile └── test.tar.gz 1 directory, 4 files # ./Dockerfile内容 [root@centos7 tmp]# cat Dockerfile FROM alpine COPY resource.tar / RUN echo \u0026#34;test\u0026#34; # ./test/Dockerfile内容 [root@centos7 tmp]# cat ./test/Dockerfile FROM alpine COPY ../resource.tar / RUN echo \u0026#34;test\u0026#34; [root@centos7 tmp]# cd test # 以test目录为上下文路径，build时无法找到resource.tar [root@centos7 test]# docker build -t demo:v1 . [+] Building 0.1s (6/7) =\u0026gt; [internal] load build definition from Dockerfile 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 149B 0.0s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; [internal] load metadata for docker.io/library/alpine:latest 0.0s =\u0026gt; [internal] load build context 0.0s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; [1/3] FROM docker.io/library/alpine 0.0s =\u0026gt; ERROR [2/3] COPY ../resource.tar / 0.0s ------ \u0026gt; [2/3] COPY ../resource.tar /: ------ Dockerfile:2 -------------------- 1 | FROM alpine 2 | \u0026gt;\u0026gt;\u0026gt; COPY ../resource.tar / 3 | RUN echo \u0026#34;test\u0026#34; 4 | -------------------- ERROR: failed to solve: failed to compute cache key: failed to calculate checksum of ref moby::rbutk0he141r0g9cluqn9j2dr: \u0026#34;/resource.tar\u0026#34;: not found # 以上级目录为上下文路径，可见Dockerfile使用的是上下文中的，也就是上级目录 [root@centos7 test]# docker build -t demo:v1 .. [+] Building 0.1s (8/8) FINISHED =\u0026gt; [internal] load build definition from Dockerfile 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 145B 0.0s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; [internal] load metadata for docker.io/library/alpine:latest 0.0s =\u0026gt; [internal] load build context 0.0s =\u0026gt; =\u0026gt; transferring context: 96B 0.0s =\u0026gt; [1/3] FROM docker.io/library/alpine 0.0s =\u0026gt; CACHED [2/3] COPY resource.tar / 0.0s =\u0026gt; CACHED [3/3] RUN echo \u0026#34;test\u0026#34; 0.0s =\u0026gt; exporting to image 0.0s =\u0026gt; =\u0026gt; exporting layers 0.0s =\u0026gt; =\u0026gt; writing image sha256:e1db4e1d4a736862c254cedaa742808b0014088825c214160e820101b6bf5e7c 0.0s =\u0026gt; =\u0026gt; naming to docker.io/library/demo:v1 0.0s # 指定test目录下的Dockerfile，上下文指定为上级目录，构建成功，而且Dockerfile正确 [root@centos7 test]# docker build -t demo:v2 -f ./Dockerfile .. [+] Building 0.1s (8/8) FINISHED =\u0026gt; [internal] load build definition from Dockerfile 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 149B 0.0s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; [internal] load metadata for docker.io/library/alpine:latest 0.0s =\u0026gt; [internal] load build context 0.0s =\u0026gt; =\u0026gt; transferring context: 96B 0.0s =\u0026gt; [1/3] FROM docker.io/library/alpine 0.0s =\u0026gt; CACHED [2/3] COPY ../resource.tar / 0.0s =\u0026gt; CACHED [3/3] RUN echo \u0026#34;test\u0026#34; 0.0s =\u0026gt; exporting to image 0.0s =\u0026gt; =\u0026gt; exporting layers 0.0s =\u0026gt; =\u0026gt; writing image sha256:7a6f61cab683bae940af0ec86c4b8f1a0638dba2a407c36473d51a84f1acfe83 0.0s =\u0026gt; =\u0026gt; naming to docker.io/library/demo:v2 0.0s # 删除上级目录的Dockerfile，进一步验证了上下文路径的作用 [root@centos7 test]# rm -rf ../Dockerfile [root@centos7 test]# pwd /root/tmp/test [root@centos7 test]# docker build -t demo:v3 -f /root/tmp/test/Dockerfile .. [+] Building 0.1s (8/8) FINISHED =\u0026gt; [internal] load build definition from Dockerfile 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 149B 0.0s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; [internal] load metadata for docker.io/library/alpine:latest 0.0s =\u0026gt; [internal] load build context 0.0s =\u0026gt; =\u0026gt; transferring context: 96B 0.0s =\u0026gt; [1/3] FROM docker.io/library/alpine 0.0s =\u0026gt; CACHED [2/3] COPY ../resource.tar / 0.0s =\u0026gt; CACHED [3/3] RUN echo \u0026#34;test\u0026#34; 0.0s =\u0026gt; exporting to image 0.0s =\u0026gt; =\u0026gt; exporting layers 0.0s =\u0026gt; =\u0026gt; writing image sha256:7a6f61cab683bae940af0ec86c4b8f1a0638dba2a407c36473d51a84f1acfe83 0.0s =\u0026gt; =\u0026gt; naming to docker.io/library/demo:v3 0.0s # 修改Dockerfile变更资源文件路径，指定当前目录为上下文 [root@centos7 test]# mkdir demo [root@centos7 test]# cp ../resource.tar demo/ [root@centos7 test]# vim Dockerfile [root@centos7 test]# cat Dockerfile FROM alpine COPY demo/resource.tar / RUN echo \u0026#34;test\u0026#34; [root@centos7 test]# docker build -t demo:v4 . [+] Building 0.4s (8/8) FINISHED =\u0026gt; [internal] load build definition from Dockerfile 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 151B 0.0s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; [internal] load metadata for docker.io/library/alpine:latest 0.0s =\u0026gt; [1/3] FROM docker.io/library/alpine 0.0s =\u0026gt; [internal] load build context 0.2s =\u0026gt; =\u0026gt; transferring context: 10.30MB 0.2s =\u0026gt; CACHED [2/3] COPY demo/resource.tar / 0.0s =\u0026gt; CACHED [3/3] RUN echo \u0026#34;test\u0026#34; 0.0s =\u0026gt; exporting to image 0.0s =\u0026gt; =\u0026gt; exporting layers 0.0s =\u0026gt; =\u0026gt; writing image sha256:3ac067b6d5dcfd571f43c1983627d587e372f140e097eea88825c57996a9c871 0.0s =\u0026gt; =\u0026gt; naming to docker.io/library/demo:v4 0.0s # 指定目录为上下文 [root@centos7 test]# cd .. [root@centos7 tmp]# ls resource.tar test test.tar.gz [root@centos7 tmp]# docker build -t demo:v5 -f test/Dockerfile test [+] Building 0.1s (8/8) FINISHED =\u0026gt; [internal] load build definition from Dockerfile 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 151B 0.0s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; [internal] load metadata for docker.io/library/alpine:latest 0.0s =\u0026gt; [1/3] FROM docker.io/library/alpine 0.0s =\u0026gt; [internal] load build context 0.0s =\u0026gt; =\u0026gt; transferring context: 185B 0.0s =\u0026gt; CACHED [2/3] COPY demo/resource.tar / 0.0s =\u0026gt; CACHED [3/3] RUN echo \u0026#34;test\u0026#34; 0.0s =\u0026gt; exporting to image 0.0s =\u0026gt; =\u0026gt; exporting layers 0.0s =\u0026gt; =\u0026gt; writing image sha256:3ac067b6d5dcfd571f43c1983627d587e372f140e097eea88825c57996a9c871 0.0s =\u0026gt; =\u0026gt; naming to docker.io/library/demo:v5 0.0s # 上下文中有Dockerfile时，不需要指定Dockerfile具体路径 [root@centos7 tmp]# docker build -t demo:v6 test [+] Building 0.1s (8/8) FINISHED =\u0026gt; [internal] load build definition from Dockerfile 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 151B 0.0s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; [internal] load metadata for docker.io/library/alpine:latest 0.0s =\u0026gt; [internal] load build context 0.0s =\u0026gt; =\u0026gt; transferring context: 185B 0.0s =\u0026gt; [1/3] FROM docker.io/library/alpine 0.0s =\u0026gt; CACHED [2/3] COPY demo/resource.tar / 0.0s =\u0026gt; CACHED [3/3] RUN echo \u0026#34;test\u0026#34; 0.0s =\u0026gt; exporting to image 0.0s =\u0026gt; =\u0026gt; exporting layers 0.0s =\u0026gt; =\u0026gt; writing image sha256:2180e7f9ba294fb968084810c4f0b9e77ec79a5f8bdde88d5167ebed41ab4107 0.0s =\u0026gt; =\u0026gt; naming to docker.io/library/demo:v6 0.0s # 如果构建文件名称不为Dockerfile，无论是否存在于上下文，都需要指定 [root@centos7 tmp]# mv test/Dockerfile test/Dockerfile-demo [root@centos7 tmp]# docker build -t demo:v7 test [+] Building 0.1s (2/2) FINISHED =\u0026gt; [internal] load build definition from Dockerfile 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 2B 0.0s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s ERROR: failed to solve: failed to read dockerfile: open /var/lib/docker/tmp/buildkit-mount3807940358/Dockerfile: no such file or directory [root@centos7 tmp]# docker build -t demo:v7 test -f test/Dockerfile-demo [+] Building 0.1s (8/8) FINISHED =\u0026gt; [internal] load build definition from Dockerfile-demo 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 156B 0.0s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; [internal] load metadata for docker.io/library/alpine:latest 0.0s =\u0026gt; [internal] load build context 0.0s =\u0026gt; =\u0026gt; transferring context: 185B 0.0s =\u0026gt; [1/3] FROM docker.io/library/alpine 0.0s =\u0026gt; CACHED [2/3] COPY demo/resource.tar / 0.0s =\u0026gt; CACHED [3/3] RUN echo \u0026#34;test\u0026#34; 0.0s =\u0026gt; exporting to image 0.0s =\u0026gt; =\u0026gt; exporting layers 0.0s =\u0026gt; =\u0026gt; writing image sha256:cd2c1eec8aa54818a6541d798900d95f0c51304604472a0246037ea26f938b35 0.0s =\u0026gt; =\u0026gt; naming to docker.io/library/demo:v7 0.0s ","permalink":"https://deemoprobe.github.io/posts/tech/docker/dockerbuildcontext/","summary":"Docker架构 Docker是一种C/S架构，Docker Client通过Docker提供的REST API与守护进程Docker Daemon（dockerd）进行通信，Docker客户端和守护进程可以在同一个系统上运行，也可以将Docker客户端连接到远程Docker守护进程。守","title":"DockerbuildContext"},{"content":"字段解析 简介 # 主要字段 FROM image[基础镜像,该文件创建新镜像所依赖的镜像] MAINTAINER user\u0026lt;email\u0026gt;[作者姓名和邮箱] RUN command[镜像构建时运行的命令] ADD [文件拷贝进镜像并解压] COPY [文件拷贝进镜像] CMD [容器启动时要运行的命令或参数] ENTRYPOINT [容器启动时要运行的命令] EXPOSE port[声明端口] WORKDIR work_directory[进入容器默认进入的目录] ENV set_env[创建环境变量] VOLUME [容器数据卷,用于数据保存和持久化] ONBUILD [当前Dockerfile构建时不会调用，当子镜像依赖本镜像（FROM）构建时触发ONBUILD后的命令] USER [指定构建镜像和运行容器的用户用户组] ARG [构建镜像时设定的变量] LABEL [为镜像添加元数据] FROM 基础镜像 # 如果不指定版本，默认使用latest FROM image FROM image:tag FROM image@digest # 示例 FROM nginx:1.18.0 MAINTAINER 作者 MAINTAINER user MAINTAINER email MAINTAINER user\u0026lt;email\u0026gt; # 示例 MAINTAINER deemo\u0026lt;deemo@gmail.com\u0026gt; RUN 构建镜像时执行的命令 # Dockerfile里的指令每执行一次会在镜像文件系统中新建一层，为了避免多层文件造成镜像过大，多条命令写在一个RUN后面 # RUN指令创建的中间镜像会被缓存，并会在下次构建中使用。如果不想使用这些缓存镜像，可以在构建时指定--no-cache参数，如：docker build --no-cache RUN command RUN [\u0026#34;\u0026lt;executable\u0026gt;\u0026#34;,\u0026#34;\u0026lt;param1\u0026gt;\u0026#34;,\u0026#34;\u0026lt;param2\u0026gt;\u0026#34;,...] # 示例 RUN yum install -y curl RUN [\u0026#34;./test.php\u0026#34;,\u0026#34;dev\u0026#34;,\u0026#34;offline\u0026#34;] #等价于 RUN ./test.php dev offline ADD 本地文件拷贝进镜像，tar类型的会自动解压 ADD \u0026lt;src\u0026gt; \u0026lt;dest\u0026gt; # 示例 ADD file /dir/ #添加file到/dir/目录 ADD file dir/ #添加file到{WORKDIR}/dir目录 ADD fi* /dir #通配符，添加所有以fi开头的文件到/dir/目录 COPY 本地文件拷贝进镜像，但不会解压 COPY \u0026lt;src\u0026gt; \u0026lt;dest\u0026gt; CMD 容器启动时（docker run时）要运行的命令或参数 # 可以设置多个CMD,但最后一个生效,前面的不生效,也可以被docker run启动容器时后面加的命令替换 CMD [\u0026#34;\u0026lt;executable\u0026gt;\u0026#34;,\u0026#34;\u0026lt;param1\u0026gt;\u0026#34;,\u0026#34;\u0026lt;param2\u0026gt;\u0026#34;,...] #执行可执行文件 CMD [\u0026#34;\u0026lt;param1\u0026gt;\u0026#34;,\u0026#34;\u0026lt;param2\u0026gt;\u0026#34;,...] #已设置ENTRYPOINT，则调用ENTRYPOINT后添加CMD参数 CMD command param1 param2 ... #执行shell内部命令 # 示例 CMD [\u0026#34;/usr/bin/ls\u0026#34;,\u0026#34;-al\u0026#34;] CMD echo \u0026#34;hello\u0026#34; ENTRYPOINT 容器启动时要运行的命令 # 类似于CMD指令，但其不会被docker run的命令行参数指定的指令所覆盖 # 存在多个ENTRYPOINT时，仅最后一个生效 ENTRYPOINT [\u0026#34;\u0026lt;executeable\u0026gt;\u0026#34;,\u0026#34;\u0026lt;param1\u0026gt;\u0026#34;,\u0026#34;\u0026lt;param2\u0026gt;\u0026#34;,...] # 示例 ENTRYPOINT [\u0026#34;nginx\u0026#34;, \u0026#34;-c\u0026#34;] #定参 CMD [\u0026#34;/etc/nginx/nginx.conf\u0026#34;] #变参 EXPOSE 声明容器端口 # EXPOSE仅是声明端口。要使其可访问，需要在docker run运行容器时通过-p来指定端口映射，或通过-P参数来映射EXPOSE端口 EXPOSE \u0026lt;port\u0026gt; [\u0026lt;port\u0026gt;...] # 示例 EXPOSE 80 EXPOSE 80 443 WORKDIR 工作目录 WORKDIR path ENV 设置环境变量 ENV \u0026lt;key\u0026gt; \u0026lt;value\u0026gt; ENV \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt; ... # 示例 ENV dir=/webapp ENV name deemoprobe VOLUME 容器数据卷,用于数据保存和持久化 VOLUME [\u0026#34;/path/to/dir\u0026#34;] # 示例 VOLUME [\u0026#34;/data\u0026#34;] USER 指定构建镜像和运行容器的用户用户组 USER user USER user:group USER uid USER uid:gid USER user:gid USER uid:group # 示例 USER www USER 1080:tomcat ARG 构建镜像时设定的变量 # 与 ENV 作用一致。不过作用域不一样。ARG 设置的环境变量仅对 Dockerfile 内有效，也就是说只有 docker build 的过程中有效，构建好的镜像内不存在此环境变量。 ARG \u0026lt;name\u0026gt;[=\u0026lt;default value\u0026gt;] # 示例 ARG user=www ONBUILD 当构建一个被继承的Dockerfile时运行命令 # 子镜像构建时触发命令并执行。就是Dockerfile里用ONBUILD指定的命令，在本次构建镜像（假设镜像名为test）的过程中不会执行。当有新的Dockerfile使用了该镜像（FROM test），这时执行新镜像的Dockerfile构建时候，会执行test镜像中Dockerfile里的ONBUILD指定的命令。 ONBUILD [INSTRUCTION] # 示例 ONBUILD RUN yum install wget ONBUILD ADD . /data LABEL 为镜像添加元数据 LABEL \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt; \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt; \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt; ... # 示例 LABEL version=\u0026#34;1.0\u0026#34; des=\u0026#34;webapp\u0026#34; 实例1-简单尝试 [root@demo ~]# docker build --help Usage: docker build [OPTIONS] PATH | URL | - Build an image from a Dockerfile Options: --add-host list Add a custom host-to-IP mapping (host:ip) --build-arg list Set build-time variables --cache-from strings Images to consider as cache sources --cgroup-parent string Optional parent cgroup for the container --compress Compress the build context using gzip --cpu-period int Limit the CPU CFS (Completely Fair Scheduler) period --cpu-quota int Limit the CPU CFS (Completely Fair Scheduler) quota -c, --cpu-shares int CPU shares (relative weight) --cpuset-cpus string CPUs in which to allow execution (0-3, 0,1) --cpuset-mems string MEMs in which to allow execution (0-3, 0,1) --disable-content-trust Skip image verification (default true) -f, --file string Name of the Dockerfile (Default is \u0026#39;PATH/Dockerfile\u0026#39;) --force-rm Always remove intermediate containers --iidfile string Write the image ID to the file --isolation string Container isolation technology --label list Set metadata for an image -m, --memory bytes Memory limit --memory-swap bytes Swap limit equal to memory plus swap: \u0026#39;-1\u0026#39; to enable unlimited swap --network string Set the networking mode for the RUN instructions during build (default \u0026#34;default\u0026#34;) --no-cache Do not use cache when building the image --pull Always attempt to pull a newer version of the image -q, --quiet Suppress the build output and print image ID on success --rm Remove intermediate containers after a successful build (default true) --security-opt strings Security options --shm-size bytes Size of /dev/shm -t, --tag list Name and optionally a tag in the \u0026#39;name:tag\u0026#39; format --target string Set the target build stage to build. --ulimit ulimit Ulimit options (default []) # 创建Dockerfile [root@demo ~]# cat Dockerfile FROM centos:centos7.9.2009 # 该镜像已经提前拉取 MAINTAINER deemo\u0026lt;deemo@gmail.com\u0026gt; RUN curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo \u0026amp;\u0026amp; sed -i -e \u0026#39;/mirrors.cloud.aliyuncs.com/d\u0026#39; -e \u0026#39;/mirrors.aliyuncs.com/d\u0026#39; /etc/yum.repos.d/CentOS-Base.repo \u0026amp;\u0026amp; yum install vim -y CMD [\u0026#34;/bin/echo\u0026#34;,\u0026#34;hello\u0026#34;] # 对比父镜像和新镜像，有sed和curl没有vim [root@demo ~]# docker run -it --rm centos:centos7.9.2009 whereis sed sed: /usr/bin/sed [root@demo ~]# docker run -it --rm centos:centos7.9.2009 whereis curl curl: /usr/bin/curl [root@demo ~]# docker run -it --rm centos:centos7.9.2009 whereis vim # 构建镜像 [root@demo ~]# docker build -t centos7:v2 . [+] Building 76.1s (6/6) FINISHED =\u0026gt; [internal] load build definition from Dockerfile 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 408B 0.0s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; [internal] load metadata for docker.io/library/centos:centos7.9.2009 0.0s =\u0026gt; CACHED [1/2] FROM docker.io/library/centos:centos7.9.2009 0.0s =\u0026gt; [2/2] RUN curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo \u0026amp;\u0026amp; sed -i -e /mirrors.clou 72.2s =\u0026gt; exporting to image 3.8s =\u0026gt; =\u0026gt; exporting layers 3.8s =\u0026gt; =\u0026gt; writing image sha256:30f266dbdecb307db7ee31297d333ae19a88f4add15bfd52f17bc6acfeea13f8 0.0s =\u0026gt; =\u0026gt; naming to docker.io/library/centos7:v2 # 查看结果 [root@demo ~]# docker images | grep v2 centos7 v2 30f266dbdecb 3 minutes ago 463MB # 新镜像安装了vim，查看 [root@demo ~]# docker run -it --rm centos7:v2 whereis vim vim: /usr/bin/vim /usr/share/vim # 基于新镜像运行容器输出了hello [root@demo ~]# docker run -it centos7:v2 hello 实例2-构建Tomcat # 准备Dockerfile [root@demo ~]# vim Dockerfile # 基础镜像centos:centos7.9.2009 FROM centos:centos7.9.2009 # 作者签名 MAINTAINER deemoprobe\u0026lt;deemoprobe@gmail.com\u0026gt; # 拷贝宿主机当前目录下文件 COPY tomcat.txt /usr/local/tomcat8.txt # 添加Tomcat安装包并解压至/usr/local ADD apache-tomcat-8.5.53.tar.gz /usr/local # 添加jdk安装包并解压至/usr/local ADD jdk-8u271-linux-x64.tar.gz /usr/local # 安装vim RUN curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo \u0026amp;\u0026amp; sed -i -e \u0026#39;/mirrors.cloud.aliyuncs.com/d\u0026#39; -e \u0026#39;/mirrors.aliyuncs.com/d\u0026#39; /etc/yum.repos.d/CentOS-Base.repo \u0026amp;\u0026amp; yum install vim -y # 设置环境变量 ENV MYPATH /usr/local # 指定工作目录，使用ENV设定的环境变量 WORKDIR $MYPATH # 配置JDK环境 ENV JAVA_HOME /usr/local/jdk1.8.0_271 ENV CLASSPATH $JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar ENV CATALINA_HOME /usr/local/apache-tomcat-8.5.53 ENV CATALINA_BASE /usr/local/apache-tomcat-8.5.53 ENV PATH $PATH:$JAVA_HOME/bin:$CATALINA_HOME/lib:$CATALINA_HOME/bin # 声明端口8080 EXPOSE 8080 # 启动 # ENTRYPOINT [ \u0026#34;/usr/local/apache-tomcat-8.5.53/bin/startup.sh\u0026#34; ] # CMD [ \u0026#34;/usr/local/apache-tomcat-8.5.53/bin/catalina.sh\u0026#34;, \u0026#34;run\u0026#34; ] CMD /usr/local/apache-tomcat-8.5.53/bin/startup.sh \u0026amp;\u0026amp; tail -F /usr/local/apache-tomcat-8.5.53/bin/logs/catalina.out # 准备必要的文件到当前目录 [root@demo ~]# echo \u0026#34;tomcat\u0026#34; \u0026gt;\u0026gt; tomcat.txt # 上传Tomcat和jdk安装包 [root@demo ~]# ls apache-tomcat-8.5.53.tar.gz Dockerfile jdk-8u271-linux-x64.tar.gz tomcat.txt # 构建镜像 [root@demo ~]# docker build -t tomcat8:v1 . [+] Building 50.1s (9/10) =\u0026gt; [internal] load build definition from Dockerfile 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 1.20kB 0.0s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; [internal] load metadata for docker.io/library/centos:centos7.9.2009 0.0s =\u0026gt; CACHED [1/6] FROM docker.io/library/centos:centos7.9.2009 0.0s =\u0026gt; [internal] load build context 4.0s =\u0026gt; =\u0026gt; transferring context: 153.48MB 4.0s =\u0026gt; [2/6] COPY tomcat.txt /usr/local/tomcat8.txt 0.3s =\u0026gt; [3/6] ADD apache-tomcat-8.5.53.tar.gz /usr/local 1.2s =\u0026gt; [4/6] ADD jdk-8u271-linux-x64.tar.gz /usr/local 7.6s =\u0026gt; [5/6] RUN curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo \u0026amp;\u0026amp; sed -i -e /mirrors.clou 48.0s =\u0026gt; [6/6] WORKDIR /usr/local 0.0s =\u0026gt; exporting to image 4.9s =\u0026gt; =\u0026gt; exporting layers 4.9s =\u0026gt; =\u0026gt; writing image sha256:dbccbeed821449bb611ff35b3071547035d925775a8b9d566e888ef6f895c3cf 0.0s =\u0026gt; =\u0026gt; naming to docker.io/library/tomcat8:v1 0.0s # 查看结果 [root@demo ~]# docker images | grep tomcat tomcat8 v1 dbccbeed8214 About a minute ago 833MB # 运行一个容器 # 如果有读写权限问题可以加上--privileged=true [root@demo ~]# docker run -d -p 1080:8080 --name myweb -v /root/web:/usr/local/apache-tomcat-8.5.53/webapps/web -v /root/tomcatlog:/usr/local/apache-tomcat-8.5.53/logs --privileged=true tomcat8:v1 d5f63d3513ba696e54fd7353e52d15a7c2582101d1c71067028c6251f2d82bef [root@demo ~]# docker ps | grep tomcat d5f63d3513ba tomcat8:v1 \u0026#34;/bin/sh -c \u0026#39;/usr/lo…\u0026#34; 17 seconds ago Up 15 seconds 0.0.0.0:1080-\u0026gt;8080/tcp, :::1080-\u0026gt;8080/tcp myweb # 访问Tomcat首页，直接 curl localhost:1080 可以看到返回Tomcat首页的HTML源码 [root@demo ~]# curl -I localhost:1080 HTTP/1.1 200 Content-Type: text/html;charset=UTF-8 Transfer-Encoding: chunked Date: Mon, 10 Jan 2022 10:38:12 GMT # 查看WORKDIR [root@demo ~]# docker exec d5f63d3513ba pwd /usr/local # 查看JDK版本 [root@demo ~]# docker exec d5f63d3513ba java -version java version \u0026#34;1.8.0_271\u0026#34; Java(TM) SE Runtime Environment (build 1.8.0_271-b09) Java HotSpot(TM) 64-Bit Server VM (build 25.271-b09, mixed mode) # 创建web项目，发布服务 [root@demo ~]# ls -l total 149860 -rw-r--r--. 1 root root 10300600 Jan 10 2022 apache-tomcat-8.5.53.tar.gz -rw-r--r--. 1 root root 1073 Jan 10 18:04 Dockerfile -rw-r--r--. 1 root root 143142634 Jan 10 2022 jdk-8u271-linux-x64.tar.gz drwxr-xr-x. 2 root root 197 Jan 10 18:36 tomcatlog -rw-r--r--. 1 root root 7 Jan 10 18:06 tomcat.txt drwxr-xr-x. 2 root root 6 Jan 10 18:36 web [root@demo ~]# cd web [root@demo web]# vim web.jsp \u0026lt;%@ page language=\u0026#34;java\u0026#34; contentType=\u0026#34;text/html; charset=UTF-8\u0026#34; pageEncoding=\u0026#34;UTF-8\u0026#34;%\u0026gt; \u0026lt;!DOCTYPE html PUBLIC \u0026#34;-//W3C//DTD HTML 4.01 Transitional//EN\u0026#34; \u0026#34;http://www.w3.org/TR/html4/loose.dtd\u0026#34;\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta http-equiv=\u0026#34;Content-Type\u0026#34; content=\u0026#34;text/html; charset=UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Insert title here\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; -----------welcome------------ \u0026lt;%=\u0026#34;I am in docker tomcat8\u0026#34;%\u0026gt; \u0026lt;br\u0026gt; \u0026lt;br\u0026gt; \u0026lt;% System.out.println(\u0026#34;=============docker tomcat8\u0026#34;);%\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; [root@demo web]# mkdir WEB-INF [root@demo web]# cd WEB-INF/ [root@demo WEB-INF]# vim web.xml \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;web-app xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns=\u0026#34;http://java.sun.com/xml/ns/javaee\u0026#34; xsi:schemaLocation=\u0026#34;http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd\u0026#34; id=\u0026#34;WebApp_ID\u0026#34; version=\u0026#34;2.5\u0026#34;\u0026gt; \u0026lt;display-name\u0026gt;test-tomcat8\u0026lt;/display-name\u0026gt; \u0026lt;/web-app\u0026gt; # 查看项目结构 [root@demo ~]# yum install tree -y [root@demo ~]# tree . ├── apache-tomcat-8.5.53.tar.gz ├── Dockerfile ├── jdk-8u271-linux-x64.tar.gz ├── tomcatlog │ ├── catalina.2022-01-10.log │ ├── catalina.out │ ├── host-manager.2022-01-10.log │ ├── localhost.2022-01-10.log │ ├── localhost_access_log.2022-01-10.txt │ └── manager.2022-01-10.log ├── tomcat.txt └── web ├── WEB-INF │ └── web.xml └── web.jsp 3 directories, 12 files # 查看容器内数据卷同步结果 [root@centos7 WEB-INF]# docker ps | grep tomcat d5f63d3513ba tomcat8:v1 \u0026#34;/bin/sh -c \u0026#39;/usr/lo…\u0026#34; 5 minutes ago Up 4 minutes 0.0.0.0:1080-\u0026gt;8080/tcp, :::1080-\u0026gt;8080/tcp myweb [root@demo ~]# docker exec d5f63d3513ba ls -l /usr/local/apache-tomcat-8.5.53/webapps/web total 4 drwxr-xr-x. 2 root root 21 Jan 10 10:49 WEB-INF -rw-r--r--. 1 root root 500 Jan 10 10:48 web.jsp [root@demo ~]# docker exec d5f63d3513ba ls -l /usr/local/apache-tomcat-8.5.53/logs total 24 -rw-r-----. 1 root root 7173 Jan 10 10:49 catalina.2022-01-10.log -rw-r-----. 1 root root 7173 Jan 10 10:49 catalina.out -rw-r-----. 1 root root 0 Jan 10 10:36 host-manager.2022-01-10.log -rw-r-----. 1 root root 459 Jan 10 10:36 localhost.2022-01-10.log -rw-r-----. 1 root root 281 Jan 10 10:40 localhost_access_log.2022-01-10.txt -rw-r-----. 1 root root 0 Jan 10 10:36 manager.2022-01-10.log # 重启一下容器 [root@demo ~]# docker restart d5f63d3513ba # 访问结果 [root@demo ~]# curl localhost:1080/web/web.jsp \u0026lt;!DOCTYPE html PUBLIC \u0026#34;-//W3C//DTD HTML 4.01 Transitional//EN\u0026#34; \u0026#34;http://www.w3.org/TR/html4/loose.dtd\u0026#34;\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta http-equiv=\u0026#34;Content-Type\u0026#34; content=\u0026#34;text/html; charset=UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Insert title here\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; -----------welcome------------ I am in docker tomcat8 \u0026lt;br\u0026gt; \u0026lt;br\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; # 查看日志，可以看到访问记录，其他日志文件可以看到Tomcat启动记录等 [root@demo ~]# cd tomcatlog/ [root@demo tomcatlog]# cat localhost_access_log.2022-01-10.txt 172.17.0.1 - - [10/Jan/2022:10:38:12 +0000] \u0026#34;HEAD / HTTP/1.1\u0026#34; 200 - 172.17.0.1 - - [10/Jan/2022:10:38:21 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 11215 172.17.0.1 - - [10/Jan/2022:10:39:49 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 11215 172.17.0.1 - - [10/Jan/2022:10:39:56 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 11215 172.17.0.1 - - [10/Jan/2022:10:58:16 +0000] \u0026#34;GET /web/web.jsp HTTP/1.1\u0026#34; 200 352 参考文档：Docker官方镜像库示例\n","permalink":"https://deemoprobe.github.io/posts/tech/docker/dockerfilegrammer/","summary":"字段解析 简介 # 主要字段 FROM image[基础镜像,该文件创建新镜像所依赖的镜像] MAINTAINER user\u0026lt;email\u0026gt;[作者姓名和邮箱] RUN command[镜像构建时运行的命令] ADD [文件拷贝进镜像并解压] COPY [文件拷贝进镜像] CMD [容器启动时要运行的命令或参数] ENTRYPOINT [容器启动时要运行的命令] EXPOSE","title":"DockerfileGrammer"},{"content":"本文整理了docker常用的一些命令。包括镜像命令，容器命令，日志查看，容器的高级操作以及从容器传输文件到宿主机。\n常用 # 查看docker版本 docker version # 查看docker系统信息 docker info # 实时监控容器的运行情况 docker stats # 查看容器或镜像的底层信息 docker inspect ID/NAME # 查看容器中进程情况 docker top ID/NAME # 查看容器中进程的日志 docker logs ID/NAME # 进入某个容器系统 docker exec -it ID/NAME bash 详细用法 Usage: docker COMMAND A self-sufficient runtime for containers Commands: attach Attach local standard input, output, and error streams to a running container build Build an image from a Dockerfile commit Create a new image from a container changes cp Copy files/folders between a container and the local filesystem create Create a new container diff Inspect changes to files or directories on a container filesystem events Get real time events from the server exec Run a command in a running container export Export a container filesystem as a tar archive history Show the history of an image images List images import Import the contents from a tarball to create a filesystem image info Display system-wide information inspect Return low-level information on Docker objects kill Kill one or more running containers load Load an image from a tar archive or STDIN login Log in to a Docker registry logout Log out from a Docker registry logs Fetch the logs of a container pause Pause all processes within one or more containers port List port mappings or a specific mapping for the container ps List containers pull Pull an image or a repository from a registry push Push an image or a repository to a registry rename Rename a container restart Restart one or more containers rm Remove one or more containers rmi Remove one or more images run Run a command in a new container save Save one or more images to a tar archive (streamed to STDOUT by default) search Search the Docker Hub for images start Start one or more stopped containers stats Display a live stream of container(s) resource usage statistics stop Stop one or more running containers tag Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE top Display the running processes of a container unpause Unpause all processes within one or more containers update Update configuration of one or more containers version Show the Docker version information wait Block until one or more containers stop, then print their exit codes 镜像命令 REPOSITORY: 镜像的仓库源 TAG: 镜像标签 IMAGE ID: 镜像ID CREATED: 镜像已创建时间 SIZE: 镜像大小 docker images Usage: docker images [OPTIONS] [REPOSITORY[:TAG]] List images Options: -a, --all Show all images (default hides intermediate images) --digests Show digests -f, --filter filter Filter output based on conditions provided --format string Pretty-print images using a Go template --no-trunc Do not truncate output -q, --quiet Only show image IDs # 查看镜像 [root@demo ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE tomcat latest fb5657adc892 12 days ago 680MB hello-world latest feb5d9fea6a5 3 months ago 13.3kB centos latest 5d0da3dc9764 3 months ago 231MB # 查看所有镜像 docker images -a # 查看镜像ID docker images -q # 查看所有镜像ID docker images -qa docker search Usage: docker search [OPTIONS] [IMAGE] Search the Docker Hub for images Options: -f, --filter filter Filter output based on conditions provided --format string Pretty-print search using a Go template --limit int Max number of search results (default 25) --no-trunc Do not truncate output # 从Docker Hub上查询已存在镜像 [root@demo ~]# docker search nginx NAME DESCRIPTION STARS OFFICIAL AUTOMATED nginx Official build of Nginx. 16062 [OK] jwilder/nginx-proxy Automated Nginx reverse proxy for docker con… 2105 [OK] richarvey/nginx-php-fpm Container running Nginx + PHP-FPM capable of… 819 [OK] jc21/nginx-proxy-manager Docker container for managing Nginx proxy ho… 303 linuxserver/nginx An Nginx container, brought to you by LinuxS… 161 tiangolo/nginx-rtmp Docker image with Nginx using the nginx-rtmp… 148 [OK] ... # 根据stars数目来搜索IMAGE # 查看800星以上的nginx镜像 [root@demo ~]# docker search nginx -f stars=800 NAME DESCRIPTION STARS OFFICIAL AUTOMATED nginx Official build of Nginx. 16062 [OK] jwilder/nginx-proxy Automated Nginx reverse proxy for docker con… 2105 [OK] richarvey/nginx-php-fpm Container running Nginx + PHP-FPM capable of… 819 [OK] # 搜索800星以上的nginx镜像，并且不切割摘要信息（摘要全部显示） [root@demo ~]# docker search nginx --no-trunc -f stars=800 NAME DESCRIPTION STARS OFFICIAL AUTOMATED nginx Official build of Nginx. 16062 [OK] jwilder/nginx-proxy Automated Nginx reverse proxy for docker containers 2105 [OK] richarvey/nginx-php-fpm Container running Nginx + PHP-FPM capable of pulling application code from git 819 [OK] docker pull Usage: docker pull [OPTIONS] NAME[:TAG|@DIGEST] Pull an image or a repository from a registry Options: -a, --all-tags Download all tagged images in the repository --disable-content-trust Skip image verification (default true) --platform string Set platform if server is multi-platform capable -q, --quiet Suppress verbose output # 从配置好的仓库拉取镜像, 未配置的话默认从Docker Hub上获取 docker pull IMAGE \u0026lt;==\u0026gt; docker pull IMAGE:latest # 拉取指定版本镜像 docker pull IMAGE:TAG docker rmi Usage: docker rmi [OPTIONS] IMAGE [IMAGE...] Remove one or more images Options: -f, --force Force removal of the image --no-prune Do not delete untagged parents # 删除最新版本镜像 docker rmi IMAGE \u0026lt;==\u0026gt; docker rmi IMAGE:latest # 删除指定版本镜像 docker rmi IMAGE:TAG # -f 强制删除镜像(可删除多层镜像) docker rmi -f IMAGE # 指定镜像ID进行删除 docker rmi -f IMAGE_ID # 删除多个镜像 docker rmi -f IMAGE1 IMAGE2 IMAGE3 # 删除所有镜像 docker rmi -f $(docker images -qa) # 实例 [root@demo ~]# docker rmi hello-world Untagged: hello-world:latest Untagged: hello-world@sha256:2498fce14358aa50ead0cc6c19990fc6ff866ce72aeb5546e1d59caac3d0d60f Deleted: sha256:feb5d9fea6a5e9606aa995e879d862b825965ba48de054caab5ef356dc6b3412 Deleted: sha256:e07ee1baac5fae6a26f30cabfe54a36d3402f96afda318fe0a96cec4ca393359 dangling悬挂镜像 仓库名、标签均为的镜像被称为悬挂镜像，这种镜像已经失去了存在的价值。出现悬挂镜像原因一般是在docker pull *:latest时产生。当新版本发布后重新pull，镜像名会被新镜像所占用，旧镜像的名字会变成。\n# 删除悬挂镜像（dangling），-f不显示提示确认信息，直接强制删除 [root@harbor ~]# docker image prune WARNING! This will remove all dangling images. Are you sure you want to continue? [y/N] # 删除所有未使用的镜像（即未运行容器的镜像），-f不显示提示确认信息，直接强制删除 [root@harbor ~]# docker image prune -a WARNING! This will remove all images without at least one container associated to them. Are you sure you want to continue? [y/N] 容器命令 docker run docker run [OPTIONS] IMAGE [COMMAND] [ARG...] OPTIONS字段说明: - --name 自定义容器名 - -d 后台运行容器 - -it 新建伪终端交互运行容器 - -P 随机分配端口映射 - -p 指定端口映射, 有以下四种方式 - ip:hostPort:containerPort - ip::containerPort - hostPort:containerPort - containerPort # 先获取镜像 [root@demo ~]# docker pull centos [root@demo ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE tomcat latest fb5657adc892 12 days ago 680MB centos latest 5d0da3dc9764 3 months ago 231MB # 启动运行CentOS容器(本地有该镜像就直接启动, 没有就自动拉取) [root@demo ~]# docker run -it centos [root@f75fd428066f /]# cat /etc/redhat-release CentOS Linux release 8.4.2105 [root@f75fd428066f /]# exit [root@demo ~]# docker run -it --namecentos8 centos [root@dd16aaf5cbcd /]# exit [root@demo ~]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES dd16aaf5cbcd centos \u0026#34;/bin/bash\u0026#34; 58 seconds ago Exited (0) 52 seconds ago centos8 f75fd428066f centos \u0026#34;/bin/bash\u0026#34; 3 minutes ago Exited (0) 2 minutes ago silly_engelbart # 为nginx镜像随机分配端口映射 [root@demo ~]# docker run -it -P nginx:1.18.0 Unable to find image \u0026#39;nginx:1.18.0\u0026#39; locally 1.18.0: Pulling from library/nginx f7ec5a41d630: Pull complete 0b20d28b5eb3: Pull complete 1576642c9776: Pull complete c12a848bad84: Pull complete 03f221d9cf00: Pull complete Digest: sha256:e90ac5331fe095cea01b121a3627174b2e33e06e83720e9a934c7b8ccc9c55a0 Status: Downloaded newer image for nginx:1.18.0 /docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration /docker-entrypoint.sh: Looking for bash scripts in /docker-entrypoint.d/ /docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh 10-listen-on-ipv6-by-default.sh: Getting the checksum of /etc/nginx/conf.d/default.conf 10-listen-on-ipv6-by-default.sh: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf /docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh /docker-entrypoint.sh: Configuration complete; ready for start up # 另起一个终端，查看分配的端口 [root@demo ~]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 97c9f5f24db2 nginx:1.18.0 \u0026#34;/docker-entrypoint.…\u0026#34; 42 seconds ago Up 40 seconds 0.0.0.0:49153-\u0026gt;80/tcp, :::49153-\u0026gt;80/tcp jovial_burnell # 指定端口映射 [root@demo ~]# docker run -it --name nginx-web -p 8080:80 nginx:1.18.0 /docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration /docker-entrypoint.sh: Looking for bash scripts in /docker-entrypoint.d/ /docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh 10-listen-on-ipv6-by-default.sh: Getting the checksum of /etc/nginx/conf.d/default.conf 10-listen-on-ipv6-by-default.sh: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf /docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh /docker-entrypoint.sh: Configuration complete; ready for start up # 另起一个终端查看容器信息 [root@demo ~]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7e243e46f6f6 nginx:1.18.0 \u0026#34;/docker-entrypoint.…\u0026#34; 23 seconds ago Up 22 seconds 0.0.0.0:8080-\u0026gt;80/tcp, :::8080-\u0026gt;80/tcp nginx-web # 访问nginx [root@demo ~]# curl localhost:8080 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; # 在第一个终端可看到访问日志 172.17.0.1 - - [04/Jan/2022:01:08:26 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 612 \u0026#34;-\u0026#34; \u0026#34;curl/7.29.0\u0026#34; \u0026#34;-\u0026#34; # 浏览器访问 docker ps Usage: docker ps [OPTIONS] List containers Options: -a, --all Show all containers (default shows just running) -f, --filter filter Filter output based on conditions provided --format string Pretty-print containers using a Go template -n, --last int Show n last created containers (includes all states) (default -1) -l, --latest Show the latest created container (includes all states) --no-trunc Do not truncate output -q, --quiet Only display container IDs -s, --size Display total file sizes # 查看在运行容器 [root@demo ~]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7e243e46f6f6 nginx:1.18.0 \u0026#34;/docker-entrypoint.…\u0026#34; 3 minutes ago Up 3 minutes 0.0.0.0:8080-\u0026gt;80/tcp, :::8080-\u0026gt;80/tcp nginx-web # 查看在运行容器ID [root@demo ~]# docker ps -q 7e243e46f6f6 # 查看所有容器（包含已退出的容器） [root@demo ~]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7e243e46f6f6 nginx:1.18.0 \u0026#34;/docker-entrypoint.…\u0026#34; 3 minutes ago Up 3 minutes 0.0.0.0:8080-\u0026gt;80/tcp, :::8080-\u0026gt;80/tcp nginx-web 97c9f5f24db2 nginx:1.18.0 \u0026#34;/docker-entrypoint.…\u0026#34; 5 minutes ago Exited (0) 4 minutes ago jovial_burnell dd16aaf5cbcd centos \u0026#34;/bin/bash\u0026#34; 10 minutes ago Exited (0) 10 minutes ago centos8 f75fd428066f centos \u0026#34;/bin/bash\u0026#34; 12 minutes ago Exited (0) 11 minutes ago silly_engelbart # 查看所有容器ID（包含已退出的容器） [root@demo ~]# docker ps -qa 7e243e46f6f6 97c9f5f24db2 dd16aaf5cbcd f75fd428066f # 查看最近使用的两个容器 [root@demo ~]# docker ps -n 2 CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7e243e46f6f6 nginx:1.18.0 \u0026#34;/docker-entrypoint.…\u0026#34; 5 minutes ago Up 5 minutes 0.0.0.0:8080-\u0026gt;80/tcp, :::8080-\u0026gt;80/tcp nginx-web 97c9f5f24db2 nginx:1.18.0 \u0026#34;/docker-entrypoint.…\u0026#34; 7 minutes ago Exited (0) 6 minutes ago jovial_burnell # 查看最近一次启动的容器 [root@demo ~]# docker ps -l CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7e243e46f6f6 nginx:1.18.0 \u0026#34;/docker-entrypoint.…\u0026#34; 5 minutes ago Up 5 minutes 0.0.0.0:8080-\u0026gt;80/tcp, :::8080-\u0026gt;80/tcp nginx-web # 查看容器的大小 [root@demo ~]# docker ps -s CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES SIZE 7e243e46f6f6 nginx:1.18.0 \u0026#34;/docker-entrypoint.…\u0026#34; 6 minutes ago Up 6 minutes 0.0.0.0:8080-\u0026gt;80/tcp, :::8080-\u0026gt;80/tcp nginx-web 1.12kB (virtual 133MB) 容器启停 # 启动已停止的容器 docker start 容器名或ID # 重启容器 docker restart 容器名或ID # 停止容器 docker stop 容器名或ID # 强制停止容器 docker kill 容器名或ID # 实例 [root@demo ~]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7e243e46f6f6 nginx:1.18.0 \u0026#34;/docker-entrypoint.…\u0026#34; 7 minutes ago Exited (0) 8 seconds ago nginx-web 97c9f5f24db2 nginx:1.18.0 \u0026#34;/docker-entrypoint.…\u0026#34; 9 minutes ago Exited (0) 8 minutes ago jovial_burnell dd16aaf5cbcd centos \u0026#34;/bin/bash\u0026#34; 14 minutes ago Exited (0) 14 minutes ago centos8 f75fd428066f centos \u0026#34;/bin/bash\u0026#34; 16 minutes ago Exited (0) 15 minutes ago silly_engelbart [root@demo ~]# docker start 7e243e46f6f6 7e243e46f6f6 [root@demo ~]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7e243e46f6f6 nginx:1.18.0 \u0026#34;/docker-entrypoint.…\u0026#34; 7 minutes ago Up 4 seconds 0.0.0.0:8080-\u0026gt;80/tcp, :::8080-\u0026gt;80/tcp nginx-web [root@demo ~]# docker stop nginx-web nginx-web [root@demo ~]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 删除容器 # 删除已停止容器 docker rm 容器名或ID # 强制删除(若在运行,也会强制停止后删除) docker rm -f 容器名或ID # 删除全部容器 docker rm -f $(docker ps -qa) or docker ps -qa | xargs docker rm # 实例 [root@demo ~]# docker rm $(docker ps -qa) 7e243e46f6f6 97c9f5f24db2 dd16aaf5cbcd f75fd428066f [root@demo ~]# docker ps -qa 进入正在运行的容器 Usage: docker exec [OPTIONS] CONTAINER COMMAND [ARG...] Run a command in a running container Options: -d, --detach Detached mode: run command in the background --detach-keys string Override the key sequence for detaching a container -e, --env list Set environment variables --env-file list Read in a file of environment variables -i, --interactive Keep STDIN open even if not attached --privileged Give extended privileges to the command -t, --tty Allocate a pseudo-TTY -u, --user string Username or UID (format: \u0026lt;name|uid\u0026gt;[:\u0026lt;group|gid\u0026gt;]) -w, --workdir string Working directory inside the container # 进入正在运行的容器并启用交互 docker exec -it 容器ID /bin/bash # 不进入正在运行的容器直接交互,比如查看根目录 docker exec -it 容器ID ls -al / exit # 退出 # 实例 # 后台启一个nginx [root@demo ~]# docker run -itd --nametest_exec nginx Unable to find image \u0026#39;nginx:latest\u0026#39; locally latest: Pulling from library/nginx a2abf6c4d29d: Pull complete a9edb18cadd1: Pull complete 589b7251471a: Pull complete 186b1aaa4aa6: Pull complete b4df32aa5a72: Pull complete a0bcbecc962e: Pull complete Digest: sha256:0d17b565c37bcbd895e9d92315a05c1c3c9a29f762b011a10c54a66cd53c9b31 Status: Downloaded newer image for nginx:latest 864a1bdcf178ae110817a3d2f1d9cbf3b4f6d9bba0d7e477b971c403b5281e8a [root@demo ~]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 864a1bdcf178 nginx \u0026#34;/docker-entrypoint.…\u0026#34; 28 seconds ago Up 27 seconds 80/tcp test_exec # 进入容器 [root@demo ~]# docker exec -it test_exec /bin/bash root@864a1bdcf178:/# exit # 不进入容器执行命令，查看/bin目录下文件数量 [root@demo ~]# docker exec -it test_exec ls -al /bin | wc -l 72 退出容器 exit（等价于Ctrl+D） 退出并关闭容器(适用于docker run命令启动的容器, docker exec 进入容器exit退出后不影响容器状态)\n一般需要后台运行的容器可以使用-d先后台启动，需要交互时exec进入容器进行交互。\n容器命令高级操作 docker单独启动容器作为守护进程(后台运行), 启动后docker ps -a会发现已经退出了\n原因是：docker容器运行机制决定,docker容器后台运行就必须要有一个前台进程,否则会自动退出\n所以要解决这个问题就是将要运行的进程以前台进程的形式运行（或者交互模式启动 -itd）\n# 启动容器作为守护进程,这样会直接退出 docker run -d 镜像名 # 后台运行并每两秒在前台输出一次hello docker run -d centos /bin/sh -c \u0026#34;while true;do echo hello;sleep 2;done\u0026#34; # 查看日志, 列出时间, 动态打印日志, 保留之前num行 docker logs -f -t --tail num 容器ID # 实例 # 先删除所有容器 [root@demo ~]# docker rm $(docker ps -qa) -f f0b8817bb234 864a1bdcf178 # 后台启动一个centos，发现启动后会直接退出 [root@demo ~]# docker run -d centos acfd747d7ec4a942374bb526d41d072fe4840e3ba3d1255e67bdee5c2399513a [root@demo ~]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES acfd747d7ec4 centos \u0026#34;/bin/bash\u0026#34; 6 seconds ago Exited (0) 6 seconds ago dreamy_wilbur # 加入前台进程的在运行 [root@demo ~]# docker run -d centos /bin/sh -c \u0026#34;while true;do echo hello;sleep 2;done\u0026#34; 000da5e0a32581d4c65cb6a64292010f86feceac8ebea3f715f2d972fa7c7065 [root@demo ~]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 000da5e0a325 centos \u0026#34;/bin/sh -c \u0026#39;while t…\u0026#34; 3 seconds ago Up 2 seconds xenodochial_shamir acfd747d7ec4 centos \u0026#34;/bin/bash\u0026#34; 52 seconds ago Exited (0) 52 seconds ago dreamy_wilbur # 每两秒打印一次 [root@demo ~]# docker logs 000da5e0a325 hello hello hello hello ... # 查看容器内运行的进程 docker top 容器ID [root@demo ~]# docker top 000da5e0a325 UID PID PPID C STIME TTY TIME CMD root 10408 10388 0 09:33 ? 00:00:00 /bin/sh -c while true;do echo hello;sleep 2;done root 10549 10408 0 09:35 ? 00:00:00 /usr/bin/coreutils --coreutils-prog-shebang=sleep /usr/bin/sleep 2 # 容器内传输数据到宿主机 docker cp 容器ID:/path /宿主机path # 进入一个容器，在根目录下创建文件并拷贝到宿主机根目录 [root@demo ~]# docker exec -it 000da5e0a325 /bin/bash [root@000da5e0a325 /]# echo test \u0026gt;\u0026gt; test.txt [root@000da5e0a325 /]# cat test.txt test [root@000da5e0a325 /]# exit exit [root@demo ~]# docker cp 000da5e0a325:/test.txt / [root@demo ~]# cat /test.txt test 镜像的定制 # 如果该容器内部做了更改，提交打包后更改也包含进去，以此完成镜像的定制 docker commit -a=\u0026#34;作者名\u0026#34; -m=\u0026#34;提交信息\u0026#34; 容器ID 定制后的镜像名 # 启动一个容器，自定义端口映射，基于nginx:1.18.0镜像 [root@demo ~]# docker run -d -p 8080:80 --name nginx1.18.0 nginx:1.18.0 02c275a9254c7714d8187dc35efabf8859b245272b1ef101c4411b68aa85d9c3 [root@demo ~]# docker ps --no-trunc CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 02c275a9254c7714d8187dc35efabf8859b245272b1ef101c4411b68aa85d9c3 nginx:1.18.0 \u0026#34;/docker-entrypoint.sh nginx -g \u0026#39;daemon off;\u0026#39;\u0026#34; 29 seconds ago Up 28 seconds 0.0.0.0:8080-\u0026gt;80/tcp, :::8080-\u0026gt;80/tcp nginx1.18.0 # 定制新镜像并打上tag为1.18.0 [root@demo ~]# docker commit -a=\u0026#34;deemoprobe\u0026#34; -m=\u0026#34;nginx:1.18.0 8080-\u0026gt;80\u0026#34; 02c275a9254c7714d8187dc35efabf8859b245272b1ef101c4411b68aa85d9c3 deemoprobe/nginx:1.18.0 sha256:00b7979f0210c4ebde226fb86d789a4caef7276b2d92304b3942ef34ce733a96 # 查看新的镜像已生成 [root@demo ~]# docker images | grep deemoprobe deemoprobe/nginx 1.18.0 00b7979f0210 28 seconds ago 133MB # 以提交到docker hub为例 # 首先要创建docker hub账户，然后建立一个新仓库 # 登陆docker hub [root@demo ~]# docker login ... Login Succeeded # 推送 [root@demo ~]# docker push deemoprobe/nginx:1.18.0 The push refers to repository [docker.io/deemoprobe/nginx] 4fa6704c8474: Mounted from library/nginx 4fe7d87c8e14: Mounted from library/nginx 6fcbf7acaafd: Mounted from library/nginx f3fdf88f1cb7: Mounted from library/nginx 7e718b9c0c8c: Mounted from library/nginx 1.18.0: digest: sha256:2db445abcd9b126654035448cada7817300d646a27380916a6b6445e8ede699b size: 1362 # docker hub上就能查看到nginx镜像仓库，并且标签为1.18.0 # 拉下来查看 [root@demo ~]# docker pull deemoprobe/nginx:1.18.0 1.18.0: Pulling from deemoprobe/nginx f7ec5a41d630: Already exists 0b20d28b5eb3: Already exists 1576642c9776: Already exists c12a848bad84: Already exists 03f221d9cf00: Already exists Digest: sha256:2db445abcd9b126654035448cada7817300d646a27380916a6b6445e8ede699b Status: Downloaded newer image for deemoprobe/nginx:1.18.0 docker.io/deemoprobe/nginx:1.18.0 [root@demo ~]# docker images deemoprobe/nginx:1.18.0 REPOSITORY TAG IMAGE ID CREATED SIZE deemoprobe/nginx 1.18.0 b5fd6cb4ca9e 20 minutes ago 133MB # 或者说直接将拉取的镜像保存在自己的仓库里，可以直接打标签上传 # 比如拷贝cnych/ingress-nginx-defaultbackend镜像并上传至自己的DockerHub # 前提是已经登陆成功 [root@k8s-node01 ~]# docker images | grep 1.5 cnych/ingress-nginx-defaultbackend 1.5 b5af743e5984 3 years ago 5.13MB [root@k8s-node01 ~]# docker tag cnych/ingress-nginx-defaultbackend:1.5 deemoprobe/defaultbackend:1.5 [root@k8s-node01 ~]# docker push deemoprobe/defaultbackend:1.5 The push refers to repository [docker.io/deemoprobe/defaultbackend] b108d4968233: Mounted from cnych/ingress-nginx-defaultbackend 1.5: digest: sha256:4dc5e07c8ca4e23bddb3153737d7b8c556e5fb2f29c4558b7cd6e6df99c512c7 size: 528 [root@k8s-node01 ~]# docker images | grep 1.5 cnych/ingress-nginx-defaultbackend 1.5 b5af743e5984 3 years ago 5.13MB deemoprobe/defaultbackend 1.5 b5af743e5984 3 years ago 5.13MB 高级命令 docker run进阶 # 临时运行容器，用完即删不产生容器docker ps记录 # 临时开启容器查看镜像的WORKDIR docker run -it --rm IMAGE pwd # 临时开启容器进入查看镜像内容 docker run -it --rm IMAGE /bin/sh 查看docker配置信息 # 查看容器详情信息的某个字段 docker inspect -f \u0026#34;{{ .首字段.子字段 }}\u0026#34; \u0026lt;ContainerNameOrId\u0026gt; # 查看容器IP地址 [root@demo ~]# docker inspect -f \u0026#34;{{ .NetworkSettings.IPAddress }}\u0026#34; 38798985efb9 172.17.0.2 # 查看容器主机名 [root@demo ~]# docker inspect -f \u0026#34;{{ .Config.Hostname }}\u0026#34; 38798985efb9 38798985efb9 # 查看开放的端口 [root@demo ~]# docker inspect -f \u0026#34;{{ .Config.ExposedPorts }}\u0026#34; 38798985efb9 map[80/tcp:{}] 查看网络 # 启动并开放nginx80端口，80端口映射到主机的1234端口 [root@demo ~]# docker run -p 1234:80 -d nginx 03694540d34be5f69d951f15316dbdeae63fdc60a09e1da078273d5e15cb74ff [root@demo ~]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 03694540d34b nginx \u0026#34;/docker-entrypoint.…\u0026#34; 4 seconds ago Up 2 seconds 0.0.0.0:1234-\u0026gt;80/tcp nifty_williams # 查看端口映射关系 [root@demo ~]# docker port 03694540d34b 80 0.0.0.0:1234 # 查看nat规则 [root@demo ~]# iptables -t nat -nL ... Chain DOCKER (2 references) target prot opt source destination RETURN all -- 0.0.0.0/0 0.0.0.0/0 DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:1234 to:172.17.0.3:80 ... # 若容器内部访问不了外网，检查ip_forward和SNAT/MASQUERADE # 开启ip_forward [root@demo ~]# sysctl net.ipv4.ip_forward=1 net.ipv4.ip_forward = 1 # 查看SNAT/MASQUERADE是否是ACCEPT [root@demo ~]# iptables -t nat -nL Chain POSTROUTING (policy ACCEPT) target prot opt source destination # 这条规则指定从容器内出来的包都要进行一次地址转换 MASQUERADE all -- 172.17.0.0/16 0.0.0.0/0 ... ","permalink":"https://deemoprobe.github.io/posts/tech/docker/dockercli/","summary":"本文整理了docker常用的一些命令。包括镜像命令，容器命令，日志查看，容器的高级操作以及从容器传输文件到宿主机。 常用 # 查看docker版本 docker version # 查看docker系统信息 docker info # 实时监控容器的运行情况 docker stats # 查看容器或镜像的底层信息 docker inspect ID/NAME # 查看容器中进程情况 docker top ID/NAME # 查看容器中进程的日","title":"DockerCommandLineInterface"},{"content":"环境说明 操作系统：CentOS Linux release 7.9.2009 (Core) 操作用户：root 内核版本：5.18.12-1 涉及技术：Linux、Docker、Vm 容器技术的基石 Linux内核功能中，Namespaces和Cgroups技术是实现轻量级进程虚拟化的基础，是Linux Container的基石，这两个Linux子系统主要作用是管理和隔离进程资源。\nNamespaces作用：封装抽象成多种namespace，用于限制和隔离进程间通信、进程号、网络以及挂载点等资源。更多Namespaces内容可见博客：Linux Namespaces Cgroups作用：管理资源的分配、限制资源的使用量。更多Cgroups内容可见博客：Linux Cgroups 轻量级进程虚拟化：经过虚拟化的进程看起来和Linux系统中其他进程一样，都是以进程的形式运行在主机中。操作系统可以承载大量的这类轻量级进程，这些轻量级进程共用一个Linux内核。\n容器和虚拟机 容器技术溯源可以追溯到chroot这个Unix/Linux命令，它可以创造出一个与文件系统隔离的环境，这种环境叫做chroot jail，这种环境真实存在，但又不会被外部的进程访问，起到了访问隔离的作用。\n容器技术、虚拟化技术（不论何种抽象层次下的虚拟化技术）都能做到资源层面上的隔离和限制。\nHypervisor是创建和运行虚拟机的管理程序，有两种类型，一种是直接在裸服务器硬件上工作；另一种是在操作系统之上工作。传统虚拟机通常采用这种技术进行虚拟化。\n如今，虚拟机和容器都能带来很好的隔离效果，相对来说虚拟机会带来一些开销，无论是启动时间、大小还是运行操作系统的资源使用。容器实际上是进程，启动速度更快，占用空间更小。如果需要更为彻底的隔离，虚拟机不失为一种选择。综合考虑开销、部署响应速度和资源利用率，容器技术更适合云原生架构。\n主要对比如下表：\n对比项 容器 虚拟机 开机时间 秒级 分钟级 运行机制 container-runtime hypervisor 内存使用 占用很小 占用较大 隔离强度 较弱 很强 部署时长 很短 较长 使用 较为复杂 简易 相比于系统级虚拟化，容器技术是进程级别的，具有启动快、体积小等优势，为软件开发、应用部署带来了诸多便利。如使用开源容器Docker技术，应用程序打包推送到镜像中心后，使用时拉取直接运行，实现了“一次打包，到处运行”，非常方便、快捷；使用开源容器编排技术K8S能够实现应用程序的弹性扩容和自动化部署，满足企业灵活扩展信息系统的需求。但是，随着Docker和K8S应用的日益广泛和深入，安全问题也越来越凸显。\nDocker容器技术应用中可能存在的技术性安全风险分为镜像安全风险、容器虚拟化安全风险、网络安全风险等类型。\nDocker Hub中的镜像可由个人开发者上传，其数量丰富、版本多样，但质量参差不齐，甚至存在包含恶意漏洞的恶意镜像，因而可能存在较大的安全风险。具体而言，Docker镜像的安全风险分布在创建过程、获取来源、获取途径等方方面面。\n与传统虚拟机相比，Docker容器不拥有独立的资源配置，且没有做到操作系统内核层面的隔离，因此可能存在资源隔离不彻底与资源限制不到位所导致的容器虚拟化安全风险。\n网络安全风险是互联网中所有信息系统所面临的重要风险，不论是物理设备还是虚拟机，都存在难以完全规避的网络安全风险问题。而在轻量级虚拟化的容器网络和容器编排环境中，其网络安全风险较传统网络而言更为复杂严峻。\n上面主要是Docker容器技术面临的安全风险，当然如今容器运行时已经不止Docker一种（诸如：containerd、CRI-O等），但面临的安全风险是同样的，都需要引起同样的关注和安全风险的评估。\n镜像 镜像是一种轻量级、独立可执行的软件包，用来打包软件运行环境和基于该环境开发的软件, 包括代码、运行时、库、环境变量和配置文件等。\n镜像的特点 Docker镜像都是只读的，当容器启动时，一个新的可写层被加载到镜像的顶部。这一层通常被称作为\u0026quot;容器层\u0026quot;，“容器层”之下的都叫\u0026quot;镜像层\u0026quot;。\nUnionFS(联合文件系统) UnionFS(联合文件系统): Union文件系统(UnionFS)是一种分层、轻量级并且高性能的文件系统，它支持对文件系统的修改作为一次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下(unite several directories into a single virtual filesystem)Union文件系统是Docker镜像的基础。镜像可以通过分层来进行继承, 基于基础镜像(没有父镜像)， 可以制作各种具体的应用镜像。\n特性: 一次同时加载多个文件系统，但从外面看起来，只能看到一个文件系统，联合加载会把各层文件系统叠加起来，这样最终的文件系统会包含所有底层的文件和目录。\nDocker镜像加载原理 docker的镜像实际上由一层一层的文件系统组成，这种层级的文件系统UnionFS。\nbootfs(boot file system),主要包含bootloader和kernel，bootloader主要是引导加载kernel，Linux刚启动时会加载bootfs文件系统，在Docker镜像的最底层是bootfs。这一层与我们典型的Linux/Unix系统是一样的, 包含boot加载器和内核。当boot加载完成之后整个内核就都在内存中了，此时内存的使用权已由bootfs转交给内核，此时系统也会卸载bootfs。\nrootfs(root file system),在bootfs之上。包含的就是典型Linux系统中的/dev, /proc, /bin, /etc等标准目录和文件。rootfs就是各种不同的操作系统发行版，比如Ubuntu，Centos等等。\n虚拟机的CentOS一般是几个G，docker这里231M\n对于一个精简的OS，rootfs可以很小，只需要包括最基本的命令、工具和程序库就可以了，因为底层直接用Host的kernel，自己只需要提供rootfs就行了。由此可见对于不同的linux发行版，bootfs基本是一致的，rootfs会有差别，因此不同的发行版可以共用bootfs。\n分层的镜像 以docker pull为例，在下载的过程中可以看到docker的镜像是在一层一层的在下载\nDocker镜像采用分层结构最大的一个好处就是共享资源。比如：有多个镜像都从相同的base镜像构建而来，那么宿主机只需在磁盘上保存一份base镜像,同时内存中也只需加载一份base镜像，就可以为所有容器服务了。而且镜像的每一层都可以被共享。\n镜像大小优化 RUN、COPY 和 ADD 指令会在已有镜像层的基础上创建一个新的镜像层，执行指令产生的所有文件系统变更会在指令结束后作为一个镜像层整体提交。 镜像层具有 copy-on-write 的特性，如果去更新其他镜像层中已存在的文件，会先将其复制到新的镜像层中再修改，造成双倍的文件空间占用。 如果去删除其他镜像层的一个文件，只会在当前镜像层生成一个该文件的删除标记，并不会减少整个镜像的实际体积。\nDocker部署 官方参考文档 个人博客文档\n数据卷 Docker 镜像是由多个文件系统（只读层）叠加而成。当我们启动一个容器的时候，Docker 会加载只读镜像层并在其上（镜像栈顶部）添加一个读写层。如果运行中的容器修改了现有的一个已经存在的文件，那该文件将会从读写层下面的只读层复制到读写层，该文件的只读版本仍然存在，只是已经被读写层中该文件的副本所隐藏。当删除Docker容器，并通过该镜像重新启动时，之前的更改将会丢失。\n为了能够保存（持久化）数据以及共享容器间的数据，Docker提出了Volume的概念。简单来说，数据卷是存在于一个或多个容器中的特定文件或文件夹，它可以绕过默认的联合文件系统，以正常的文件或者目录的形式存在于宿主机上。其生存周期独立于容器的生存周期。\nDocker提供了三种方式将数据从宿主机挂载到容器中：\nvolumes: Docker管理宿主机文件系统的一部分，默认位于 var/lib/docker/volumes 目录中，这是最常用的方式。 bind mounts: 可以存储在宿主机系统的任意位置，但在目录结构不同的操作系统之间不可移植。 tmpfs: 挂载存储在宿主机系统的内存中，而不会写入宿主机的文件系统。 docker volume [root@demo ~]# docker volume --help Usage: docker volume COMMAND Manage volumes Commands: create Create a volume inspect Display detailed information on one or more volumes ls List volumes prune Remove all unused local volumes rm Remove one or more volumes # 创建volume [root@demo ~]# docker volume create new_volume new_volume [root@demo ~]# docker volume ls DRIVER VOLUME NAME local new_volume [root@demo ~]# docker volume inspect new_volume [ { \u0026#34;CreatedAt\u0026#34;: \u0026#34;2022-01-10T07:43:43+08:00\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Labels\u0026#34;: {}, \u0026#34;Mountpoint\u0026#34;: \u0026#34;/var/lib/docker/volumes/new_volume/_data\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;new_volume\u0026#34;, \u0026#34;Options\u0026#34;: {}, \u0026#34;Scope\u0026#34;: \u0026#34;local\u0026#34; } ] # 在宿主机可以找到对应目录 [root@demo ~]# ls -al /var/lib/docker/volumes/ .. drwx-----x. 3 root root 19 Jan 10 07:43 new_volume 命令行中可以用-v使用数据卷 -v/--volume，由（:）分隔的三个字段组成，卷名:容器路径:选项。选项可以ro/rw。 --mount，由多个键值对组成，由,分隔，每个由一个key=value元组组成。 type，值可以为 bind，volume，tmpfs。 source，对于命名卷，是卷名。对于匿名卷，这个字段被省略。可能被指定为 source 或 src。 destination，文件或目录将被挂载到容器中的路径。可以指定为 destination，dst 或 target。 volume-opt 可以多次指定。 # 挂载数据卷new_volume到容器的/volume目录，创建文件并查看同步效果 # 下面命令等效于 docker run -itd --name mountvol --mount source=new_volume,target=/volume nginx [root@demo ~]# docker run -itd --name mountvol -v new_volume:/volume nginx c3450a454f209f30987f60863547c7a5a60d58fffa19faf89c08d0840cb6e1ac [root@demo ~]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c3450a454f20 nginx \u0026#34;/docker-entrypoint.…\u0026#34; 22 seconds ago Up 20 seconds 80/tcp mountvol [root@demo ~]# docker exec -it c3450a454f20 /bin/bash root@c3450a454f20:/# cd /volume/ root@c3450a454f20:/volume# echo volume \u0026gt; testfile root@c3450a454f20:/volume# ls testfile root@c3450a454f20:/volume# exit exit [root@demo ~]# cat /var/lib/docker/volumes/new_volume/_data/testfile volume # 默认数据卷在容器内挂载内容具备读写（rw）权限，指定只读 [root@demo ~]# docker run -itd --name mountvol2 -v new_volume:/volume:ro nginx 440ca257ed6de9b4387dc83d85b67e071ebafdb27ce5b5b2e4faa970c21cbd97 [root@demo _data]# docker exec -it 440ca257ed6d /bin/bash root@440ca257ed6d:/# cd /volume/ root@440ca257ed6d:/volume# touch file touch: cannot touch \u0026#39;file\u0026#39;: Read-only file system # 清理容器和数据卷 [root@demo _data]# docker volume rm new_volume Error response from daemon: remove new_volume: volume is in use - [c3450a454f209f30987f60863547c7a5a60d58fffa19faf89c08d0840cb6e1ac] [root@demo _data]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c3450a454f20 nginx \u0026#34;/docker-entrypoint.…\u0026#34; 16 minutes ago Up 16 minutes 80/tcp mountvol [root@demo _data]# docker stop c3450a454f20 c3450a454f20 [root@demo _data]# docker rm c3450a454f20 c3450a454f20 [root@demo _data]# docker volume rm new_volume new_volume [root@demo _data]# docker volume ls DRIVER VOLUME NAME # 清除未使用的数据卷 docker volume prune 使用主机目录 # 将主机任意目录挂载到容器作为数据卷，-v参数，如果宿主机没有相关目录，会自动创建 [root@demo ~]# docker run -itd --name web -v /webapp:/usr/share/nginx/html nginx 46b47e81ee8e53687b78aa109389f75135cdb70005934f1e4d49a992e47eb3ff [root@demo ~]# docker inspect web | grep -e Mounts -A 9 \u0026#34;Mounts\u0026#34;: [ { \u0026#34;Type\u0026#34;: \u0026#34;bind\u0026#34;, \u0026#34;Source\u0026#34;: \u0026#34;/webapp\u0026#34;, \u0026#34;Destination\u0026#34;: \u0026#34;/usr/share/nginx/html\u0026#34;, \u0026#34;Mode\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;RW\u0026#34;: true, \u0026#34;Propagation\u0026#34;: \u0026#34;rprivate\u0026#34; } ], # --mount，宿主机目录不存在会报错 [root@demo ~]# docker run -itd --name web2 --mount type=bind,source=/app/webapp,target=/usr/share/nginx/html,readonly nginx docker: Error response from daemon: invalid mount config for type \u0026#34;bind\u0026#34;: bind source path does not exist: /app/webapp. See \u0026#39;docker run --help\u0026#39;. [root@demo ~]# mkdir -p /app/webapp [root@demo ~]# docker run -itd --name web2 --mount type=bind,source=/app/webapp,target=/usr/share/nginx/html,readonly nginx 2f0f5e1a42bf63995fbe3918842bd541dd173d1b25dfe458e1043591b9d42ef8 网络参考文件：国家保密局-开源容器技术安全分析\n","permalink":"https://deemoprobe.github.io/posts/tech/docker/docker/","summary":"环境说明 操作系统：CentOS Linux release 7.9.2009 (Core) 操作用户：root 内核版本：5.18.12-1 涉及技术：Linux、Docker、Vm 容器技术的基石 Linux内核功能中，Namespaces和Cgroups技术是实现轻量级进程虚拟化的基础，是Linux Container的基石，这两个Lin","title":"Docker"},{"content":" 备用下载链接是本人存在阿里云OSS的安装包，不能保证版本是最新的，仅供交流使用，请支持正版，若有侵权请联系删除。\nWindows10平台 Super F4: 强制杀前台进程，可有效解决进程页面卡死问题，快捷键Ctrl+Alt+F4\n备用下载链接\nQTTabbar: 文件资源管理器中使用Tab标签功能，安装后此电脑(或文件资源管理器)-\u0026gt;查看(最上面)-\u0026gt;点击选项两个字-\u0026gt;QTTabbar-\u0026gt;启用成功\n备用下载链接\nCaptura: 录屏软件\n备用下载链接\nBitwarden: 全平台密码管理工具，在Google浏览器可下载对应插件，不过不建议存放敏感的密码（如银行卡密码等），本人相信只要是联网的密码管理工具安全性都不可能到100%，但管理网页登陆口令是比较方便的，可自动填充。\n备用下载链接\nKeePass: 开源密码管理器，密码数据库存放在本地，不联网即可管理，数据库文件使用当前已知的最佳和最安全的加密算法（AES-256、ChaCha20 和 Twofish）进行加密，安全性比较高。缺点是不如Bitwarden方便且不支持多平台，没有浏览器插件。\n备用下载链接\nrufus: 启动盘制作工具\n备用下载链接\nEverything: 全局资源搜索软件\n备用下载链接\nHWiNFO: 优秀的硬件信息搜集软件\n备用下载链接\nBrave浏览器: 保护隐私的浏览器\n备用下载链接\nFastStonecapture: 非常好用的滚动截图工具，当然常规截图功能都有。\n备用下载链接\nGPU-Z: 显卡检测工具\n备用下载链接\nScreenToGif: 截图生成GIF格式图片\n备用下载链接\nShadowsocks-4.1.9.2: 程序员必备的知识工具\nSecureCRT+FX: 好用的SSH工具\n备用下载链接\nTranslucentTB: Win10任务栏透明化小工具\n备用下载链接\nWox: 超级好用的快速启动器，搭配Everything使用更是如虎添翼，还可以添加很多插件。\n备用下载链接\nPscp: Windows和linux之间传输文件的小工具，使用方法见我的blogpscp使用说明\n备用下载链接\nRenamer: 强大的文件批量重命名工具\n备用下载链接\nVLC: 媒体播放器，多种解码。\n备用下载链接\n小孩桌面便签: 简约实用轻量的桌面便签,有很多便签样式可以选择,目前支持Windows/Android/IOS三端云同步.\n备用下载链接\nSnipaste: 开源轻量级截图贴图软件,可以看作QQ截图的升级版\n备用下载链接\nWGestures: 全局鼠标手势软件,可以设定多种鼠标快捷操作(如复制粘贴等)\n备用下载链接\nDism++: Windows系统优化神器\n备用下载链接\nLinux平台 Vim8.2\nPython3.7.7\nApache-tomcat-8.5.53\nHelm-v3.7.2\nJdk-8u271-linux-x64\nansible-collection-ansible-posix-1.2.0-1.el7.noarch.rpm\n移动终端 暂无\n其他 暂无\n","permalink":"https://deemoprobe.github.io/tools/","summary":"备用下载链接是本人存在阿里云OSS的安装包，不能保证版本是最新的，仅供交流使用，请支持正版，若有侵权请联系删除。 Windows10平台 Super F4: 强制杀前台进程，可有效解决进程页面卡死问题，快捷键Ctrl+Alt+F4 备用下载链接 QTTabbar: 文件资源管理器中使用Tab标签功能，安装后此电脑(或文件","title":"🪁 Tools"},{"content":" ","permalink":"https://deemoprobe.github.io/posts/life/4k-wallpaper/","summary":"","title":"4k Wallpaper"},{"content":"页面测试 由于访问速度问题，没有域名也无法使用CDN加速，所以，去掉了很多花里胡哨的功能\n","permalink":"https://deemoprobe.github.io/posts/life/life/","summary":"页面测试 由于访问速度问题，没有域名也无法使用CDN加速，所以，去掉了很多花里胡哨的功能","title":"Life"},{"content":"页面测试 由于访问速度问题，没有域名也无法使用CDN加速，所以，去掉了很多花里胡哨的功能\n","permalink":"https://deemoprobe.github.io/posts/read/read/","summary":"页面测试 由于访问速度问题，没有域名也无法使用CDN加速，所以，去掉了很多花里胡哨的功能","title":"Read"},{"content":"擅长Linux、Docker、Kubernetes等运维工作，熟悉常见的网络协议和网络数据包分析，熟悉常见的中间件，熟悉CI/CD、Kubesphere、Harbor、Helm、Prometheus、Microservices等云原生相关技术。了解业务架构设计的基本理念，对云原生和业务架构设计很感兴趣，目标是成为一名优秀的云原生全栈架构师。\n自知水平尚浅，但也始终相信“一分耕耘，一分收获”。\n本博客将永久更新和维护，记录技术生涯的点滴以及生活的感悟。\n由于个人建立博客的目的仅是为了记录笔记以及生活感悟。所以采用了Hugo+GitHubPages这种静态博客方式发布，没有留言系统，没有特殊字体，没有过多的JavaScript外链，即使如此网页访问速度也一般，但足够简洁实用了。\nNetName: deemoprobe Email: deemoprobe@gmail.com Blog: https://deemoprobe.github.com GitHub: deemoprobe BlogTheme: hugo-papermod2 Skills: Linux（RHCSA/RHCE/Shell） Docker/Kubernetes（CKA/CKS） DevOps（CICD：Jenkins/Ansible/Gitlab） Middleware（Weblogic/Nginx/Apache/Tomcat/JVM） Network(HTTP/TCPIP/netwox/tcpdump/Wireshark/PT/eNSP) Hobby: Cooking/Journey/Marathon/Cloud Native/Architecture/Kali/eBPF/Keyboard/ Address: Shanghai China 截至2023年1月1日，已取得相关技能证书清单如下：\nRED HAT CERTIFIED SYSTEM ADMINISTRATOR 证书验证：https://rhtapps.redhat.com/verify\nRED HAT CERTIFIED ENGINEER 证书验证：https://rhtapps.redhat.com/verify\nCERTIFIED KUBERNETES ADMINISTRATOR 证书验证：https://training.linuxfoundation.org/certification/verify\nCERTIFIED KUBERNETES SECURITY SPECIALIST 证书验证：https://training.linuxfoundation.org/certification/verify\nKUBERNETES AND CLOUD NATIVE ASSOCIATE 证书验证：https://training.linuxfoundation.org/certification/verify\n诸君共勉: 业精于勤荒于嬉,行成于思毁于随\n","permalink":"https://deemoprobe.github.io/about/","summary":"擅长Linux、Docker、Kubernetes等运维工作，熟悉常见的网络协议和网络数据包分析，熟悉常见的中间件，熟悉CI/CD、Kubesphere、Harbor、Helm、Prometheus、Microservices等云原生相关技术。了解业务架构设计的基本理念，对云原生","title":"🏃‍♂️ 关于作者"},{"content":"页面测试 由于访问速度问题，没有域名也无法使用CDN加速，所以，去掉了很多花里胡哨的功能\n","permalink":"https://deemoprobe.github.io/posts/blog/blog/","summary":"页面测试 由于访问速度问题，没有域名也无法使用CDN加速，所以，去掉了很多花里胡哨的功能","title":"Blog"},{"content":" William\u0026#39;s Blog 一个记录技术、阅读、生活的博客 👉友链格式 名称： William\u0026rsquo;s Blog 网址： https://YOUR_DOMAIN 图标： https://YOUR_DOMAIN/img/bear.gif 描述： 一个记录技术、阅读、生活的博客 👉友链申请要求 秉承互换友链原则、文章定期更新、不能有太多广告、个人描述字数控制在15字内\n👉Hugo博客交流群 YOUR_QQ\n","permalink":"https://deemoprobe.github.io/links/","summary":"William\u0026#39;s Blog 一个记录技术、阅读、生活的博客 👉友链格式 名称： William\u0026rsquo;s Blog 网址： https://YOUR_DOMAIN 图标： https://YOUR_DOMAIN/img/bear.gif 描述： 一个记录技术、阅读、生活的博客 👉友链申请要求 秉承互换友链原则、文章定期更新、不能有太多广告、个人描述字数控制在15字内 👉Hugo博客交流群 YOUR_QQ","title":"🤝友链"}]