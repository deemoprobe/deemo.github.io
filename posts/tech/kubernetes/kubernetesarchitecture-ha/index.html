<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>KubernetesArchitecture HA | William&#39;s Blog</title>
<meta name="keywords" content="kubernetes">
<meta name="description" content="Kubernetes高可用架构介绍">
<meta name="author" content="deemoprobe">
<link rel="canonical" href="https://deemoprobe.github.io/posts/tech/kubernetes/kubernetesarchitecture-ha/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.437c6a2af4fc60ed76299fc9cd9077545ae1b4ff1cc6481716ecd767dee77e57.css" integrity="sha256-Q3xqKvT8YO12KZ/JzZB3VFrhtP8cxkgXFuzXZ97nflc=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
        onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://deemoprobe.github.io/img/favicon/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://deemoprobe.github.io/img/favicon/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://deemoprobe.github.io/img/favicon/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://deemoprobe.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://deemoprobe.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.min.css">
<script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<script src="https://cdn.staticfile.org/jquery/3.6.3/jquery.min.js"></script>


<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = ""; 
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
<meta property="og:title" content="KubernetesArchitecture HA" />
<meta property="og:description" content="Kubernetes高可用架构介绍" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://deemoprobe.github.io/posts/tech/kubernetes/kubernetesarchitecture-ha/" />
<meta property="og:image" content="https://deemoprobe.github.io/img/kubernetes.png" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-28T11:45:50+08:00" />
<meta property="article:modified_time" content="2023-04-28T11:45:50+08:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://deemoprobe.github.io/img/kubernetes.png" />
<meta name="twitter:title" content="KubernetesArchitecture HA"/>
<meta name="twitter:description" content="Kubernetes高可用架构介绍"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [

        {
          "@type": "ListItem",
          "position":  1 ,
          "name": "📚文章",
          "item": "https://deemoprobe.github.io/posts/"
        },

        {
          "@type": "ListItem",
          "position":  2 ,
          "name": "👨🏻‍💻 技术",
          "item": "https://deemoprobe.github.io/posts/tech/"
        },

        {
          "@type": "ListItem",
          "position":  3 ,
          "name": "Kubernetes",
          "item": "https://deemoprobe.github.io/posts/tech/kubernetes/"
        }, 
    {
      "@type": "ListItem",
      "position":  4 ,
      "name": "KubernetesArchitecture HA",
      "item": "https://deemoprobe.github.io/posts/tech/kubernetes/kubernetesarchitecture-ha/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "KubernetesArchitecture HA",
  "name": "KubernetesArchitecture HA",
  "description": "Kubernetes高可用架构介绍",
  "keywords": [
    "kubernetes"
  ],
  "articleBody": "概念 Kubernetes 是谷歌开源的容器集群管理系统，是 Google 多年大规模容器管理技术 Borg 的开源版本。Borg作业调度系统详见论文：Google使用Borg进行大规模集群管理\nKubernetes以计算节点为基础管理集群，在容器技术的基础上，抽象出Pod，Pod暴露的服务抽象为Service，负载均衡抽象为Ingress，密码和配置抽象为Secret和ConfigMap。存储资源方面抽象为Volume、PV、PVC。为了实现滚动更新等功能在Pod基础上进一步抽象了ReplicaSet、Deployment、DaemonSet、StatefulSet、CronJob等高级资源。拥有强大的作业调度和管理功能，诸如滚动更新、服务发现、健康检查、资源配额和QoS、密码和配置管理、认证授权和准入控制、支持多种调度机制和存储类型。不依赖具体语言技术栈，为微服务提供全套的解决方案。插件化的架构和云原生标准化的推动使Kubernetes具备更强大的生态和发展前景。\nKubernetes理念：\n集群管理：以计算节点为基础，管理集群彼此通信的节点 作业调度和管理 支持多种存储类型：如本地存储和网络存储 滚动更新和回退 高利用率调度机制 资源配额和QoS 自愈机制：健康检查和故障恢复 密码和配置管理 服务发现与治理 声明式API 控制器模式 插件化架构 标准化 Kubernetes高可用架构图如图所示\n基于Kubeadm或二进制搭建高可用集群，最为重要的是数据持久化，通过ETCD集群的高可用实现，ETCD集群可以内建于Kubernetes集群节点中，也可以独立于Kubernetes集群外，不管是什么方式，ETCD存储必须采用高性能的磁盘。\n模型设计 TypeMeta定义对象类型 Group：对象分组，如apps、node.k8s.io Kind：对象类型，如Node、Pod、Deployment Version：对象版本，如v1、v1beta1 Metadata定义对象身份 Namespace：对象所在命名空间 Name：对象名称 Label：对象标签 Annotation：对象注释信息 Finalizer：资源锁，当对象接收删除请求时，如果该字段不为空，则会等待列表中资源释放 ResourceVersion：资源版本，保证对象多线程操作时的一致性，是一种乐观锁 时间戳、UID Spec定义对象的状态 ETCD Etcd是轻量型分布式键值数据存储组件，与APIServer交互实现资源配置数据的持久化。诸如集群中Node、Pod、Service等对象的状态和元数据，以及配置数据等。ETCD高可用实现的两种方式：\nKubernetes集群内建高可用etcd集群：在每个Master节点中部署etcd实例，Master节点高可用的同时部署etcd的高可用。这种方式将etcd与Kubernetes主控节点耦合在一起。 Kubernetes集群外部高可用etcd集群：即etcd集群是独立于Kubernetes集群存在的，这种方式将etcd与Kubernetes集群解耦，使得二者故障影响系数降低，更专注于各自本身的集群管理工作。缺点是架构中需要的独立主机数量增加。 任何etcd实例都可以处理读请求，只有领导者可以处理写请求；当etcd实例接收到Kubernetes集群apiserver的写请求时候，如果该实例不是领导者，则请求会转交到领导者处理；领导者将请求复制到其他etcd成员节点进行仲裁，当仲裁过半数实例同意后，领导者才会对请求进行操作。每个集群仅有一名领导者，当领导者不再响应时，其余etcd节点会在选举倒计时结束后开始新领导者的选举，将自己标记为候选者，集群内投票选举。\nETCD集群实例之间通过Raft一致性共识算法确保数据的一致性，一般是3或5个etcd实例组成高可用集群：奇数个是为了保证etcd leader选举的合理性；不使用更多的etcd实例原因是一方面为了节省计算资源，另一方面是Raft算法机制导致如果实例太多，集群写仲裁时间边长，性能会一定程度上变低。此外，etcd集群所能容忍的故障节点数最多为(N-1)/2，N为etcd集群实例数。\n一个三节点etcd高可用集群的状态：\nRaft一致性共识算法机制：\n候选者（Candidate）、领导者（Leader）、追随者（Follower）和任期（Term）； 最初所有节点都是follower，每个节点分配一个不同的随机election-timeout（150ms~300ms），term=0 当最短election-timeout节点超时后，term+1，标记自己为candidate，发起投票并投自己一票 follower收到term比自己大节点的选举请求后，选举该节点为leader，并更新自己的term与leader节点相同，重置election-timeout 运行期间，领导者间歇性发送heatbeat-timeout，证明自己还活着，不需要选举新领导 leader宕机后，follower监视到leader心跳消失，出现新的election-timeout超时节点，标记自己为candidate，开启新一轮选举 如果出现争抢则重新开始选举保证不会发生脑裂现象 leader处理写操作时要得到该区域半数以上节点的仲裁确认信息才能向数据库提交数据 如果网络原因导致脑裂，则节点较少的区域因为无法得到该区域半数以上节点确认而停止数据提交操作并取消该区域leader等待网络恢复 网络恢复后，小区域节点回退到故障操作前状态，加入新leader区并同步数据 至多容忍(N-1)/2节点故障 Master节点 集群的控制节点，核心组件如下：\nAPIServer：集群控制平面的前端，承担API网关的职责。是用户请求和系统组件与集群交互的唯一入口，所有资源的创建、删除和更新等操作都是通过调用apiserver的API接口进行。集群内，apiserver是各个模块之间的通信枢纽，提供etcd API接口，这些API能够让集群内其他组件监听到资源对象的增删改的变化；集群外，apiserver充当网关的作用，拥有完整的安全机制，完成客户端身份的认证（Authentication）和授权（Authorization），并对资源进行准入控制（Addmission Control）。 Controller Manager：集群自动化管理和控制中心，包含多种控制器。诸如Pod控制器（rs/deploy/ds/sts/cj/hpa）、网络管理方面（ep/svc/ingress等）和存储方面（pv/pvc等），还有其他几十种控制器。控制器采用主备模式和领导选举机制实现故障转移，允许多个副本同时运行，但只有领导者在工作，其他副本在领导者无法工作时选举新领导者提供服务。 Scheduler 集群Pod调度器，监听apiserver处的Pod变化，根据预定的条件对Pod进行调度。调度程序会综合考虑Pod的资源需求（Quota：CPU或内存）、服务质量（QoS）、亲和力反亲和力、策略约束和集群的运行状况等，将Pod调度到合适的计算节点，实现资源的高利用率。调度器也采用主备模式和领导选举机制。 master节点资源一定要尽量给够，以至于后期不会拖累集群的整体性能。并且允许的情况下，etcd最好也要和master节点区分开来，单独创建集群进行数据的存储，etcd-cluster必须使用高性能ssd硬盘，否则后期将大大影响集群的性能。\n# 集群一个master节点中核心组件守护进程状态和参数 [root@k8s-master01 ~]# systemctl status -l kube-apiserver.service ● kube-apiserver.service - Kubernetes API Server Loaded: loaded (/usr/lib/systemd/system/kube-apiserver.service; enabled; vendor preset: disabled) Active: active (running) since Sun 2022-03-13 19:59:48 CST; 33min ago Docs: https://github.com/kubernetes/kubernetes Main PID: 1782 (kube-apiserver) Tasks: 15 Memory: 375.2M CGroup: /system.slice/kube-apiserver.service └─1782 /usr/local/bin/kube-apiserver --v=2 --logtostderr=true --allow-privileged=true --bind-address=0.0.0.0 --secure-port=6443 --insecure-port=0 --advertise-address=192.168.43.183 --service-cluster-ip-range=10.96.0.0/12 --service-node-port-range=30000-32767 --etcd-servers=https://192.168.43.183:2379,https://192.168.43.184:2379,https://192.168.43.185:2379 --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem --etcd-certfile=/etc/etcd/ssl/etcd.pem --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem --client-ca-file=/etc/kubernetes/pki/ca.pem --tls-cert-file=/etc/kubernetes/pki/apiserver.pem --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-account-issuer=https://kubernetes.default.svc.cluster.local --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota --authorization-mode=Node,RBAC --enable-bootstrap-token-auth=true --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem --requestheader-allowed-names=aggregator --requestheader-group-headers=X-Remote-Group --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-username-headers=X-Remote-User --feature-gates=EphemeralContainers=true ... [root@k8s-master01 ~]# systemctl status -l kube-controller-manager.service ● kube-controller-manager.service - Kubernetes Controller Manager Loaded: loaded (/usr/lib/systemd/system/kube-controller-manager.service; enabled; vendor preset: disabled) Active: active (running) since Sun 2022-03-13 19:58:12 CST; 34min ago Docs: https://github.com/kubernetes/kubernetes Main PID: 1002 (kube-controller) Tasks: 6 Memory: 66.7M CGroup: /system.slice/kube-controller-manager.service └─1002 /usr/local/bin/kube-controller-manager --v=2 --logtostderr=true --address=127.0.0.1 --root-ca-file=/etc/kubernetes/pki/ca.pem --cluster-signing-cert-file=/etc/kubernetes/pki/ca.pem --cluster-signing-key-file=/etc/kubernetes/pki/ca-key.pem --service-account-private-key-file=/etc/kubernetes/pki/sa.key --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig --leader-elect=true --use-service-account-credentials=true --node-monitor-grace-period=40s --node-monitor-period=5s --pod-eviction-timeout=2m0s --controllers=*,bootstrapsigner,tokencleaner --allocate-node-cidrs=true --cluster-cidr=172.16.0.0/12 --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem --node-cidr-mask-size=24 --feature-gates=EphemeralContainers=true ... [root@k8s-master01 ~]# systemctl status -l kube-scheduler.service ● kube-scheduler.service - Kubernetes Scheduler Loaded: loaded (/usr/lib/systemd/system/kube-scheduler.service; enabled; vendor preset: disabled) Active: active (running) since Sun 2022-03-13 19:58:12 CST; 35min ago Docs: https://github.com/kubernetes/kubernetes Main PID: 998 (kube-scheduler) Tasks: 8 Memory: 50.6M CGroup: /system.slice/kube-scheduler.service └─998 /usr/local/bin/kube-scheduler --v=2 --logtostderr=true --address=127.0.0.1 --leader-elect=true --kubeconfig=/etc/kubernetes/scheduler.kubeconfig --feature-gates=EphemeralContainers=true ... Node节点 应用部署的节点，工作节点，资源调度的对象，核心组件如下：\nkubelet：运行在节点上负责启动容器的守护进程，监听节点上pod的状态，与master节点apiserver进行通信将状态上报到主控节点。kubelet需定时（nodeStatusUpdateFrequency: 10s 默认10s）向apiserver汇报自身的情况（磁盘空间状态、CPU和Memory是否有压力和自身服务是否Ready等），如果Node上的kubelet停止了汇报，NodeLifecycle控制器将标记对应node状态为NotReady，过一段时间后驱逐该node节点上的pod。pod被调度到kubelet所在节点时，kubelet会首先将pod中申请的volume挂载到当前节点，之后调用容器运行时创建pause沙箱（PodSandBox）容器（该容器用以维护pod网络协议栈）和pod容器。kubelet周期性地查询容器的状态，并定期汇报容器状态，通过cAdvisor监控容器资源的使用情况。 kube-proxy：负责pod之间的通信和负载均衡，将指定的流量分发到后端正确的机器上，一般工作模式默认为ipvs。具体工作行为是从apiserver监听service和endpoint对象，根据endpoint信息设置service到后端pod的路由，维护网络规则。 上面两个守护进程并不是工作节点特有，一般master控制节点也会启动kubelet和kube-proxy进程提高集群的可用性。\nIPVS：监听master节点增加和删除service以及endpoint的消息，调用netlink接口创建相应的ipvs规则。通过ipvs规则将流量转到相应的pod上。ipvs是内核级的转发，速度很快。 Iptables：监听master节点增加和删除service以及endpoint的消息，对于每一个service，都会创建一个iptables规则，将service的clusterIP代理到后端对应的pod上。iptables由于线性查找匹配、全量更新等特点，当规则很多时，性能会比ipvs差，所以一般选择ipvs即可。\niptables工具基于Linux内核的Netfilter模块，默认定义了多张表。每张表里包含若干内置链（chain），也可能包含用户自定义的链。每条链是一套规则（rule）列表，用于匹配一组数据包。每条规则都指定如何处理匹配的数据包。\ntable：包含一组chain的表\nchain：包含一组rule的链\nrule：匹配数据包的规则，例如：协议，端口号\ntarget：规则中的具体行为，例如：ACCEPT，DROP，INPUT，FORWARD，OUTPUT\n表\nfilter：默认表，通用数据包过滤表 mangle：为特定的数据包设计 nat：针对创建新连接的数据包 raw：主要用于结合 NOTRACK target 配置连接跟踪的豁免 security：用于MAC（Mandatory Access Control，强制访问控制）规则 规则链\nINPUT：以本机为目标的入口数据包规则链 OUTPUT：本机产生，向外转发的数据包规则链 FORWARD：路由经过本机的数据包规则链 PREROUTING：数据包进入路由之前的规则链 POSTROUTING：数据包发送到目标前（出路由）的规则链 IPtables处理链接的算法复杂度为O(n)，IPVS为O(1)，Service规模在1000内二者差距并不是很大。在Service规模超过1000后，IPtables规则链达到2000以上，性能开始下降，响应时间成倍数增加，IPVS则几乎不受Service规模的影响。Calico虽然也采用IPtables技术，但对规则链进行了优化，算法复杂度也达到了了O(1)的水平。除了大规模服务性能的差距，IPVS还具备复杂均衡算法全（轮询、最小连接数、哈希值、最小延迟等）、支持健康检查等优点。\n# 集群一个node节点核心组件运行状态和参数 [root@k8s-node01 ~]# systemctl status -l kubelet.service ● kubelet.service - Kubernetes Kubelet Loaded: loaded (/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled) Drop-In: /etc/systemd/system/kubelet.service.d └─10-kubelet.conf Active: active (running) since Sun 2022-03-13 20:03:34 CST; 33min ago Docs: https://github.com/kubernetes/kubernetes Main PID: 1572 (kubelet) Tasks: 14 Memory: 164.2M CGroup: /system.slice/kubelet.service └─1572 /usr/local/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig --kubeconfig=/etc/kubernetes/kubelet.kubeconfig --config=/etc/kubernetes/kubelet-conf.yml --network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin --container-runtime=remote --runtime-request-timeout=15m --container-runtime-endpoint=unix:///run/containerd/containerd.sock --cgroup-driver=systemd --node-labels=node.kubernetes.io/node= ... [root@k8s-node01 ~]# systemctl status -l kube-proxy.service ● kube-proxy.service - Kubernetes Kube Proxy Loaded: loaded (/usr/lib/systemd/system/kube-proxy.service; enabled; vendor preset: disabled) Active: active (running) since Sun 2022-03-13 20:03:32 CST; 33min ago Docs: https://github.com/kubernetes/kubernetes Main PID: 996 (kube-proxy) Tasks: 6 Memory: 57.9M CGroup: /system.slice/kube-proxy.service └─996 /usr/local/bin/kube-proxy --config=/etc/kubernetes/kube-proxy.yaml --v=2 --feature-gates=EphemeralContainers=true ... # kubelet进程对应的配置文件--config=/etc/kubernetes/kubelet-conf.yml [root@k8s-node01 ~]# cat /etc/kubernetes/kubelet-conf.yml apiVersion: kubelet.config.k8s.io/v1beta1 kind: KubeletConfiguration address: 0.0.0.0 port: 10250 readOnlyPort: 10255 authentication: anonymous: enabled: false webhook: cacheTTL: 2m0s enabled: true x509: clientCAFile: /etc/kubernetes/pki/ca.pem authorization: mode: Webhook webhook: cacheAuthorizedTTL: 5m0s cacheUnauthorizedTTL: 30s cgroupDriver: systemd cgroupsPerQOS: true clusterDNS: - 10.96.0.10 clusterDomain: cluster.local containerLogMaxFiles: 5 containerLogMaxSize: 10Mi contentType: application/vnd.kubernetes.protobuf cpuCFSQuota: true cpuManagerPolicy: none cpuManagerReconcilePeriod: 10s enableControllerAttachDetach: true enableDebuggingHandlers: true enforceNodeAllocatable: - pods eventBurst: 10 eventRecordQPS: 5 evictionHard: imagefs.available: 15% memory.available: 100Mi nodefs.available: 10% nodefs.inodesFree: 5% evictionPressureTransitionPeriod: 5m0s failSwapOn: true fileCheckFrequency: 20s hairpinMode: promiscuous-bridge healthzBindAddress: 127.0.0.1 healthzPort: 10248 httpCheckFrequency: 20s imageGCHighThresholdPercent: 85 imageGCLowThresholdPercent: 80 imageMinimumGCAge: 2m0s iptablesDropBit: 15 iptablesMasqueradeBit: 14 kubeAPIBurst: 10 kubeAPIQPS: 5 makeIPTablesUtilChains: true maxOpenFiles: 1000000 maxPods: 110 nodeStatusUpdateFrequency: 10s oomScoreAdj: -999 podPidsLimit: -1 registryBurst: 10 registryPullQPS: 5 resolvConf: /etc/resolv.conf rotateCertificates: true runtimeRequestTimeout: 2m0s serializeImagePulls: true staticPodPath: /etc/kubernetes/manifests streamingConnectionIdleTimeout: 4h0m0s syncFrequency: 1m0s volumeStatsAggPeriod: 1m0s featureGates: EphemeralContainers: true # kube-proxy对应的配置文件--config=/etc/kubernetes/kube-proxy.yaml [root@k8s-node01 ~]# cat /etc/kubernetes/kube-proxy.yaml apiVersion: kubeproxy.config.k8s.io/v1alpha1 bindAddress: 0.0.0.0 clientConnection: acceptContentTypes: \"\" burst: 10 contentType: application/vnd.kubernetes.protobuf kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig qps: 5 clusterCIDR: 172.16.0.0/12 configSyncPeriod: 15m0s conntrack: max: null maxPerCore: 32768 min: 131072 tcpCloseWaitTimeout: 1h0m0s tcpEstablishedTimeout: 24h0m0s enableProfiling: false healthzBindAddress: 0.0.0.0:10256 hostnameOverride: \"\" iptables: masqueradeAll: false masqueradeBit: 14 minSyncPeriod: 0s syncPeriod: 30s ipvs: masqueradeAll: true minSyncPeriod: 5s scheduler: \"rr\" syncPeriod: 30s kind: KubeProxyConfiguration metricsBindAddress: 127.0.0.1:10249 mode: \"ipvs\" nodePortAddresses: null oomScoreAdj: -999 portRange: \"\" udpIdleTimeout: 250ms 容器运行时 符合CRI（Container Runtime Interface）标准的容器运行时（Container Runtime）是实际上管理容器的组件，容器运行时可分为高层和底层运行时。高层运行时诸如：Docker、containerd、CRI-O，官方介绍:容器运行时 底层运行时诸如：runc、kata、gVisor，kata和gVisor相对不是很成熟，目前底层运行时一般默认选择runc。\n# containerd容器运行时服务状态 [root@k8s-master01 ~]# systemctl status -l containerd.service ● containerd.service - containerd container runtime Loaded: loaded (/usr/lib/systemd/system/containerd.service; enabled; vendor preset: disabled) Active: active (running) since Sun 2022-03-13 19:58:14 CST; 1h 13min ago Docs: https://containerd.io Process: 996 ExecStartPre=/sbin/modprobe overlay (code=exited, status=0/SUCCESS) Main PID: 1011 (containerd) Tasks: 48 Memory: 99.9M CGroup: /system.slice/containerd.service ├─1011 /usr/bin/containerd ├─1857 /usr/bin/containerd-shim-runc-v2 -namespace k8s.io -id 83f9705395fb4a165bd52894f6545a58b8040a5fe6774b03cdcd02ca90900708 -address /run/containerd/containerd.sock ├─2394 /usr/bin/containerd-shim-runc-v2 -namespace k8s.io -id ae7a1e3a28a331bf11762c3b049e8237c322cddabd842dca398911a45dc656db -address /run/containerd/containerd.sock └─2802 /usr/bin/containerd-shim-runc-v2 -namespace k8s.io -id 30e5560350fdcb76efbbb10f7c8e9a41fa4fdc10e4483f62830fff749a448fb1 -address /run/containerd/containerd.sock 网络插件 符合CNI（Container Network Interface）标准的网络插件，诸如：Calico、Cilium、Flannel等。会为每个pod生成唯一的IP地址，并且把每个节点当做一个路由器。Cilium官方有一个CNI性能测试报告，采用eBPF技术的Cilium和Calico性能优越，具备TCP吞吐量大、CPU和网络开销低等特点。eBPF是一种内核级包过滤技术，诸如tcpdump、Netfilter均采用该技术，性能比iptables要好很多。可以使用route -n查看\n# 使用calico插件的路由 [root@k8s-master01 ~]# route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 192.168.43.1 0.0.0.0 UG 0 0 0 ens33 169.254.0.0 0.0.0.0 255.255.0.0 U 1002 0 0 ens33 172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0 172.17.125.0 192.168.43.186 255.255.255.192 UG 0 0 0 tunl0 172.18.195.0 192.168.43.185 255.255.255.192 UG 0 0 0 tunl0 172.25.92.64 192.168.43.184 255.255.255.192 UG 0 0 0 tunl0 172.25.244.192 0.0.0.0 255.255.255.192 U 0 0 0 * 172.25.244.213 0.0.0.0 255.255.255.255 UH 0 0 0 cali183ffe150c7 172.25.244.214 0.0.0.0 255.255.255.255 UH 0 0 0 calicec0b1cfb68 172.27.14.192 192.168.43.187 255.255.255.192 UG 0 0 0 tunl0 192.168.43.0 0.0.0.0 255.255.255.0 U 0 0 0 ens33 CoreDNS 用于集群内部service的域名解析系统，可以让pod把service名称解析成ip地址，然后通过service的IP地址连接到对应的应用上。\nPod Pod是Kubernetes中最小的单元，是由一个或多个容器组成的。每个pod还包含一个pause容器，pause容器是pod的父容器，负责僵尸进程的回收管理，通过pause容器可以使同一个pod内多个容器共享存储、网络、PID、IPC等。详细可以查看博客：Pod定义及零宕机部署\n本文相关概念参考书籍：《Kubernetes生产化实践之路》\n",
  "wordCount" : "6236",
  "inLanguage": "en",
  "image":"https://deemoprobe.github.io/img/kubernetes.png","datePublished": "2023-04-28T11:45:50+08:00",
  "dateModified": "2023-04-28T11:45:50+08:00",
  "author":[{
    "@type": "Person",
    "name": "deemoprobe"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://deemoprobe.github.io/posts/tech/kubernetes/kubernetesarchitecture-ha/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "William's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://deemoprobe.github.io/img/favicon/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    (function () {
        let  arr,reg = new RegExp("(^| )"+"change-themes"+"=([^;]*)(;|$)");
        if(arr = document.cookie.match(reg)) {
        } else {
            if (new Date().getHours() >= 19 || new Date().getHours() < 6) {
                document.body.classList.add('dark');
                localStorage.setItem("pref-theme", 'dark');
            } else {
                document.body.classList.remove('dark');
                localStorage.setItem("pref-theme", 'light');
            }
        }
    })()

    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }
</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://deemoprobe.github.io/" accesskey="h" title="William&#39;s Blog (Alt + H)">
            <img src="https://deemoprobe.github.io/img/favicon/favicon-32x32.png" alt="logo" aria-label="logo"
                 height="32">William&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://deemoprobe.github.io/" title="🏠 主页">
                <span>🏠 主页</span>
                </a>
            </li>
            <li>
                <a href="https://deemoprobe.github.io/tags" title="🎨 标签">
                <span>🎨 标签</span>
                </a>
            </li>
            <li>
                <a href="https://deemoprobe.github.io/archives/" title="📈 归档">
                <span>📈 归档</span>
                </a>
            </li>
            <li>
                <a href="https://deemoprobe.github.io/tools" title="🪁 工具">
                <span>🪁 工具</span>
                </a>
            </li>
            <li>
                <a href="https://deemoprobe.github.io/search" title="🔍 搜索 (Alt &#43; /)" accesskey=/>
                <span>🔍 搜索</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main page">
<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }
</style>

<article class="post-single">
    <div id="single-content">
        <header class="post-header">
            
            <h1 class="post-title">
                KubernetesArchitecture HA
            </h1>
            <div class="post-description">
                Kubernetes高可用架构介绍
            </div>
            <div class="post-meta">

<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }

    .parent-post-meta {
        display: flex;
        flex-wrap: wrap;
        opacity: 0.8;
    }
</style>

<span class="parent-post-meta">
    <span id="post_meta_style_1">
        <span class="fa fa-calendar-check-o"></span>
        <span>2023-04-28
            &nbsp;&nbsp;
        </span>
    </span>
    
    
    
    
    
    
    
    <span id="post_meta_style_3">
        <span class="fa fa-file-word-o"></span>
        <span>6236字
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_4">
        <span class="fa fa-clock-o"></span>
        <span>13分钟
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_5">
        <span class="fa fa-user-o"></span>
        <span>deemoprobe
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_6">
        <span class="fa fa-tags" style="opacity: 0.8"></span>
        <span>
            <span class="post-tags-meta">
                <a href="https://deemoprobe.github.io/tags/kubernetes/" style="color: var(--secondary)!important;">kubernetes</a>
            </span>
        </span>
    </span>
</span>
<span style="opacity: 0.8;">
                    <span id="post_meta_style_7">
                        &nbsp;&nbsp;
                        <span class="fa fa-eye" ></span>
                        <span>
                            <span id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv"></span></span>
                            &nbsp;&nbsp;
                        </span>
                    </span>
                    
                </span>

</div>
        </header> 
<figure class="entry-cover1"><img style="zoom:;" loading="lazy" src="https://deemoprobe.github.io/img/kubernetes.png" alt="">
    
</figure><aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">文章目录</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#%e6%a6%82%e5%bf%b5" aria-label="概念">概念</a></li>
                <li>
                    <a href="#%e6%a8%a1%e5%9e%8b%e8%ae%be%e8%ae%a1" aria-label="模型设计">模型设计</a></li>
                <li>
                    <a href="#etcd" aria-label="ETCD">ETCD</a></li>
                <li>
                    <a href="#master%e8%8a%82%e7%82%b9" aria-label="Master节点">Master节点</a></li>
                <li>
                    <a href="#node%e8%8a%82%e7%82%b9" aria-label="Node节点">Node节点</a></li>
                <li>
                    <a href="#%e5%ae%b9%e5%99%a8%e8%bf%90%e8%a1%8c%e6%97%b6" aria-label="容器运行时">容器运行时</a></li>
                <li>
                    <a href="#%e7%bd%91%e7%bb%9c%e6%8f%92%e4%bb%b6" aria-label="网络插件">网络插件</a></li>
                <li>
                    <a href="#coredns" aria-label="CoreDNS">CoreDNS</a></li>
                <li>
                    <a href="#pod" aria-label="Pod">Pod</a>
                </li>
            </ul>
        </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
        
        activeElement = elements[0];
        const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
        document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
    }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 &&
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
            const id = encodeURI(element.getAttribute('id')).toLowerCase();
            if (element === activeElement){
                document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
            } else {
                document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
            }
        })
    }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;
        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;
    }
</script>
        <div class="post-content"><h2 id="概念">概念<a hidden class="anchor" aria-hidden="true" href="#概念">#</a></h2>
<p>Kubernetes 是谷歌开源的容器集群管理系统，是 Google 多年大规模容器管理技术 Borg 的开源版本。Borg作业调度系统详见论文：<a href="https://deemoprobe.oss-cn-shanghai.aliyuncs.com/doc/Large-scale%20cluster%20management%20at%20Google%20with%20Borg.pdf?versionId=CAEQNBiBgMCU4u_3.xciIGM1MGQ0YmI0NDQ4MjQxNzQ4MzhhNmI2MWVmYThiZmNl"  target="_blank" rel="noopener" style="color:#42b983" ;>Google使用Borg进行大规模集群管理</a></p>
<p>Kubernetes以计算节点为基础管理集群，在容器技术的基础上，抽象出Pod，Pod暴露的服务抽象为Service，负载均衡抽象为Ingress，密码和配置抽象为Secret和ConfigMap。存储资源方面抽象为Volume、PV、PVC。为了实现滚动更新等功能在Pod基础上进一步抽象了ReplicaSet、Deployment、DaemonSet、StatefulSet、CronJob等高级资源。拥有强大的作业调度和管理功能，诸如滚动更新、服务发现、健康检查、资源配额和QoS、密码和配置管理、认证授权和准入控制、支持多种调度机制和存储类型。不依赖具体语言技术栈，为微服务提供全套的解决方案。插件化的架构和云原生标准化的推动使Kubernetes具备更强大的生态和发展前景。</p>
<p>Kubernetes理念：</p>
<ul>
<li>集群管理：以计算节点为基础，管理集群彼此通信的节点</li>
<li>作业调度和管理
<ul>
<li>支持多种存储类型：如本地存储和网络存储</li>
<li>滚动更新和回退</li>
<li>高利用率调度机制</li>
<li>资源配额和QoS</li>
<li>自愈机制：健康检查和故障恢复</li>
<li>密码和配置管理</li>
</ul>
</li>
<li>服务发现与治理</li>
<li>声明式API</li>
<li>控制器模式</li>
<li>插件化架构</li>
<li>标准化</li>
</ul>
<p>Kubernetes高可用架构图如图所示</p>
<p><img loading="lazy" src="https://deemoprobe.oss-cn-shanghai.aliyuncs.com/images/Kubernetes%e9%ab%98%e5%8f%af%e7%94%a8%e6%9e%b6%e6%9e%84.png" alt="1"  />
</p>
<p>基于Kubeadm或二进制搭建高可用集群，最为重要的是数据持久化，通过ETCD集群的高可用实现，ETCD集群可以内建于Kubernetes集群节点中，也可以独立于Kubernetes集群外，不管是什么方式，ETCD存储必须采用高性能的磁盘。</p>
<h2 id="模型设计">模型设计<a hidden class="anchor" aria-hidden="true" href="#模型设计">#</a></h2>
<ul>
<li>TypeMeta定义对象类型
<ul>
<li>Group：对象分组，如apps、node.k8s.io</li>
<li>Kind：对象类型，如Node、Pod、Deployment</li>
<li>Version：对象版本，如v1、v1beta1</li>
</ul>
</li>
<li>Metadata定义对象身份
<ul>
<li>Namespace：对象所在命名空间</li>
<li>Name：对象名称</li>
<li>Label：对象标签</li>
<li>Annotation：对象注释信息</li>
<li>Finalizer：资源锁，当对象接收删除请求时，如果该字段不为空，则会等待列表中资源释放</li>
<li>ResourceVersion：资源版本，保证对象多线程操作时的一致性，是一种乐观锁</li>
<li>时间戳、UID</li>
</ul>
</li>
<li>Spec定义对象的状态</li>
</ul>
<h2 id="etcd">ETCD<a hidden class="anchor" aria-hidden="true" href="#etcd">#</a></h2>
<p>Etcd是轻量型分布式键值数据存储组件，与APIServer交互实现资源配置数据的持久化。诸如集群中Node、Pod、Service等对象的状态和元数据，以及配置数据等。ETCD高可用实现的两种方式：</p>
<ul>
<li>Kubernetes集群内建高可用etcd集群：在每个Master节点中部署etcd实例，Master节点高可用的同时部署etcd的高可用。这种方式将etcd与Kubernetes主控节点耦合在一起。</li>
<li>Kubernetes集群外部高可用etcd集群：即etcd集群是独立于Kubernetes集群存在的，这种方式将etcd与Kubernetes集群解耦，使得二者故障影响系数降低，更专注于各自本身的集群管理工作。缺点是架构中需要的独立主机数量增加。</li>
</ul>
<p><img loading="lazy" src="https://deemoprobe.oss-cn-shanghai.aliyuncs.com/images/%e5%86%85%e5%bb%ba%e5%bc%8fetcd%e9%ab%98%e5%8f%af%e7%94%a8%e9%9b%86%e7%be%a4%e6%9e%b6%e6%9e%84.drawio.png" alt="2"  />
</p>
<p><img loading="lazy" src="https://deemoprobe.oss-cn-shanghai.aliyuncs.com/images/%e5%a4%96%e9%83%a8etcd%e9%ab%98%e5%8f%af%e7%94%a8%e9%9b%86%e7%be%a4%e6%9e%b6%e6%9e%84.drawio.png" alt="3"  />
</p>
<p>任何etcd实例都可以处理读请求，只有领导者可以处理写请求；当etcd实例接收到Kubernetes集群apiserver的写请求时候，如果该实例不是领导者，则请求会转交到领导者处理；领导者将请求复制到其他etcd成员节点进行仲裁，当仲裁过半数实例同意后，领导者才会对请求进行操作。每个集群仅有一名领导者，当领导者不再响应时，其余etcd节点会在选举倒计时结束后开始新领导者的选举，将自己标记为候选者，集群内投票选举。</p>
<p>ETCD集群实例之间通过Raft一致性共识算法确保数据的一致性，一般是3或5个etcd实例组成高可用集群：奇数个是为了保证<code>etcd leader</code>选举的合理性；不使用更多的etcd实例原因是一方面为了节省计算资源，另一方面是Raft算法机制导致如果实例太多，集群写仲裁时间边长，性能会一定程度上变低。此外，etcd集群所能容忍的故障节点数最多为<code>(N-1)/2</code>，N为etcd集群实例数。</p>
<p>一个三节点etcd高可用集群的状态：</p>
<p><img loading="lazy" src="https://deemoprobe.oss-cn-shanghai.aliyuncs.com/images/20220313212422.png" alt="4"  />
</p>
<p>Raft一致性共识算法机制：</p>
<ul>
<li>候选者（Candidate）、领导者（Leader）、追随者（Follower）和任期（Term）；</li>
<li>最初所有节点都是follower，每个节点分配一个不同的随机election-timeout（150ms~300ms），term=0</li>
<li>当最短election-timeout节点超时后，term+1，标记自己为candidate，发起投票并投自己一票</li>
<li>follower收到term比自己大节点的选举请求后，选举该节点为leader，并更新自己的term与leader节点相同，重置election-timeout</li>
<li>运行期间，领导者间歇性发送heatbeat-timeout，证明自己还活着，不需要选举新领导</li>
<li>leader宕机后，follower监视到leader心跳消失，出现新的election-timeout超时节点，标记自己为candidate，开启新一轮选举</li>
<li>如果出现争抢则重新开始选举保证不会发生脑裂现象</li>
<li>leader处理写操作时要得到该区域半数以上节点的仲裁确认信息才能向数据库提交数据</li>
<li>如果网络原因导致脑裂，则节点较少的区域因为无法得到该区域半数以上节点确认而停止数据提交操作并取消该区域leader等待网络恢复</li>
<li>网络恢复后，小区域节点回退到故障操作前状态，加入新leader区并同步数据</li>
<li>至多容忍<code>(N-1)/2</code>节点故障</li>
</ul>
<h2 id="master节点">Master节点<a hidden class="anchor" aria-hidden="true" href="#master节点">#</a></h2>
<p>集群的控制节点，核心组件如下：</p>
<ul>
<li>APIServer：集群控制平面的前端，承担API网关的职责。是用户请求和系统组件与集群交互的唯一入口，所有资源的创建、删除和更新等操作都是通过调用apiserver的API接口进行。集群内，apiserver是各个模块之间的通信枢纽，提供etcd API接口，这些API能够让集群内其他组件监听到资源对象的增删改的变化；集群外，apiserver充当网关的作用，拥有完整的安全机制，完成客户端身份的认证（Authentication）和授权（Authorization），并对资源进行准入控制（Addmission Control）。</li>
<li>Controller Manager：集群自动化管理和控制中心，包含多种控制器。诸如Pod控制器（rs/deploy/ds/sts/cj/hpa）、网络管理方面（ep/svc/ingress等）和存储方面（pv/pvc等），还有其他几十种控制器。控制器采用主备模式和领导选举机制实现故障转移，允许多个副本同时运行，但只有领导者在工作，其他副本在领导者无法工作时选举新领导者提供服务。</li>
<li>Scheduler 集群Pod调度器，监听apiserver处的Pod变化，根据预定的条件对Pod进行调度。调度程序会综合考虑Pod的资源需求（Quota：CPU或内存）、服务质量（QoS）、亲和力反亲和力、策略约束和集群的运行状况等，将Pod调度到合适的计算节点，实现资源的高利用率。调度器也采用主备模式和领导选举机制。</li>
</ul>
<p>master节点资源一定要尽量给够，以至于后期不会拖累集群的整体性能。并且允许的情况下，etcd最好也要和master节点区分开来，单独创建集群进行数据的存储，etcd-cluster必须使用高性能ssd硬盘，否则后期将大大影响集群的性能。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># 集群一个master节点中核心组件守护进程状态和参数</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">[</span>root@k8s-master01 ~<span style="color:#f92672">]</span><span style="color:#75715e"># systemctl status -l kube-apiserver.service</span>
</span></span><span style="display:flex;"><span>● kube-apiserver.service - Kubernetes API Server
</span></span><span style="display:flex;"><span>   Loaded: loaded <span style="color:#f92672">(</span>/usr/lib/systemd/system/kube-apiserver.service; enabled; vendor preset: disabled<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>   Active: active <span style="color:#f92672">(</span>running<span style="color:#f92672">)</span> since Sun 2022-03-13 19:59:48 CST; 33min ago
</span></span><span style="display:flex;"><span>     Docs: https://github.com/kubernetes/kubernetes
</span></span><span style="display:flex;"><span> Main PID: <span style="color:#ae81ff">1782</span> <span style="color:#f92672">(</span>kube-apiserver<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    Tasks: <span style="color:#ae81ff">15</span>
</span></span><span style="display:flex;"><span>   Memory: 375.2M
</span></span><span style="display:flex;"><span>   CGroup: /system.slice/kube-apiserver.service
</span></span><span style="display:flex;"><span>           └─1782 /usr/local/bin/kube-apiserver --v<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span> --logtostderr<span style="color:#f92672">=</span>true --allow-privileged<span style="color:#f92672">=</span>true --bind-address<span style="color:#f92672">=</span>0.0.0.0 --secure-port<span style="color:#f92672">=</span><span style="color:#ae81ff">6443</span> --insecure-port<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span> --advertise-address<span style="color:#f92672">=</span>192.168.43.183 --service-cluster-ip-range<span style="color:#f92672">=</span>10.96.0.0/12 --service-node-port-range<span style="color:#f92672">=</span>30000-32767 --etcd-servers<span style="color:#f92672">=</span>https://192.168.43.183:2379,https://192.168.43.184:2379,https://192.168.43.185:2379 --etcd-cafile<span style="color:#f92672">=</span>/etc/etcd/ssl/etcd-ca.pem --etcd-certfile<span style="color:#f92672">=</span>/etc/etcd/ssl/etcd.pem --etcd-keyfile<span style="color:#f92672">=</span>/etc/etcd/ssl/etcd-key.pem --client-ca-file<span style="color:#f92672">=</span>/etc/kubernetes/pki/ca.pem --tls-cert-file<span style="color:#f92672">=</span>/etc/kubernetes/pki/apiserver.pem --tls-private-key-file<span style="color:#f92672">=</span>/etc/kubernetes/pki/apiserver-key.pem --kubelet-client-certificate<span style="color:#f92672">=</span>/etc/kubernetes/pki/apiserver.pem --kubelet-client-key<span style="color:#f92672">=</span>/etc/kubernetes/pki/apiserver-key.pem --service-account-key-file<span style="color:#f92672">=</span>/etc/kubernetes/pki/sa.pub --service-account-signing-key-file<span style="color:#f92672">=</span>/etc/kubernetes/pki/sa.key --service-account-issuer<span style="color:#f92672">=</span>https://kubernetes.default.svc.cluster.local --kubelet-preferred-address-types<span style="color:#f92672">=</span>InternalIP,ExternalIP,Hostname --enable-admission-plugins<span style="color:#f92672">=</span>NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota --authorization-mode<span style="color:#f92672">=</span>Node,RBAC --enable-bootstrap-token-auth<span style="color:#f92672">=</span>true --requestheader-client-ca-file<span style="color:#f92672">=</span>/etc/kubernetes/pki/front-proxy-ca.pem --proxy-client-cert-file<span style="color:#f92672">=</span>/etc/kubernetes/pki/front-proxy-client.pem --proxy-client-key-file<span style="color:#f92672">=</span>/etc/kubernetes/pki/front-proxy-client-key.pem --requestheader-allowed-names<span style="color:#f92672">=</span>aggregator --requestheader-group-headers<span style="color:#f92672">=</span>X-Remote-Group --requestheader-extra-headers-prefix<span style="color:#f92672">=</span>X-Remote-Extra- --requestheader-username-headers<span style="color:#f92672">=</span>X-Remote-User --feature-gates<span style="color:#f92672">=</span>EphemeralContainers<span style="color:#f92672">=</span>true
</span></span><span style="display:flex;"><span>   ...
</span></span><span style="display:flex;"><span><span style="color:#f92672">[</span>root@k8s-master01 ~<span style="color:#f92672">]</span><span style="color:#75715e"># systemctl status -l kube-controller-manager.service</span>
</span></span><span style="display:flex;"><span>● kube-controller-manager.service - Kubernetes Controller Manager
</span></span><span style="display:flex;"><span>   Loaded: loaded <span style="color:#f92672">(</span>/usr/lib/systemd/system/kube-controller-manager.service; enabled; vendor preset: disabled<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>   Active: active <span style="color:#f92672">(</span>running<span style="color:#f92672">)</span> since Sun 2022-03-13 19:58:12 CST; 34min ago
</span></span><span style="display:flex;"><span>     Docs: https://github.com/kubernetes/kubernetes
</span></span><span style="display:flex;"><span> Main PID: <span style="color:#ae81ff">1002</span> <span style="color:#f92672">(</span>kube-controller<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    Tasks: <span style="color:#ae81ff">6</span>
</span></span><span style="display:flex;"><span>   Memory: 66.7M
</span></span><span style="display:flex;"><span>   CGroup: /system.slice/kube-controller-manager.service
</span></span><span style="display:flex;"><span>           └─1002 /usr/local/bin/kube-controller-manager --v<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span> --logtostderr<span style="color:#f92672">=</span>true --address<span style="color:#f92672">=</span>127.0.0.1 --root-ca-file<span style="color:#f92672">=</span>/etc/kubernetes/pki/ca.pem --cluster-signing-cert-file<span style="color:#f92672">=</span>/etc/kubernetes/pki/ca.pem --cluster-signing-key-file<span style="color:#f92672">=</span>/etc/kubernetes/pki/ca-key.pem --service-account-private-key-file<span style="color:#f92672">=</span>/etc/kubernetes/pki/sa.key --kubeconfig<span style="color:#f92672">=</span>/etc/kubernetes/controller-manager.kubeconfig --leader-elect<span style="color:#f92672">=</span>true --use-service-account-credentials<span style="color:#f92672">=</span>true --node-monitor-grace-period<span style="color:#f92672">=</span>40s --node-monitor-period<span style="color:#f92672">=</span>5s --pod-eviction-timeout<span style="color:#f92672">=</span>2m0s --controllers<span style="color:#f92672">=</span>*,bootstrapsigner,tokencleaner --allocate-node-cidrs<span style="color:#f92672">=</span>true --cluster-cidr<span style="color:#f92672">=</span>172.16.0.0/12 --requestheader-client-ca-file<span style="color:#f92672">=</span>/etc/kubernetes/pki/front-proxy-ca.pem --node-cidr-mask-size<span style="color:#f92672">=</span><span style="color:#ae81ff">24</span> --feature-gates<span style="color:#f92672">=</span>EphemeralContainers<span style="color:#f92672">=</span>true
</span></span><span style="display:flex;"><span>   ...
</span></span><span style="display:flex;"><span><span style="color:#f92672">[</span>root@k8s-master01 ~<span style="color:#f92672">]</span><span style="color:#75715e"># systemctl status -l kube-scheduler.service </span>
</span></span><span style="display:flex;"><span>● kube-scheduler.service - Kubernetes Scheduler
</span></span><span style="display:flex;"><span>   Loaded: loaded <span style="color:#f92672">(</span>/usr/lib/systemd/system/kube-scheduler.service; enabled; vendor preset: disabled<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>   Active: active <span style="color:#f92672">(</span>running<span style="color:#f92672">)</span> since Sun 2022-03-13 19:58:12 CST; 35min ago
</span></span><span style="display:flex;"><span>     Docs: https://github.com/kubernetes/kubernetes
</span></span><span style="display:flex;"><span> Main PID: <span style="color:#ae81ff">998</span> <span style="color:#f92672">(</span>kube-scheduler<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    Tasks: <span style="color:#ae81ff">8</span>
</span></span><span style="display:flex;"><span>   Memory: 50.6M
</span></span><span style="display:flex;"><span>   CGroup: /system.slice/kube-scheduler.service
</span></span><span style="display:flex;"><span>           └─998 /usr/local/bin/kube-scheduler --v<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span> --logtostderr<span style="color:#f92672">=</span>true --address<span style="color:#f92672">=</span>127.0.0.1 --leader-elect<span style="color:#f92672">=</span>true --kubeconfig<span style="color:#f92672">=</span>/etc/kubernetes/scheduler.kubeconfig --feature-gates<span style="color:#f92672">=</span>EphemeralContainers<span style="color:#f92672">=</span>true
</span></span><span style="display:flex;"><span>   ...
</span></span></code></pre></div><h2 id="node节点">Node节点<a hidden class="anchor" aria-hidden="true" href="#node节点">#</a></h2>
<p>应用部署的节点，工作节点，资源调度的对象，核心组件如下：</p>
<ul>
<li>kubelet：运行在节点上负责启动容器的守护进程，监听节点上pod的状态，与master节点apiserver进行通信将状态上报到主控节点。kubelet需定时（nodeStatusUpdateFrequency: 10s 默认10s）向apiserver汇报自身的情况（磁盘空间状态、CPU和Memory是否有压力和自身服务是否Ready等），如果Node上的kubelet停止了汇报，NodeLifecycle控制器将标记对应node状态为NotReady，过一段时间后驱逐该node节点上的pod。pod被调度到kubelet所在节点时，kubelet会首先将pod中申请的volume挂载到当前节点，之后调用容器运行时创建pause沙箱（PodSandBox）容器（该容器用以维护pod网络协议栈）和pod容器。kubelet周期性地查询容器的状态，并定期汇报容器状态，通过cAdvisor监控容器资源的使用情况。</li>
<li>kube-proxy：负责pod之间的通信和负载均衡，将指定的流量分发到后端正确的机器上，一般工作模式默认为ipvs。具体工作行为是从apiserver监听service和endpoint对象，根据endpoint信息设置service到后端pod的路由，维护网络规则。</li>
</ul>
<p>上面两个守护进程并不是工作节点特有，一般master控制节点也会启动kubelet和kube-proxy进程提高集群的可用性。</p>
<p>IPVS：监听master节点增加和删除service以及endpoint的消息，调用netlink接口创建相应的ipvs规则。通过ipvs规则将流量转到相应的pod上。ipvs是内核级的转发，速度很快。
Iptables：监听master节点增加和删除service以及endpoint的消息，对于每一个service，都会创建一个iptables规则，将service的clusterIP代理到后端对应的pod上。iptables由于线性查找匹配、全量更新等特点，当规则很多时，性能会比ipvs差，所以一般选择ipvs即可。</p>
<p><code>iptables</code>工具基于Linux内核的<code>Netfilter</code>模块，默认定义了多张表。每张表里包含若干内置链（chain），也可能包含用户自定义的链。每条链是一套规则（rule）列表，用于匹配一组数据包。每条规则都指定如何处理匹配的数据包。</p>
<ul>
<li>
<p>table：包含一组chain的表</p>
</li>
<li>
<p>chain：包含一组rule的链</p>
</li>
<li>
<p>rule：匹配数据包的规则，例如：协议，端口号</p>
</li>
<li>
<p>target：规则中的具体行为，例如：ACCEPT，DROP，INPUT，FORWARD，OUTPUT</p>
</li>
<li>
<p>表</p>
<ul>
<li>filter：默认表，通用数据包过滤表</li>
<li>mangle：为特定的数据包设计</li>
<li>nat：针对创建新连接的数据包</li>
<li>raw：主要用于结合 NOTRACK target 配置连接跟踪的豁免</li>
<li>security：用于MAC（Mandatory Access Control，强制访问控制）规则</li>
</ul>
</li>
<li>
<p>规则链</p>
<ul>
<li>INPUT：以本机为目标的入口数据包规则链</li>
<li>OUTPUT：本机产生，向外转发的数据包规则链</li>
<li>FORWARD：路由经过本机的数据包规则链</li>
<li>PREROUTING：数据包进入路由之前的规则链</li>
<li>POSTROUTING：数据包发送到目标前（出路由）的规则链</li>
</ul>
</li>
</ul>
<p>IPtables处理链接的算法复杂度为O(n)，IPVS为O(1)，Service规模在1000内二者差距并不是很大。在Service规模超过1000后，IPtables规则链达到2000以上，性能开始下降，响应时间成倍数增加，IPVS则几乎不受Service规模的影响。Calico虽然也采用IPtables技术，但对规则链进行了优化，算法复杂度也达到了了O(1)的水平。除了大规模服务性能的差距，IPVS还具备复杂均衡算法全（轮询、最小连接数、哈希值、最小延迟等）、支持健康检查等优点。</p>
<p><img loading="lazy" src="https://deemoprobe.oss-cn-shanghai.aliyuncs.com/images/IPVSvsIPtables.png" alt="IPVSvsIPtables"  />
</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># 集群一个node节点核心组件运行状态和参数</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">[</span>root@k8s-node01 ~<span style="color:#f92672">]</span><span style="color:#75715e"># systemctl status -l kubelet.service</span>
</span></span><span style="display:flex;"><span>● kubelet.service - Kubernetes Kubelet
</span></span><span style="display:flex;"><span>   Loaded: loaded <span style="color:#f92672">(</span>/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>  Drop-In: /etc/systemd/system/kubelet.service.d
</span></span><span style="display:flex;"><span>           └─10-kubelet.conf
</span></span><span style="display:flex;"><span>   Active: active <span style="color:#f92672">(</span>running<span style="color:#f92672">)</span> since Sun 2022-03-13 20:03:34 CST; 33min ago
</span></span><span style="display:flex;"><span>     Docs: https://github.com/kubernetes/kubernetes
</span></span><span style="display:flex;"><span> Main PID: <span style="color:#ae81ff">1572</span> <span style="color:#f92672">(</span>kubelet<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    Tasks: <span style="color:#ae81ff">14</span>
</span></span><span style="display:flex;"><span>   Memory: 164.2M
</span></span><span style="display:flex;"><span>   CGroup: /system.slice/kubelet.service
</span></span><span style="display:flex;"><span>           └─1572 /usr/local/bin/kubelet --bootstrap-kubeconfig<span style="color:#f92672">=</span>/etc/kubernetes/bootstrap-kubelet.kubeconfig --kubeconfig<span style="color:#f92672">=</span>/etc/kubernetes/kubelet.kubeconfig --config<span style="color:#f92672">=</span>/etc/kubernetes/kubelet-conf.yml --network-plugin<span style="color:#f92672">=</span>cni --cni-conf-dir<span style="color:#f92672">=</span>/etc/cni/net.d --cni-bin-dir<span style="color:#f92672">=</span>/opt/cni/bin --container-runtime<span style="color:#f92672">=</span>remote --runtime-request-timeout<span style="color:#f92672">=</span>15m --container-runtime-endpoint<span style="color:#f92672">=</span>unix:///run/containerd/containerd.sock --cgroup-driver<span style="color:#f92672">=</span>systemd --node-labels<span style="color:#f92672">=</span>node.kubernetes.io/node<span style="color:#f92672">=</span>
</span></span><span style="display:flex;"><span>   ...
</span></span><span style="display:flex;"><span><span style="color:#f92672">[</span>root@k8s-node01 ~<span style="color:#f92672">]</span><span style="color:#75715e"># systemctl status -l kube-proxy.service</span>
</span></span><span style="display:flex;"><span>● kube-proxy.service - Kubernetes Kube Proxy
</span></span><span style="display:flex;"><span>   Loaded: loaded <span style="color:#f92672">(</span>/usr/lib/systemd/system/kube-proxy.service; enabled; vendor preset: disabled<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>   Active: active <span style="color:#f92672">(</span>running<span style="color:#f92672">)</span> since Sun 2022-03-13 20:03:32 CST; 33min ago
</span></span><span style="display:flex;"><span>     Docs: https://github.com/kubernetes/kubernetes
</span></span><span style="display:flex;"><span> Main PID: <span style="color:#ae81ff">996</span> <span style="color:#f92672">(</span>kube-proxy<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    Tasks: <span style="color:#ae81ff">6</span>
</span></span><span style="display:flex;"><span>   Memory: 57.9M
</span></span><span style="display:flex;"><span>   CGroup: /system.slice/kube-proxy.service
</span></span><span style="display:flex;"><span>           └─996 /usr/local/bin/kube-proxy --config<span style="color:#f92672">=</span>/etc/kubernetes/kube-proxy.yaml --v<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span> --feature-gates<span style="color:#f92672">=</span>EphemeralContainers<span style="color:#f92672">=</span>true
</span></span><span style="display:flex;"><span>   ...
</span></span><span style="display:flex;"><span><span style="color:#75715e"># kubelet进程对应的配置文件--config=/etc/kubernetes/kubelet-conf.yml</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">[</span>root@k8s-node01 ~<span style="color:#f92672">]</span><span style="color:#75715e"># cat /etc/kubernetes/kubelet-conf.yml </span>
</span></span><span style="display:flex;"><span>apiVersion: kubelet.config.k8s.io/v1beta1
</span></span><span style="display:flex;"><span>kind: KubeletConfiguration
</span></span><span style="display:flex;"><span>address: 0.0.0.0
</span></span><span style="display:flex;"><span>port: <span style="color:#ae81ff">10250</span>
</span></span><span style="display:flex;"><span>readOnlyPort: <span style="color:#ae81ff">10255</span>
</span></span><span style="display:flex;"><span>authentication:
</span></span><span style="display:flex;"><span>  anonymous:
</span></span><span style="display:flex;"><span>    enabled: false
</span></span><span style="display:flex;"><span>  webhook:
</span></span><span style="display:flex;"><span>    cacheTTL: 2m0s
</span></span><span style="display:flex;"><span>    enabled: true
</span></span><span style="display:flex;"><span>  x509:
</span></span><span style="display:flex;"><span>    clientCAFile: /etc/kubernetes/pki/ca.pem
</span></span><span style="display:flex;"><span>authorization:
</span></span><span style="display:flex;"><span>  mode: Webhook
</span></span><span style="display:flex;"><span>  webhook:
</span></span><span style="display:flex;"><span>    cacheAuthorizedTTL: 5m0s
</span></span><span style="display:flex;"><span>    cacheUnauthorizedTTL: 30s
</span></span><span style="display:flex;"><span>cgroupDriver: systemd
</span></span><span style="display:flex;"><span>cgroupsPerQOS: true
</span></span><span style="display:flex;"><span>clusterDNS:
</span></span><span style="display:flex;"><span>- 10.96.0.10
</span></span><span style="display:flex;"><span>clusterDomain: cluster.local
</span></span><span style="display:flex;"><span>containerLogMaxFiles: <span style="color:#ae81ff">5</span>
</span></span><span style="display:flex;"><span>containerLogMaxSize: 10Mi
</span></span><span style="display:flex;"><span>contentType: application/vnd.kubernetes.protobuf
</span></span><span style="display:flex;"><span>cpuCFSQuota: true
</span></span><span style="display:flex;"><span>cpuManagerPolicy: none
</span></span><span style="display:flex;"><span>cpuManagerReconcilePeriod: 10s
</span></span><span style="display:flex;"><span>enableControllerAttachDetach: true
</span></span><span style="display:flex;"><span>enableDebuggingHandlers: true
</span></span><span style="display:flex;"><span>enforceNodeAllocatable:
</span></span><span style="display:flex;"><span>- pods
</span></span><span style="display:flex;"><span>eventBurst: <span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>eventRecordQPS: <span style="color:#ae81ff">5</span>
</span></span><span style="display:flex;"><span>evictionHard:
</span></span><span style="display:flex;"><span>  imagefs.available: 15%
</span></span><span style="display:flex;"><span>  memory.available: 100Mi
</span></span><span style="display:flex;"><span>  nodefs.available: 10%
</span></span><span style="display:flex;"><span>  nodefs.inodesFree: 5%
</span></span><span style="display:flex;"><span>evictionPressureTransitionPeriod: 5m0s
</span></span><span style="display:flex;"><span>failSwapOn: true
</span></span><span style="display:flex;"><span>fileCheckFrequency: 20s
</span></span><span style="display:flex;"><span>hairpinMode: promiscuous-bridge
</span></span><span style="display:flex;"><span>healthzBindAddress: 127.0.0.1
</span></span><span style="display:flex;"><span>healthzPort: <span style="color:#ae81ff">10248</span>
</span></span><span style="display:flex;"><span>httpCheckFrequency: 20s
</span></span><span style="display:flex;"><span>imageGCHighThresholdPercent: <span style="color:#ae81ff">85</span>
</span></span><span style="display:flex;"><span>imageGCLowThresholdPercent: <span style="color:#ae81ff">80</span>
</span></span><span style="display:flex;"><span>imageMinimumGCAge: 2m0s
</span></span><span style="display:flex;"><span>iptablesDropBit: <span style="color:#ae81ff">15</span>
</span></span><span style="display:flex;"><span>iptablesMasqueradeBit: <span style="color:#ae81ff">14</span>
</span></span><span style="display:flex;"><span>kubeAPIBurst: <span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>kubeAPIQPS: <span style="color:#ae81ff">5</span>
</span></span><span style="display:flex;"><span>makeIPTablesUtilChains: true
</span></span><span style="display:flex;"><span>maxOpenFiles: <span style="color:#ae81ff">1000000</span>
</span></span><span style="display:flex;"><span>maxPods: <span style="color:#ae81ff">110</span>
</span></span><span style="display:flex;"><span>nodeStatusUpdateFrequency: 10s
</span></span><span style="display:flex;"><span>oomScoreAdj: -999
</span></span><span style="display:flex;"><span>podPidsLimit: -1
</span></span><span style="display:flex;"><span>registryBurst: <span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>registryPullQPS: <span style="color:#ae81ff">5</span>
</span></span><span style="display:flex;"><span>resolvConf: /etc/resolv.conf
</span></span><span style="display:flex;"><span>rotateCertificates: true
</span></span><span style="display:flex;"><span>runtimeRequestTimeout: 2m0s
</span></span><span style="display:flex;"><span>serializeImagePulls: true
</span></span><span style="display:flex;"><span>staticPodPath: /etc/kubernetes/manifests
</span></span><span style="display:flex;"><span>streamingConnectionIdleTimeout: 4h0m0s
</span></span><span style="display:flex;"><span>syncFrequency: 1m0s
</span></span><span style="display:flex;"><span>volumeStatsAggPeriod: 1m0s
</span></span><span style="display:flex;"><span>featureGates:
</span></span><span style="display:flex;"><span>  EphemeralContainers: true
</span></span><span style="display:flex;"><span><span style="color:#75715e"># kube-proxy对应的配置文件--config=/etc/kubernetes/kube-proxy.yaml</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">[</span>root@k8s-node01 ~<span style="color:#f92672">]</span><span style="color:#75715e"># cat /etc/kubernetes/kube-proxy.yaml </span>
</span></span><span style="display:flex;"><span>apiVersion: kubeproxy.config.k8s.io/v1alpha1
</span></span><span style="display:flex;"><span>bindAddress: 0.0.0.0
</span></span><span style="display:flex;"><span>clientConnection:
</span></span><span style="display:flex;"><span>  acceptContentTypes: <span style="color:#e6db74">&#34;&#34;</span>
</span></span><span style="display:flex;"><span>  burst: <span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>  contentType: application/vnd.kubernetes.protobuf
</span></span><span style="display:flex;"><span>  kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig
</span></span><span style="display:flex;"><span>  qps: <span style="color:#ae81ff">5</span>
</span></span><span style="display:flex;"><span>clusterCIDR: 172.16.0.0/12 
</span></span><span style="display:flex;"><span>configSyncPeriod: 15m0s
</span></span><span style="display:flex;"><span>conntrack:
</span></span><span style="display:flex;"><span>  max: null
</span></span><span style="display:flex;"><span>  maxPerCore: <span style="color:#ae81ff">32768</span>
</span></span><span style="display:flex;"><span>  min: <span style="color:#ae81ff">131072</span>
</span></span><span style="display:flex;"><span>  tcpCloseWaitTimeout: 1h0m0s
</span></span><span style="display:flex;"><span>  tcpEstablishedTimeout: 24h0m0s
</span></span><span style="display:flex;"><span>enableProfiling: false
</span></span><span style="display:flex;"><span>healthzBindAddress: 0.0.0.0:10256
</span></span><span style="display:flex;"><span>hostnameOverride: <span style="color:#e6db74">&#34;&#34;</span>
</span></span><span style="display:flex;"><span>iptables:
</span></span><span style="display:flex;"><span>  masqueradeAll: false
</span></span><span style="display:flex;"><span>  masqueradeBit: <span style="color:#ae81ff">14</span>
</span></span><span style="display:flex;"><span>  minSyncPeriod: 0s
</span></span><span style="display:flex;"><span>  syncPeriod: 30s
</span></span><span style="display:flex;"><span>ipvs:
</span></span><span style="display:flex;"><span>  masqueradeAll: true
</span></span><span style="display:flex;"><span>  minSyncPeriod: 5s
</span></span><span style="display:flex;"><span>  scheduler: <span style="color:#e6db74">&#34;rr&#34;</span>
</span></span><span style="display:flex;"><span>  syncPeriod: 30s
</span></span><span style="display:flex;"><span>kind: KubeProxyConfiguration
</span></span><span style="display:flex;"><span>metricsBindAddress: 127.0.0.1:10249
</span></span><span style="display:flex;"><span>mode: <span style="color:#e6db74">&#34;ipvs&#34;</span>
</span></span><span style="display:flex;"><span>nodePortAddresses: null
</span></span><span style="display:flex;"><span>oomScoreAdj: -999
</span></span><span style="display:flex;"><span>portRange: <span style="color:#e6db74">&#34;&#34;</span>
</span></span><span style="display:flex;"><span>udpIdleTimeout: 250ms
</span></span></code></pre></div><h2 id="容器运行时">容器运行时<a hidden class="anchor" aria-hidden="true" href="#容器运行时">#</a></h2>
<p>符合CRI（Container Runtime Interface）标准的容器运行时（Container Runtime）是实际上管理容器的组件，容器运行时可分为高层和底层运行时。高层运行时诸如：Docker、containerd、CRI-O，官方介绍:<a href="https://kubernetes.io/zh/docs/setup/production-environment/container-runtimes/"  target="_blank" rel="noopener" style="color:#42b983" ;>容器运行时</a> 底层运行时诸如：runc、kata、gVisor，kata和gVisor相对不是很成熟，目前底层运行时一般默认选择runc。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># containerd容器运行时服务状态</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">[</span>root@k8s-master01 ~<span style="color:#f92672">]</span><span style="color:#75715e"># systemctl status -l containerd.service </span>
</span></span><span style="display:flex;"><span>● containerd.service - containerd container runtime
</span></span><span style="display:flex;"><span>   Loaded: loaded <span style="color:#f92672">(</span>/usr/lib/systemd/system/containerd.service; enabled; vendor preset: disabled<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>   Active: active <span style="color:#f92672">(</span>running<span style="color:#f92672">)</span> since Sun 2022-03-13 19:58:14 CST; 1h 13min ago
</span></span><span style="display:flex;"><span>     Docs: https://containerd.io
</span></span><span style="display:flex;"><span>  Process: <span style="color:#ae81ff">996</span> ExecStartPre<span style="color:#f92672">=</span>/sbin/modprobe overlay <span style="color:#f92672">(</span>code<span style="color:#f92672">=</span>exited, status<span style="color:#f92672">=</span>0/SUCCESS<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span> Main PID: <span style="color:#ae81ff">1011</span> <span style="color:#f92672">(</span>containerd<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    Tasks: <span style="color:#ae81ff">48</span>
</span></span><span style="display:flex;"><span>   Memory: 99.9M
</span></span><span style="display:flex;"><span>   CGroup: /system.slice/containerd.service
</span></span><span style="display:flex;"><span>           ├─1011 /usr/bin/containerd
</span></span><span style="display:flex;"><span>           ├─1857 /usr/bin/containerd-shim-runc-v2 -namespace k8s.io -id 83f9705395fb4a165bd52894f6545a58b8040a5fe6774b03cdcd02ca90900708 -address /run/containerd/containerd.sock
</span></span><span style="display:flex;"><span>           ├─2394 /usr/bin/containerd-shim-runc-v2 -namespace k8s.io -id ae7a1e3a28a331bf11762c3b049e8237c322cddabd842dca398911a45dc656db -address /run/containerd/containerd.sock
</span></span><span style="display:flex;"><span>           └─2802 /usr/bin/containerd-shim-runc-v2 -namespace k8s.io -id 30e5560350fdcb76efbbb10f7c8e9a41fa4fdc10e4483f62830fff749a448fb1 -address /run/containerd/containerd.sock
</span></span></code></pre></div><h2 id="网络插件">网络插件<a hidden class="anchor" aria-hidden="true" href="#网络插件">#</a></h2>
<p>符合CNI（Container Network Interface）标准的网络插件，诸如：Calico、Cilium、Flannel等。会为每个pod生成唯一的IP地址，并且把每个节点当做一个路由器。Cilium官方有一个CNI性能测试报告，采用eBPF技术的Cilium和Calico性能优越，具备TCP吞吐量大、CPU和网络开销低等特点。eBPF是一种内核级包过滤技术，诸如tcpdump、Netfilter均采用该技术，性能比iptables要好很多。可以使用<code>route -n</code>查看</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># 使用calico插件的路由</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">[</span>root@k8s-master01 ~<span style="color:#f92672">]</span><span style="color:#75715e"># route -n</span>
</span></span><span style="display:flex;"><span>Kernel IP routing table
</span></span><span style="display:flex;"><span>Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
</span></span><span style="display:flex;"><span>0.0.0.0         192.168.43.1    0.0.0.0         UG    <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> ens33
</span></span><span style="display:flex;"><span>169.254.0.0     0.0.0.0         255.255.0.0     U     <span style="color:#ae81ff">1002</span>   <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> ens33
</span></span><span style="display:flex;"><span>172.17.0.0      0.0.0.0         255.255.0.0     U     <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> docker0
</span></span><span style="display:flex;"><span>172.17.125.0    192.168.43.186  255.255.255.192 UG    <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> tunl0
</span></span><span style="display:flex;"><span>172.18.195.0    192.168.43.185  255.255.255.192 UG    <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> tunl0
</span></span><span style="display:flex;"><span>172.25.92.64    192.168.43.184  255.255.255.192 UG    <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> tunl0
</span></span><span style="display:flex;"><span>172.25.244.192  0.0.0.0         255.255.255.192 U     <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> *
</span></span><span style="display:flex;"><span>172.25.244.213  0.0.0.0         255.255.255.255 UH    <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> cali183ffe150c7
</span></span><span style="display:flex;"><span>172.25.244.214  0.0.0.0         255.255.255.255 UH    <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> calicec0b1cfb68
</span></span><span style="display:flex;"><span>172.27.14.192   192.168.43.187  255.255.255.192 UG    <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> tunl0
</span></span><span style="display:flex;"><span>192.168.43.0    0.0.0.0         255.255.255.0   U     <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> ens33
</span></span></code></pre></div><h2 id="coredns">CoreDNS<a hidden class="anchor" aria-hidden="true" href="#coredns">#</a></h2>
<p>用于集群内部service的域名解析系统，可以让pod把service名称解析成ip地址，然后通过service的IP地址连接到对应的应用上。</p>
<h2 id="pod">Pod<a hidden class="anchor" aria-hidden="true" href="#pod">#</a></h2>
<p>Pod是Kubernetes中最小的单元，是由一个或多个容器组成的。每个pod还包含一个pause容器，pause容器是pod的父容器，负责僵尸进程的回收管理，通过pause容器可以使同一个pod内多个容器共享存储、网络、PID、IPC等。详细可以查看博客：<a href="https://www.deemoprobe.com/share/k-pod/"  target="_blank" rel="noopener" style="color:#42b983" ;>Pod定义及零宕机部署</a></p>
<blockquote>
<p>本文相关概念参考书籍：<a href="http://www.broadview.com.cn/book/6213"  target="_blank" rel="noopener" style="color:#42b983" ;>《Kubernetes生产化实践之路》</a></p>
</blockquote>


        </div>

        <footer class="post-footer">
            
<nav class="paginav">
  <a class="prev" href="https://deemoprobe.github.io/posts/tech/kubernetes/kuberneteskubeadminstallation-ha/">
    <span class="title">« 上一页</span>
    <br>
    <span>KubernetesKubeadmInstallation HA</span>
  </a>
  <a class="next" href="https://deemoprobe.github.io/posts/tech/docker/dockerimagesoptimization/">
    <span class="title">下一页 »</span>
    <br>
    <span>DockerImagesOptimization</span>
  </a>
</nav>

        </footer>
    </div>
</article>
</main>

<footer class="footer">
    
        <span id="runtime_span"></span>
        <script
            type="text/javascript">function show_runtime() { window.setTimeout("show_runtime()", 1000); X = new Date("1/1/2023 1:00:00"); Y = new Date(); T = (Y.getTime() - X.getTime()); M = 24 * 60 * 60 * 1000; a = T / M; A = Math.floor(a); b = (a - A) * 24; B = Math.floor(b); c = (b - B) * 60; C = Math.floor((b - B) * 60); D = Math.floor((c - C) * 60); runtime_span.innerHTML = "网站已运行" + A + "天" + B + "小时" + C + "分" + D + "秒" } show_runtime();</script>
    
    
    <br>
    <span>
        Copyright
        &copy;
        2022-2023
        <a href="https://deemoprobe.github.io/" style="color:#939393;">William&#39;s Blog</a>
        All Rights Reserved
    </span>
    
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <span class="topInner">
        <svg class="topSvg" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z"/>
        </svg>
        <span id="read_progress"></span>
    </span>
</a>

<script>
    document.addEventListener('scroll', function (e) {
        const readProgress = document.getElementById("read_progress");
        const scrollHeight = document.documentElement.scrollHeight;
        const clientHeight = document.documentElement.clientHeight;
        const scrollTop = document.documentElement.scrollTop || document.body.scrollTop;
        readProgress.innerText = ((scrollTop / (scrollHeight - clientHeight)).toFixed(2) * 100).toFixed(0);
    })
</script>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });
</script>
<script>
    let mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 200 || document.documentElement.scrollTop > 200) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };
</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        (function() {
            document.cookie = "change-themes" + "="+ escape ("false");
        })()

        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    });
</script>

<script>
    document.body.addEventListener('copy', function (e) {
        if (window.getSelection().toString() && window.getSelection().toString().length > 50) {
            let clipboardData = e.clipboardData || window.clipboardData;
            if (clipboardData) {
                e.preventDefault();
                let htmlData = window.getSelection().toString() +
                    '\r\n\n————————————————\r\n' +
                    '版权声明：本文为「'+"William's Blog"+'」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。' +
                '\r\n原文链接：' + location.href;
                let textData = window.getSelection().toString() +
                    '\r\n\n————————————————\r\n' +
                    '版权声明：本文为「'+"William's Blog"+'」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。' +
                '\r\n原文链接：' + location.href;
                clipboardData.setData('text/html', htmlData);
                clipboardData.setData('text/plain', textData);
            }
        }
    });
</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;
        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = '📄复制';

        function copyingDone() {
            copybutton.innerText = '👌🏻已复制!';
            setTimeout(() => {
                copybutton.innerText = '📄复制';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                let text = codeblock.textContent +
                    '\r\n————————————————\r\n' +
                    '版权声明：本文为「'+"William's Blog"+'」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。' +
                '\r\n原文链接：' + location.href;
                navigator.clipboard.writeText(text);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) {}
            selection.removeRange(range);
        });

        let language = codeblock.className.replaceAll("language-", "")
        let macTool = document.createElement("div")
        let macTool1 = document.createElement("div")
        let macTool2 = document.createElement("div")
        let macTool3 = document.createElement("div")
        let languageType = document.createElement("div")
        languageType.innerText = language
        macTool.setAttribute('class', 'mac-tool')
        macTool1.setAttribute('class', 'mac bb1')
        macTool2.setAttribute('class', 'mac bb2')
        macTool3.setAttribute('class', 'mac bb3')
        languageType.setAttribute('class', 'language-type')
        macTool.appendChild(macTool1)
        macTool.appendChild(macTool2)
        macTool.appendChild(macTool3)
        macTool.appendChild(languageType)

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
            container.appendChild(macTool)
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
            container.appendChild(macTool)
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
            container.appendChild(macTool)
        }
    });
</script>

<script>
    $("code[class^=language] ").on("mouseover", function () {
        if (this.clientWidth < this.scrollWidth) {
            $(this).css("width", "135%")
            $(this).css("border-top-right-radius", "var(--radius)")
        }
    }).on("mouseout", function () {
        $(this).css("width", "100%")
        $(this).css("border-top-right-radius", "unset")
    })
</script>
</body>

</html>
